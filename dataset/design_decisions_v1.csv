,label,text
0,1,string indexer inverse strictly associated string indexer name super clear rename string indexer inverse index tostring
1,1,requires discussion sure whether runs useful parameter certainly complicates implementation might want optimize k means implementation block matrix operations case runs may worth tradeoffs remove runs k means pipeline api
2,1,exists chance prefixes keep growing maximum pattern length final local processing step becomes unnecessary skip local processing prefix span small prefixes
3,1,parquet mr fixed several issues affects park example parquet spark upgrade parquet mr
4,1,utilities substring substring binary sql binary prefix comparator compute prefix binary data put together byte array easy read move utilities binary data byte array
5,1,platform dependent unsafe way verbose rename platform dependent unsafe platform
6,1,took look commit messages git log looks like individual commit messages useful include make commit messages verbose usually bunch extremely concise descriptions bug fixes merges etc see mailing list discussions http apache spark developers list n nabble com discuss removing individual commit messages squash commit message td html remove individual commit messages squash commit message
7,1,add python api user guide example ml regression isotonic regression add python api ml regression isotonic regression
8,1,add python api multilayer perceptron classifier add python api multilayer perceptron classifier
9,1,add python api user guide example ml feature count vectorizer model add python api ml feature count vectorizer
10,1,introduced netty network module shuffles park turned default releases old connection manager difficult maintain time remove remove connection manager
11,0,check add miss doc spy spark ml issue check miss docs mlml lib check add missing doc spy spark ml
12,1,see discussion https git hub com apache spark pull issue comment improve performance decimal times casting integral
13,1,type check longer applies new tungsten world remove type check debug package
14,1,https git hub com apache spark pull added from unsafe convert next ed unsafe data like array map struct safe versions quick solution already generate safe conversion codegen ed remove from unsafe implement codegen version generate safe remove from unsafe add codegen version generate safe
15,1,joined row any null currently loops every field check null inefficient underlying rows unsafe rows delegate underlying implementation joined row any null delegate underlying rows
16,0,data sources show physical rdd physical plan explain better show name data source existing improve explain message data sources can node
17,0,identify potential api issues list public api changes affect binary source incompatibility using command report result attached list binary source compatibility issues jap i compliance checker
18,1,jira making several m lapis public make easier users write pipeline stages issue brought er on wright descriptions copied http apache spark developers list n nabble com make ml developer apis public post td html plan make apis public spark however marked developer api likely broken future vector udt define relation vector field vector udt must instantiated identifiable trait trait generates unique identifier associated pipeline component nice consistent format reusing trait probabilistic classifier third party components leverage complex logic around computing selected columns yet make public schema utils third party pipeline components need checking column types appending columns probably moved spark sql users copy methods needed make m lapis public vector udt identifiable probabilistic classifier
19,1,consider sort merge join requires sorted clustered distribution input rows says mj children produce unsorted output single partition case need inject sort operators need inject exchanges unfortunately looks like exchange unnecessarily re partitions using hash partitioning update exchange unnecessarily repartition children ordering requirements unsatisfied like fix spark since makes certain types unit tests easier write ensure requirements add unnecessary shuffles ordering requirements unsatisfied
20,1,previously use mb default page size way big lots park applications especially single node patch changes default page size unset user determined number cores available total execution memory available pick default page size intelligently
21,1,add feature interaction transformer takes list vector double columns generate single vector column contains interactions multiplication among proper handling feature names add feature interaction transformer
22,1,eg currently sorts within task aggregation sortkey shuffle environments tight memory restrictions first operator may acquire much memory subsequent ones task starved simple fix reserve least page advance places reserved pagesize need normal pagesize sister problems parks park core reserve page unsafe operators avoid starving operator
23,1,small performance optimization need generate tuple immediately discard key also need extra wrapper remove sql new had oop rdd generated tuple interruptible iterator
24,1,subsumed new aggregate implementation remove generated aggregate
25,1,generate unsafe projection used directly code generated serializer longer need spark sql serializer removes park sql serializer favor unsafe exchange
26,1,currently support using decimal type precision new unsafe aggregation good support support update decimal type precision unsafe row
27,1,many modeling application data points necessarily sampled equal probabilities linear regression support weighting account sampling linear regression supported weighted data
28,1,update internal row to seq make accept datatype info
29,1,spark style checker banus escala java conversions provides implicit conversions java scala collections types instead performing conversions explicitly using java converters for going conversions altogether occurring inside performance critical code ban use java conversions migrate existing uses java converters
30,1,add streamingcontext get active or create python api
31,0,epics park release qa plans tracking various components spark testing plan
32,1,see http apache spark developers list n nabble com spark ec get repot dhtml details moves park ec me sosa mplab
33,1,remove create code create struct code replace usage create struct code
34,1,reviewing y huai patch spark noticed exchange compatible check may incorrectly returning false many cases far know actually problem compatible meets requirements needs any sort checks serving short circuit performance optimizations necessary correctness order reduce code complexity think remove checks unconditionally rewrite operator children safe rewrite tree single bottom pass remove compatible with meets requirements needs any sort checks exchange
35,1,added improve performance jit inline joined row calls however also improve projecting output unsafe row tungsten variant operators remove extra joined rows
36,1,remove unsafe row converter favor unsafe projection
37,1,consolidate local scheduler cluster scheduler given functionalities duplicated done removing local scheduler create local scheduler backend connects directly executor consolidate local scheduler cluster scheduler
38,1,big change lets us use type information prevent accidentally passing internal types external types remove internal row inheritance row
39,1,see spark added varargs though technically correct often requires developers clean assembly rather clean assembly nuisance development jira remove pending fix scala compiler params set default keep varargs annotation
40,1,spark option called spark local execution enabled according docs quote enables spark run certain jobs first take driver without sending tasks cluster make certain jobs execute quickly may require shipping whole partition data driver quote feature ends adding quite bit complexity dag scheduler especially run locally within thread method far know nobody uses feature searched mailing list seen recent mentions configuration stack trace s including run locally method step towards scheduler complexity reduction propose remove feature code related spark removed ag scheduler run locally within threads park local execution enabled
41,1,small change based code review offline discussion dragos removing unnecessary self types catalyst
42,1,new parquet external data source matures remove old parquet support remove sold parquet support code
43,1,useful cause problems tostring due order mixed remove leafnode unary node binary node treenode
44,1,spark context constructor takes preferred node locality data worked since spark also features park strictly better correct implementation feature remove documentation references feature print warning used saying work remove references preferred node locality data javadoc print warning used
45,1,unnecessary makes type hierarchy slightly complicated needed remove extract value with ordinal abstract class
46,1,right internal row mega morphic many different implementations work towards one two internal row implementations remove empty row class
47,1,type alias initially moved row around want massive changes expression code pretty easy remove one less concept worry remove internal row type alias expressions package
48,1,based discussion offline mar mbr us remove generate projection remove generate projection
49,1,make convenient r users uses park r browsers install configurer studio servers park ec
50,1,remove existing expression optimization suite update check evaluation also run optimizer version expression eval helper check evaluation also run optimizer version
51,1,perspective code reviewer find confusing using string directly remove term code type aliases code generation
52,1,useful type use remove simplify expressions slightly remove evaluated type sql expression
53,1,external sorter contains bunch move functionality external sorter separate class shares common interface insert all write partitioned file stepping stone towards eventually removing bypass path see spark move hash style shuffle code external sorter file
54,1,learnt lessons parks park avoid use scala concurrent execution context implicit s global user may submit blocking actions scala concurrent execution context implicit s global exhaust threads could crash sparks park always use thread pools safety remove import scala concurrent execution context implicit s global
55,1,removed calling size length condition avoid extra jvm call
56,1,rewrite distinct using group bye aggregate operator remove physical distinct operator favor aggregate
57,1,removed diff sum theoretical zero linear regression coding for mating
58,1,want change improves park ml api trees ensembles can not change old apis park ml lib support changes want make move implementations park ml libs park ml generalize modify also ensure change behavior old api several steps copy implementations park ml changes park ml classes use implementation rather calling spark ml lib implementation current spark ml tests ensure implementations learn exactly models note include performance testing make sure updated code regressions update run tests using spark perf regressions removes park ml lib implementation makes park ml lib apis wrappers around spark ml implementations park ml tests ensure change behavior move unit tests spark ml changes park ml lib unit tests verify model equivalence jira step steps separate jira s updates safely generalize improves park ml implementation move tree forest implementations park ml libs park ml
59,1,maven repository blocked china get rid dependency people china compiles park remove dependency twitter j repository
60,1,always possible whenever possible remove reduce differences pandas spark data frames python improve data frame api compatibility pandas
61,1,depends internal interfaces parks qld on emerging spark data frame udfs r
62,1,would great create apis external block stores rather bunch statements everywhere create external block store api
63,1,upgrade tachyon dependency
64,1,deprecated configs currently strewn across codebase would good simplify handling deprecated configs central location avoid duplicating deprecation logic everywhere centralize deprecated configs spark conf
65,1,continue discussion ldap r check poing dir global spark configuration altered ml algorithm could check whether checkpoint dir set checkpoint interval positive remove set checkpoint dir lda tree strategy
66,1,method survived code review since v exposes jbl as types let remove public api expect one calls directly hi deals solve least squares
67,1,to local iterator available java scala add functionality python also able use py spark iterate dataset partition partition add to local iterator py spark rdd
68,1,fix to do spark sql remove command use runnable command instead re factory commands park sql
69,1,remove unneeded staging repositories build
70,1,mqt t client removed eclipse paho repository hence breaking spark build upgrade mqt t dependency use mqt t client
71,1,refactoring performance slightly increased removing overhead breeze vector bottleneck still breeze norm implemented active iterator inefficiency breeze norm addressed next pr least pr makes base re factorize normalizer make code cleaner
72,1,rdd sampler try use numpy gain better performance possi on number call random faction n pure python implementation possi on much performance gain numpy numpy dependent py spark may be introduce problem numpy installed slaves installed master reported xxxx also complicate code lot may remove numpy rdd sampler remove numpy rdd sampler py spark
73,1,example yarn rm client yarn rm client impl merged yarn allocator yarn allocation handler merged remove layers abstraction yarn code longer needed dropping yarn alpha
74,1,due vertex attribute caching edge rdd previously took two type parameters e dvd however implementation detail exposed interface pr drop svd type parameter requires removing filter method edge rdd interface depends vertex attribute caching drop vd type parameter edge rdd
75,0,complicated modules sure whether intellij correctly understands source locations also might require specifying profiles build work directly document clearly start vanilla spark master get entire thing building intellij create instructions fully buildings park intellij
76,1,upgrade snappy java across maintenance branches release improves error messages attempting deserialize empty inputs using snappy input stream operation always error old error messages made hard distinguish failures due empty streams ones due reading invalid corrupted streams seehttpsgithubcomxerial snappy java issues context major help snappy debugging work upgrade snappy java
77,1,pyro lite release new version pr https githubcomirmenpyrolite pull remove workaround introduced pr https git hub com apache spark pull remove workaround pickle array float pyro lite
78,1,depend bit user demand commitment level maintainers like propose following timeline yarn alpha supports park deprecate yarn alpha spark remove yarn alpha e require yarns table since yarn alpha clearly identified alpha api seems reasonable drop support minor release however depend bit whether anyone uses outside yahoo sure past api used maintained yahoo migrating soon stable api deprecate later remove yarn alpha support
79,1,hive local context nearly completely redundant hive context consider deprecating removing uses get rid local hive context
80,1,according httpdocakkaiodocsakka intro getting started html akka published maven central documentation pom files need use old akka repo one less step users worry remove use special maven repo akka
81,0,sql parser fails resolve nested case statement like select case case else end else end tb exception exception thread main org apache spark sql catalyst parser parseexception mismatched input expecting like rlikeeqgtedivlinepossql select case case else end else end tb remove parentheses fine select case case else end else end tb sql sql parser fails resolve nested case statement parentheses
82,0,names method fails check validity assignment values fixed calling col names within names see example fix bug name assignment methods park r
83,0,range operator currently output input metrics results q lui number rows shown always fix input metrics range operator
84,1,removed codes reachable in conversion already resolve type coercion issues remove type coercion promote strings
85,0,current executor id to int cause numberformatexception unit test pass currently ask with retry catching exception rpc call thus go branch return true executor id hear beat receiver suite incorrect
86,1,metadata introduced mark optional columns merged parquet schema filter predicate pushdown upgrade parquet includes fix pushdown optional columns need metadata remove metadata used mark optional columns merged parquet schema filter predicate pushdown
87,1,apache parquet released officially last week jan issue aims bump parquet version since includes many fixes https lists apache org thread html a fcfecbbbeecaeaaaeffccdev parquet apache orge upgrade parquet
88,0,function json ignores user provided options
89,0,number names exported exist module data frame data frame na functions data frame stat functions https git hub com apache spark blob cadbfefafceedapythonpys park sql column py l results unexpected importer ro spy spark sql column exports nonexistent names
90,0,resolving jira https issues apache org jira browse spark doc needs update doc options case insensitive since spark
91,0,need updates programming guide example vignettes bisecting k means spark r documentation
92,0,although udf udf different udf name plans looks confusing always identical name udf explain output
93,1,create python wrappers park ml classification linear svc linear svc python api
94,0,similar spark codegen danger arbitrary fix code injection vulnerability related generator functions
95,0,whenever stdout outputs spark jvm typically calling println dropped spark r example explain column https git hub com apache spark blob master sql core src main scala org apache spark sql columns cal al jvm stdout output dropped spark r
96,0,k means using init mode random random seed possible actual cluster size equal configured k case summary model returns error due number cols coefficient matrix equal ks park rk means summary returns error cluster size equal k
97,0,patch spark applied spark conf object ignored launching spark context programmatically via python spark submit https git hub com apache spark blob master python py spark context py l case running python spark context conf xxx spark submit conf set conf j conf none passed arg conf object ignored used launching java gateway fix python py spark context py spark caused ignorance conf object passed spark context
98,1,implement ddl commands added several analyzer rules sql hive module analyzed dl related plans however analyzer currently one extending interface extended resolution rules defines extra rules run together rules resolution batch fit ddl rules well ddl rules may checking normalization may many times resolution batch run rules fixed point hard tell ddl rule already done checking normalization fine ddl rules idempotent bad analysis performance ddl rules may depend others pretty hard write conditions guarantee dependencies good batch run rules one pass guarantee dependencies rules order add new extending interface analyzer post hoc resolution
99,0,bq run example sql streaming java structured kafka word count subscribe topic runs park example raises following error quote exception thread main debug context cleaner got cleaning task clean broadcast org apache spark sql streaming streaming query exception job aborted due stage failure task stage failed times recent failure lost task stage tid localhost executor driver java lang illegalstateexception error reading delta file tmp temporary state delta hdfs state store provider id op part dirt mp temporary bcbb fcc abb state tmp temporary bcbb fcc abb state delta exist org apache spark sql execution streaming state hdfs backed state store provider org apache spark sql execution streaming state hdfs backed state store provider update from delta file hdfs backed state store provider scala org apache spark sql execution streaming state hdfs backed state store provider a non fun org apache spark sql execution streaming state hdfs backed state store provider load map an on fun apply hdfs backed state store provider scala org apache spark sql execution streaming state hdfs backed state store provider a non fun org apache spark sql execution streaming state hdfs backed state store provider load map an on fun apply hdfs backed state store providers ca lascala option get or else options cala org apache spark sql execution streaming state hdfs backed state store provider a non fun org apache spark sql execution streaming state hdfs backed state store provider load map apply hdfs backed state store provider scala org apache spark sql execution streaming state hdfs backed state store provider a non fun org apache spark sql execution streaming state hdfs backed state store provider load map apply hdfs backed state store providers ca lascala option get or else options cala org apache spark sql execution streaming state hdfs backed state store provider org apache spark sql execution streaming state hdfs backed state store provider load map hdfs backed state store provider scala org apache spark sql execution streaming state hdfs backed state store provider get store hdfs backed state store provider scala org apache spark sql execution streaming state state store get state store scala org apache spark sql execution streaming state state store rdd compute state store rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark rdd map partitions rdd compute map partitions rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark rdd map partitions rdd compute map partitions rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark scheduler result task run task result task scala org apache spark scheduler task run task scala org apache spark executor executor task runner run executor scala java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java caused java io filenotfoundexception file exist tmp temporary state delta org apache had oop hdfs servername node inode file value of inode file java org apache had oop hdfs servername node inode file value of inode file java org apache had oop hdfs servername no defs name system get block locations in tfs name system java org apache had oop hdfs servername no defs name system get block locations fs name system java org apache had oop hdfs servername no defs name system get block locations fs name system java org apache had oop hdfs servername node name node rpc server get block locations name node rpc server java org apache had oop hdfs protocol pb client name node protocol server side translator pb get block locations client name node protocol server side translator pb java org apache had oop hdfs protocol proto client name node protocol protos client name node protocol call blocking method client name node protocol protos java org apache hadoopipcprotobufrpc engine server proto buf rpc invoker call proto buf rpc engine java org apache had oop ipc rpc server call rpc java org apache had oop ipc server handler run server java org apache had oop ipc server handler run server java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java org apache had oop ipc server handler run server java sun reflect native constructor access or impl new instance native method sun reflect native constructor access or impl new instance native constructor access or impl java sun reflect delegating constructor access or impl new instance delegating constructor access or impl java java lang reflect constructor new instance constructor java org apache had oop ipc remoteexception instantiate exception remoteexception java org apache had oop ipc remoteexception unwrap remoteexception remoteexception java org apache had oop hdfs dfs client call get block locations dfs client java org apache had oop hdfs dfs client get located blocks dfs client java org apache had oop hdfs dfs client get located blocks dfs client java org apache had oop hdfs dfs input stream fetch located blocks and get last block length dfs input stream java org apache had oop hdfs dfs input stream open info dfs input stream java org apache had oop hdfs dfs input stream dfs input stream java org apache had oop hdfs dfs client open dfs client java org apache had oop hdfs distributed filesystem do call distributed filesystem java org apache had oop hdfs distributed filesystem do call distributed filesystem java org apache had oop fs filesystem link resolver resolve filesystem link resolver java org apache had oop hdfs distributed filesystem open distributed filesystem java org apache had oop fs filesystem open file system java org apache spark sql execution streaming state hdfs backed state store provider org apache spark sql execution streaming state hdfs backed state store provider update from delta file hdfs backed state store provider scala quote checked spark configuration file found values park sql adaptive enabled true modify false example program work bug thanks file exist tmp temporary bcbb fcc abb state delta
100,0,add test setting location managed table
101,1,use jdbc py spark check lower bound upper bound give friendly suggestion check lower bound upper bound whether equal none jdbc api
102,0,partial download download error cleaned spark r session continue stuck error messages park r hangs download untar failure
103,0,data source file formats park xml based base relation use shadooprddnewhadooprdd input file block holder work python udf method reproduce running following codes bin py spark packages com data bricks spark xml input file block holder work python udf data source file format
104,0,redirect handler started http port ssl enabled redirects root server additional handlers go handler deeplink non https server redirected http sport tested history server normal ui fix ssl redirect handler redirects serverroot
105,1,currently sql implement overwrites calling fs delete directly original data ideal since original file send deleted even job aborts extend commit protocol allow file overwrites managed well add delete with job hook internal commit protocol api
106,0,mlr application fails spark yarn cluster modem lr example fails yarn cluster mode due lacks e package
107,1,spark support aes encryption added spark network library authentication different spark processes still performed using sasl digest md mechanism means authentication part weakest link since aes keys currently encrypted using des strongest cipher supported sasl spark really claim provide full benefits using a es encryption add new auth protocol need disclaimers aes based authentication mechanisms park
108,0,glm family gamma data dy error r backend handler fit org apache spark mlr generalized linear regression wrapper failed java lang reflect invocation target exception sun reflect native method access or impl invoke native method sun reflect native method access or impl invoke native method access or impl java sun reflect delegating method access or impl invoke delegating method access or impl java java lang reflect method invoke method java org apache spark api rr backend handler handle method call r backend handler scala org apache spark api rr backend handler channel read r backend handler scala org apache spark api rr backend handler channel read r backend handler scala i one tty channel simple channel inbound handler channel read simple channel inbound handler java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java i one tty handler timeout idle state handler channel read idle state handler java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java i one tty handler codec message to message decoder channel read message to message decoder java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java i one tty handler codec byte to message decoder fire channel read byte to message decoder java i one tty handler codec byte to message decoder channel read byte to message decoder java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java i one tty channel default channel pipeline head context channel read default channel pipeline java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel default channel pipeline fire channel read default channel pipeline java i one tty channel nio abstract nio byte channel nio byte unsafe read abstract nio byte channel java i one tty channel nio nio event loop process selected key nio event loop java i one tty channel nio nio event loop process selected keys optimized nio event loop java i one tty channel nio nio event loop process selected keys nio event loop java i one tty channel nio nio event loop run nio event loop java i one tty util concurrent single thread event executor run single thread event executor java i one tty util concurrent default thread factory default runnable decorator run default thread factory java java lang thread run thread java caused java lang illegalargumentexception gl me cdf parameter family given invalid value gamma org apache spark m lparam param validate params scala org apache spark m lparam param pair params scala org apache spark m lparam param minus greater params scala org apache spark m lparam params class set params scala org apache spark ml pipeline stage set pipeline scala org apache spark ml regression generalized linear regression set family generalized linear regression scala org apache spark mlr generalized linear regression wrapper fit generalized linear regression wrapper scala org apache spark mlr generalized linear regression wrapper fit generalized linear regression wrapper scala error handle errors return status conn java lang illegalargumentexception gl me cdf parameter family given invalid value gamma org apache spark m lparam param validate params scala org apache spark m lparam param pair params scala org apache spark m lparam param minus greater params scala org apache spark m lparam params class set params scala org apache spark ml pipeline stage set pipeline scala org apache spark ml regression generalized linear regression set family generalized linear regression scala org apache spark mlr generalized linear regression wrapper fit generalized linear regression wrapper scala org apache spark mlr generalized linear regression wrapper fit generalized linear regression wrapper scala sun reflect native method access or impl invoke native method sun reflect native method access or impl invoke native method access or impl java sun reflect delegating method access or impl invoke delegating method access or impl java spark r glm gamma family results error
109,0,may be partition match something wrong partition value set empty string alter table tablename drop partition empty string drop whole table
110,0,returned result empty table loading refresh metadata cache loading datatable returned empty result loading hive table
111,0,tests failed windows via app vey or due problem problem line https git hub com apache sparkblobcebcsqlhivesrc main scala org apache spark sql hive execution script transformations cal al always assume bash located bin bash cases installing cygwin use bash cmd using bash windows located script transformation work windows due fixed bash executable location
112,0,adding distributed lda model training summary spark r found log prior original loaded model different example test readwrite distributed lda model add test val log prior model as instance of distributed lda model log prior val log prior model as instance of distributed lda model log prior assert log prior log prior test fails equal distributed lda model returns different log prior original loaded model
113,0,see reproduction https data bricks prod cloud front cloud data bricks com public ececeaaafbcfc latest html consider following cached tables used subquery expression
114,0,config spark sql files ignore corrupt files used ignore corrupt files reading files sql currently ignore corrupt files config two issues work parquet ignore corrupt files files can rdd actually begin read files early inferring data schema files corrupt files read schema fail program related issue reported http apache spark developers list n nabble com skip corrupted parquet blocks footer tc html files can rdd assume begin read files starting consume iterator however possibly files read case ignore corrupt files config work config ignore corrupt files work parquet
115,0,met problem like https issues apache org jira browse spark parameter map evaluate function return map public map evaluate text url runs park sql udf getting following exceptions cala match error interface java util map class java lang class org apache spark sql hive hive inspectors class java class to datatype hive inspectors scala org apache spark sql hive hive simple udf java class to datatype hive udfs scala org apache spark sql hive hive simple udf datatype lzycomputehiveudfsscala org apache spark sql hive hive simple udf datatype hive udfs scala org apache spark sql catalyst expressions alias to attribute named expressions scala org apache spark sql catalyst plans logical project an on fun output apply basic operators scala org apache spark sql catalyst plans logical project an on fun output apply basic operators scala scala collection traversable like a non fun map apply traversable like scala scala collection traversable like a non fun map apply traversable like scala scala collection mutable resizable array class for each resizable array scala scala collection mutable array buffer for each array buffer scala scala collection traversable like class map traversable like scala scala collection abstract traversable map traversable scala org apache spark sql catalyst plans logical project output basic operators scala org apache spark sql catalyst plans logical insert into table resolved lz y compute basic operators scala spark sql use hive udf throw exception return map value
116,0,update structured streaming programming guide update mode
117,0,bug caused fix spark https git hub com apache spark pull reproduced adding following test predicate suite scala consistently fail val value non foldable literal double positive infinity double type check evaluation value list value true bug causing org apache spark sql catalyst expressions predicate suite fail approximately time fails any time value infinity infinity correct answer true eg http sampl abcs berkeley edu jenkins jobs park pull request builder test report org apache spark sql catalyst expressions predicate suite http sampl abcs berkeley edu jenkins jobs park pull request builder console catalyst always returns false infinity
118,0,spark ldap ass optimizer em online backend however lda wrapper set optimizer based valuer therefore optimizer em is distributed field false true addition summary method bring back results related distributed lda models park rld a set optimizer correctly
119,0,right use drop duplicates stream get exception attribute replacement broken example drop duplicates uses expression id alias attribute breaks attribute replacement
120,1,remove supports partial flag aggregate function
121,0,looks like bug introduced spark preventing read data parquet table hive support enabled whose name starts underscore create insert statements table instead seems work expected problem reproduced spark shell following steps create table values scala spark sql create table int using parquet show scala spark sql insert values show select data created filled table results scala spark sql select show rename table prefixing underscore disappears scala spark sql alter table rename show select data renamed table results shown scala spark sql select show unable retrieve data parquet table whose name starts underscore
122,0,fix event time watermark suite delay months years handled correctly
123,0,data managed table deleted table dropped however partition location location partitioned table deleted expected users specify location partition adding partition managed partitioned table in memory catalog user specified partition location deleted table dropping
124,1,remove useless database name simple catalog relation remove database name simple catalog relation
125,0,currently py spark work python running bin py spark simply throws error works fine seems properly sethi jected one py spark work python
126,0,putting one column query may return correctly null data demonstrate problem following data set query subquery one column may return incorrect results
127,0,using view name fist char numerical value data frame create or replace temp view view name string causes create or replace temp view throws org apache spark sql catalyst parser parseexception view name firstchar numerical
128,0,add doc streaming rest api
129,0,https git hub com apache spark pull merged unable build intellij got following compilation error unable build compiles park intellij due missing scala deps spark tags
130,1,since spark sql hive thrift server single session configuration sql component conf moved spark conf static sql conf introduced spark sql hive thrift server single session sql configuration modified different sessions later static sql configuration added perfect fit spark sql hive thrift server single session previously moves park sql warehouse dirs park conf static sql conf moves park sql hive thrift server single session sql conf
131,1,right context cleaner reference buffer concurrent linked queue time complexity remove action n changed use concurrent hashmap whose remove change context cleaner reference buffer concurrent hashmap make faster
132,1,sort partitions redistributed at a logical operators actually used removed note sort operator global flag false subsumed sort partitions remove sort partitions redistributed at a
133,0,https spark tests app spot com test details suite name org apache spark streaming basic operations suite test name rdd cleanup map window fix flaky test streaming basic operations suite rdd cleanup map window
134,1,recently hit bug com thoughtworks para name r para name r causes jackson fail handle byte array defined case class find https git hub com faster xml jackson module scala issues suggests caused bug parana merlet upgrade para name r since using jackson jackson module para name ruse com thoughtworks para name r para name r suggests upgrade para name r upgrade com thoughtworks para name r para name r
135,1,currently sql interface recovering partitions directory table update catalog msc k repair table alter table table recover partitions actually hard remember msc k clue means new scalable partition handling table repair becomes much important making visible data created data source partitioned tabled esri able add catalog interface users repair table add recover partitions api catalog
136,0,logical plan fails create eg source options invalid user can not use code detect failure place receiving error thread uncaught exception handler bug logical plan lazy try create streaming query exception wrap exception thrown creating logical plan calls logical plan again shard user see failure stream execution fails create logical plan
137,0,current implementation spark streaming considers batch completed matter results jobs https git hub com apache sparkblobdbbcefebbabcc streaming src main scala org apache spark streaming scheduler job schedulers cal all et consider following case micro batch contains jobs read two different kafka topics respectively one jobs failed due problem user defined logic one finished successfully main threads park streaming application execute line mentioned another thread checkpoint writer make checkpoint file immediately line executed due current error handling mechanisms park streaming streamingcontext closed https git hub com apache sparkblobdbbcefebbabcc streaming src main scala org apache spark streaming scheduler job scheduler scala l user recovers checkpoint file job set containing failed job removed taken completed checkpoint constructed data processed failed job would never reprocessed might missed something checkpoint thread handle job completion potential bug potential issue semantics batch completed
138,0,currently spark r tests r runtest ssh succeeds tests park sqlr clean test table people result test data accumulated every run test cases fail following failure result second run fix spark r sql test drop test table
139,0,internally use calendar interval parse delay non determin stic intervals like month year handled way generated delay milliseconds delay threshold months years event time watermark delay threshold specified months years gives incorrect results
140,0,reading mentioned csv data even though maximum decimal precision following exception thrown java lang illegalargumentexception requirement failed decimal precision exceeds max precision decimal unable read given csv data excep ion java lang illegalargumentexception requirement failed decimal precision exceeds max precision
141,0,unsupported operations checking dont check whether aggregation expression is distinct true streaming df group by agg count distinct key gives incorrect results distinct aggregates give in correct answers streaming data frames
142,0,smile gator found following query raise syntax error note group clause commented could multiple values ccv value avg ccv output non aggregate expressions without group subquery yield error
143,1,spark rml libr getting bigger add ml wrappers like split multiple files make us easy maintain ml lib classification rml lib regression rml lib clustering rml lib feature rml lib classification rml lib regression rml lib clustering rml lib features rr convention prefer first way sure whether r supports second organized way check later please let know preference think start new release cycle good opportunity since involves less conflicts proposal approved work cc felix cheung joseph kb mengxrsplitsparkrmllibr multiple files
144,0,trying run sqlquery spark cluster extracting around million records spark sql thrift server interface query works fine spark version however spark thrift server hangs fetching data partitions using incremental collect mode partitions per documentation max memory taken thrift server required biggest data partition observed thrift server releasing old partitions memory whenever gc occurs even though moved next partition data fetches case version investigation found spark execute statement operation scala modified spark sql fix spark sql thrift server fetch results bug result set iterator duplicated keep reference first set vali trait r biter duplicate iter header i traiter it rb suspect resulting memory cleared gc confirm created iterator test class fetched data without duplicating second time creating duplicate could see first instance ran fine fetched entire dataset second instance driver hanged fetching data partitions spark sql thrift server hangs extracting huge data volumes incremental collect mode
145,0,bug introduced subquery handling generate tree string numbers trees including inner children used print subqueries getnode numbered ignores result getnode numbered always correct repro output looks like note project node getnode numbered ignores inner child result returns wrong one getnode numbered generate tree string consistent
146,0,currently define statistics unary node issues aggressively underestimate size project assume array map elements overestimate user projects single field deeply nested field would lead huge underestimation safer sane default probably property unary node propagate statistics way property project project unary node way aggressive estimating statistics
147,1,make stream execution progress classes serializable easy get captured normal usage make stream execution progress classes serializable
148,1,expose event time time stats streaming query progress
149,0,reproduce nosuchelementexception throw since spark broadcast can not cache memory reason change can not cover unrolled has next next function can n read broadcast broadcast blocks stored disk
150,1,starting stream lot backfill max files per trigger user could often want start recent files first would let keep low latency recent data slowly backfill historical data better add option control behavior makefile streamable start recent files
151,1,implement wrappers park r support bisecting k means bisecting k means wrappers park r
152,0,reproduce standalone mode launch spark master launch spark shell point executor associated application launch slave executor assigned spark shell however link stdout stderr executor page please see https issues apache org jira secure attachment screenshot png executor page fails show log links executors added app launched
153,0,seems check analysis rule introduced spark incorrect rejecting tp cds query ran fines park seem obvious error query check rule though plan scalar subquery condition field scalar subquery cs items k items k reference cs items k nonetheless check analysis complains cs items k referenced scalar subquery predicates analysis error check analysis rejects tp cds query
154,0,publish source archives spark r package rcs nightly snapshot builds one problems still remains installs park work looks finals park version present apache download mirrors spark r installs park work rcs snapshots
155,1,spark logit added need updates park vignettes reflect changes parts park rq a work updates park log its park r vignettes
156,1,currently users use python udf filter batch eval python always generated filter exec however predicates need evaluated python udf execution thus push predicates batch eval python push filter batch eval python
157,0,currently rely file format implementations override tostring order get proper explain output better depend short name provide consistent format output file formats
158,1,informal poll bunch users found name clear rename recent progresses recent progress
159,0,found inconsistent behavior using parquet inconsistent behavior writing parquet files
160,0,spark context stop called utils try or stops park context following three places cause deadlocks top method needs wait thread running stop exit context cleaner keep cleaning live listener bus listener thread run task scheduler impl start deadlocks park context stop called utils try or stops park context
161,0,running sql queries large data sets job fails stack overflow warning shows requesting lots executors looks like limit number executors even upper bound based yarn available resources notice error yarn trying request executor containers whereas available cores driver requesting executor high exception fixed spark able control number executor throw stack over slow
162,0,running query decreased executor memory using gb executors instead gb tb parquet database using spark master dated gave index out of bounds exception query follows likely integer overflow issue java lang index out of bounds exception running query spark sql tb
163,1,otherwise threads can not query content memory sink data frame collect takes long time finish memory sink call data frame collect holding lock
164,0,datasets crash compile exception mapping immutable scala map
165,0,aic calculation binomial glm seems wrong weights result different r current implementation fix wrong aic calculation binomial glm
166,1,many spark developers often want test run time function interactive debugging testing really useful simple spark time method test run times park session time simple timer function
167,0,bug branch think rc generation aggregator result npe generated specific mutable projection aggregator
168,0,converting rdd float type fields park data frame integer type long type schema fields park silently convert field values nulls instead throwing error like long type accept object type however seems fixed spark following example make problem clear purposes computation map partitions spark data frame partition converting pandas data frame computations pandas data frame return value list lists converted rdd returned map partitions partitions rdd converted spark data frame similar example using sql context create data framerd d schema rdd column converted long types park data frame since missing values float types park tries create data frame converts values column nulls instead throwing error type mismatch automatic null conversion bug instead throwing error creating spark data rame incompatible types fields
169,0,files directories generated three inserts against shive table ideally drop created staging files temporary data files insert ct as temporary files directories could accumulate lot issue many inserts since insert gene rats least six files could eat lot spaces slow jvm termination insertion cta shive tables staging directories data files dropped normal termination jvm
170,1,currently function input file name getpath input file functions get block start offset length patch introduces two functions input file block start returns file block start offset available input file block length returns file block length available input file block start input file block length function
171,0,poisson glm fails many standard data sets issue incorrect initialization leading almost zero probability weights following simple example reproduces error create pr poisson glm fails due wrong initialization
172,0,bump master branch version snapshot
173,0,local spark instance built hive support pyarnphadoopdhadoop version phi vep hive thrift server following script sequence work spy spark without error x fails x error produced error goes away sql context replaced sql context last error line since sql context class preserved backward compatibility changes x break scripts notebooks follow pattern calls used run fine x backward compatibility creating data frame new sql context object fails derby error
174,0,current json path parser fail sparse expressions like key used named expressions spaces json path implementation fail sparse key
175,0,improve error message using join
176,0,able restart streaming queries across spark version already made logs offset log file source log files ink log use json added tests actual json files spark incompatible changes reading logs immediately caught add tests ensure stability structured streaming log formats
177,0,update apache docs regard watermarking structured streaming
178,0,input file name return filename empty string instead used input udf py spark data seem spy spark specific issue input filename function work udf
179,0,spark changes event log format structured streaming make sure changes break history server ignore structured streaming logs history server
180,0,start thrift servers h reproduces park daemons h arguments error lead throws unrecognized option
181,0,installs park r source package ie rcmd installs park rtargzstartsparkrsparkr shell library spark rs park r session notices park r hangs find spark submit launch jvm backends park r running package previously downloaded spark jar able run without sets park home basically bug autoinstall spark work first session seems regression earlier behaviors park r hangs session start installed package without spark home set
182,0,weird issues exploding python udfs spark sql cases based data type exploded column result flat wrong corrupt seems like something bad happening telling tungsten schema rows applying udf please check code reproduction notebook https data bricks prod cloud front cloud data bricks com public ececeaaafbcfc latest html corruption correctness issues exploding python udfs
183,1,aggregate function currently implements implicit cast input types enables implicit input type casting lead unexpected results enabled suitable function hand aggregate function implicit cast input types
184,0,key points produce issue union clauses one column sum aggregate one union clause integer type union clause another column different date types union clauses reason issue step apply type coercion widen set operation types add project cast since union clauses different data types one column union clauses inner union clause also projected cast step apply type coercion function argument conversion return type sum int extended bigint meaning one column union clauses changed datatype step apply type coercion widen set operation types another cast project added inner union clause since sum int datatype changed point reference project inner union missed since project inner union newly added see analyzed logical plan solutions fix since set operation type coercion applied inner clauses tabled apply widen set operation types last fix issue avoiding multilevel projects set operation clause handle existing cast project carefully widen set operation types also work appreciate comments missing reference multi union clauses cause type coercion
185,0,recently added collapse window optimizer rule changes column order attributes actually modifies schema logical plan optimization breaks collect subtle way bind row encoder output logical plan optimized plan example following code collapse window optimizer rule changes column order
186,1,moved trf gbt param setter methods subclasses deprecate methods model classes make java friendly see discussion https git hub com apache spark pull discussion r moved trf gbt param setter methods subclasses
187,1,includes park distribution built source packages park r enable help vignettes package used also source package would release cran r include package vignettes help pages build source packages park distribution
188,0,smells like another optimizer bug similar sparks park seeing master commit fbbbeaabefdfbfa minimal repro yet error seeing note error ends park tries print physical plan scrubbed project fields plan simplify display scrubbed anything think important let know get around problem adding persist right operation fails failing operation filter clues boil minimal repro clues problem persist resolves java lang runtimeexception invalid python udf requires attributes one child
189,0,full outer join correlated predicate left operand subquery may return incorrect results example full outer join correlated subquery returns incorrect results
190,1,remove overwrite options
191,0,hyper log log plus plus relative error small p cause array index out of bounds exception thresholds p check pp regress original hll result use small range correction use fix hll small relative error
192,0,task memory manager memory leak detector gets called task completion callback checks whether memory released released time callback invoked task memory manager releases current error message says something like following practice multiple reasons triggered normal code path eg limit task failures fact messageslog means leak fixed task memory manager confuse users downgrade message warning debuglevel avoid using word leak since actually leak downgrade memory leak warning message
193,0,manly issue clarified following example given dataset scala data show b theoretically call na fill nothing change current results cala data na fill show bna fill miss original values long integers
194,0,due bug task scheduler impl complete sudden loss executor may cause task set manager leaked causing shuffle dependencies data structures kept alive indefinitely leading various types resource leaks including shuffle file leaks nutshell problem task scheduler impl maintain mapping executor id running tas kids leaving unable clean task id task set manager maps executor totally lost executor loss may cause task set manager leaked
195,0,merging algorithm unsafe shuffle writer consider encryption tries merge encrypted files result data can not read since data encrypted different initial vectors interleaved partition data leads exceptions trying read files shuffle internal branch worry lines necessarily match unsafe shuffle writer corrupts encrypted shuffle files merging
196,0,can not filter nonexisting column parquet file
197,0,two data frame reader jdbc apis ignore user specified parameters parallelism degree concurrent fetching data frame reader jdbc apis work
198,1,trying monitoring streaming application using spark rest interface found api streaming let us choice implement one our self api cover except ly amount information get web interface implementation base current rest implementations park core available running applications use endpoint root streaming api v endpoint meaning statisticsstatistics information stream receivers list receiver streams receivers stream id details given receiver stream batches list retained batches batches batch id details given batch batches batch id operations list output operations given batch batches batch id operations operation id details given operation given batch add rest apis park streaming
199,0,spark user may provide sensitive information spark configuration property source environment variable executor driver environment contains sensitive information good example would reading writing data using spark secret accesskey placed had oop credential provider https had oop apache org docs current had oop project dist had oop common credential provider api html however one still needs provide password credential providers park typically supplied environment variable driver executor environments environment variable shows logs may also show ui logs shows places event logs spark listener environment update event b yarn logs printing executor launch context ui would show environment tab redacted contains words password secret magic words hardcoded https git hub com apache spark blobcddaabfbdeceecoresrc main scala org apache spark ui env environment pages cal al hence customizable jira track work make sure sensitive information redacted log suis spark still passed relevant places needs get passed redact sensitive information spark log sui
200,0,timeout inherit runtimeexception fatal error timeouts assertion errors
201,0,using limit data frame prior group by lead crash repartitioning avoid crash crash df limit group by userid count show work df limit coalesce group by userid count show work df limit repartition userid group by userid count show reproducible example along error message quoted f spark create data frame userid genre i ddf show userid genre i ddf group by userid count show userid count df limit group by userid count show stage warn task set manager lost task stage tidlvsphdnstubprodcom java lang nullpointerexception org apache spark sql catalyst expressions generated class generated iterator agg do aggregate with keys unknown source org apache spark sql catalyst expressions generated class generated iterator process next unknown source org apache spark sql execution buffered row iterator has next buffered row iterator java org apache spark sql execution whole stage codegen exe canon fun a non has next whole stage codegen exec scala org apache spark sql executions park plan a non fun apply spark plan scala org apache spark sql executions park plan a non fun apply spark plan scala org apache spark rdd rd dan on fun map partitions internal a non fun apply apply rdd scala org apache spark rdd rd dan on fun map partitions internal a non fun apply apply rdd scala org apache spark rdd map partitions rdd compute map partitions rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark scheduler result task run task result task scala org apache spark scheduler task run task scala org apache spark executor executor task runner run executor scala java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java quote limit group by leads java lang nullpointerexception
202,0,run memory heavy spark jobs park driver may consume memory resources host available provide case oom killer comes scene successfully kills spark submit process py sparks park context able handle state things becomes completely broken can not stop stop tries calls top method bounded java context jsc fail spy j error process longer exists like connection can not start news park context broken one active one py spark still able spark context sort singleton thing shutdown ipython notebook start dives park context internal attributes reset manually initial none state oom killer case one many reasons park submit crash middle something leaves spark context broken state example error log sc stop broken state oom killer may leaves park context broken state causing connection refused errors
203,0,record watermark persistent log recover ensure determinism record recover watermark
204,0,current documentation datediff make clear one start date end date example also wrong direction fix documentation datediff
205,0,scalar subquery extra group columns returning incorrect result
206,0,spark generalized linear regression handle collinear data since underlying weighted least squares solved local l bfgs rather normal spark r wrappers park glm throw errors fitting collinear data depth study error found caused standard error coefficients value p value available underlying weighted least squares solved local l bfgs coefficients matrix generated failed spark rs park glm error collinear data
207,0,pretty standard stream call write stream for each start get for each sink fails assertion failed plan event time watermark
208,1,two methods added scala datasets available python yet add with watermark checkpoint python data frame
209,0,stumbled on to corner case inner join appears return incorrect results believe join be have identity instead values shuffled around plain wrong reproduced follows joining correctness issue inner join result window functions
210,1,refactor static invoke invoke new instance introduce invoke like extract common logic static invoke invoke new instance prepare arguments remove unneeded null checking fix null ability new instance modify short circuit arguments null pro page te null true refactor static invoke invoke new instance
211,0,old table created without providing schema seems branch fail load says schema corrupts park sql debug enabled getmetadata using describe formatted spark sql fails load tables created without providing schema
212,0,following test fails classcastexception due oddities jackson object mapping works breaking sql tab history servers park listener driver accum updates event deserialize properly history server
213,0,running query tb parquet database using spark master dated dump cores spark executors query tp cds query though query one produce core dump easiest one create errors park output showed exception easily reproducible smaller data volumes egt btb easily reproducible tb look data types may big enough handle hundreds billion core dumped running spark sql large data volume tb
214,0,using jdbc data source is in function generates invalid sql syntax called empty array causes jdbc driver throw exception array empty works fine example assume source connection sql driver table correctly defined is in causing sql syntax error jdbc
215,0,exception invocation exception function lookup return useless confusing error message example returned message null hitting invocation exception function lookup
216,0,currently cte used runnable command analyzer replace logical node child plan runnable command resolved however output plan node looks confusing example weird plan output cte used runnable command
217,0,spark random forest classification throws exception training libs vm data reproduced following error caused label column r formula already exists force index label however must index label classification algorithms need renamer formula label col new value index original label issue also appears algorithms spark naive bayes spark glm binomial family spark gbt classifications park rs park random forest classification throws exception training libs vm data
218,0,assertion https git hub com apache spark blobeaaddaedbebaaesql core src main scala org apache spark sql execution streaming stream execution scala l fails run stream json data stored partitioned folders manually specify schema schema omits partitioned columns hunch inferring columns even though schema passed manually adding end fixing bug would nice make assertion better truncating terribly useful least case truncated interesting part changed debugging also tried specifying partition columns schema appears filled corrupted data inferred partition columns cause assertionerror
219,0,testsuite fails occasionally jenkins due oom errors already reproduced locally figured root cause probably disable temporarily getting fixed break pr build often object hash aggregate suite flaky occasional oom errors
220,0,spark fixes regexp replace serialized one reviews requested updating tests expressions tested using check evaluation first serialized caused several new test failures issue add serialization tests pick commit https git hub com apache spark commit fix bugs serialization exposes test expressions serialized
221,0,steps reproduce launch spark shell run following scala code via spark shells ca laval hive sample table df sql context table hive sample table scala import org apache spark sql data frame writers ca laval dfw data frame writer hive sample tabled fwrite scala sql context sql create table exists hive sample table copy py clientid string query time string market string device platform string device make string device model string state string country string query dwell time double session id bigint session page view order bigint scala dfw insert into hive sample table copy py scala val hive sample table copy py dfdf sql context sql select clientid query time device platform query dwell time hive sample table copy py state washington device make microsoft query dwell time hive sample table copy pydfdfshowhdfscasewasb see following folders hive warehouse hive sample table copy py hive staging hive hive warehouse hive sample table copy py hive staging hive ext hive warehouse hive sample table copy py hive staging hive issue get cleaned get accumulated customer tried setting set hive exec staging dirt mph ive hive site xml make difference hive staging folders created folder hive warehouse hive sample table copy py tried adding property hive site xml restart components hive exec staging dir hive exec scratch dir username staging new hive staging folder created hive warehouse folder moreover please understand run hive query pure hive via hive cli spark cluster see behavior appear hive issue behavior cases park behavior checked am bar is park yarn preserve staging files false spark configuration already issue happens via spark submit well customer used following command reproduces park submit test hive staging cleanup py hive staging folders created spark hive context getting cleaned
222,0,query fails nullpointerexception line https git hub com apache spark blob master sql catalyst src main scala org apache spark sql catalyst expressions regexp expressions scala l problem pos explode causing regexp replace serialized instantiated null value transient stringbuffer hold result fix make result value lazy regular expression replace throws nullpointerexception serialized
223,1,spark csv file format data source uses inefficient methods reading files schema inference benefit file listing io performance improvements made spark order fix performance problem implement read paths terms text file format use text file format implementation csv file format
224,1,today could access rdd local checkpoint py spark important issue machine learning people often iterate algorithms perform operations like joins iteration lineage truncated memory usage lineage computation time exploder dd local checkpoint seems like straightforward way truncating lineage python api expose expose rdd local checkpoint py spark
225,0,spark broke files archives options check null instead localized path null throw new illegalargumentexception attempt add file multiple times distributed cache yarn files archives broke
226,0,discovered fair bit consistency documentation summary functions eg instance listed return value name phrase list items longer description mean reference links cala doc need review model summary implementations ml libr updater api documentation ml model summary
227,0,test case initialization order mavens bt different maven always creates instances test cases run together fails object hash aggregate suite randomized test cases register temporary hive function right creating test case cleared initializing successive test cases sbt fine since created test case executed immediately creating temporary function fix issue put initialization destruction code before all after all object hash aggregate suite fails maven builds
228,0,trying reproduces park sql found following sqlquery fails spark via spark beeline cass cast exception classcastexception count distinct
229,1,think undocumented naming convention call expression unit tests expressions suite end end tests functions suite great make testsuites consistent naming convention use consistent naming expression testsuites
230,1,hash scala getting pretty long obvious hash expressions belong creating hash scala put hash expressions move hash expressions misc scala hash scala
231,0,run example straight apidocs to local iterator gives time exception to local iterator yields time error py spark
232,0,standalone scheduler backend dead called rpc thread calls park context stop threads park context stop block rpc threads exit called insider pc thread deadlock potential deadlock standalone scheduler backend dead
233,0,string indexer model get collected gc java even deleted python reproduced issue similar spark probably fixed calling jvm detach model destructor implemented py spark mlib common java model wrapper missing py spark ml wrapper java wrapper models ml package may also affected memory leak memory leak py spark string indexer
234,0,schema nullable column thrown java lang numberformatexception null data deli meter specified csv specifying schema reason csv line parsed map index safe tokens short one value index safe tokens index throws nullpointerexception reading optional value map nullpointerexception given csv typecast cast to datum string datum value subsequent numberformatexception thrown due fact nullpointerexception can not cast type possible fix use provided schema parse line correct number columns since nullable implement try catch csv relation csv parser index safe tokens index numberformatexception reading csv nullable column
235,0,file stress suite report errors occur improve error reporting file stress suite streaming
236,0,dunno misinterpreting something seems like bug udfs work interface optimizer basic reproduction using length udf illustration could udf accesses fields aliased looks like second execution plan batch eval python somehow get sun aliased column names whereas project right gets aliased names udfs see aliased column names
237,0,running runtimeexception dataset holding empty object instance option type holding non nullable field instance following case class case class data rowid int value string dataset option datarow hold datarow objects can not hold empty following exception thrown bug reproduce using program httpsgistgithubcomaniket bhatnagar edfdefecafaaedatasetapi runtimeexception null value appeared non nullable field holding option case class
238,0,hive exec staging dir effects park relevant https issues apache org jira browse spark hive exec staging dir effects park
239,0,error message classcastexception occurs using select query orc file
240,1,column expr private sql actually really useful field debugging open similar use query execution make column expr public
241,0,running spark job see job fails executor oom following stack trace code trying reuse bytes to bytes map spilling calling reset function see https git hub com facebook fbs park blob fb core src main java org apache spark unsafe map bytes to bytes map java l reset function releasing memory pages reseting pointer array pointer arraysize grown beyond fair share bytes to bytes map allocated memory page hence oom executor oom due memory leak bytes to bytes map
242,0,wide data frames contain nested data structures explode one data frames include records empty nested structure outer explode supported create similar data frame null values union together see spark details hoping spark going address issue asked lwl in open jira attach class org apache spark sql catalyst expressions generated class specific unsafe projection grows beyond kb
243,0,running graph x triangle count large ish file results invalid initial capacity error running spark tested spark see results httpbitlyeqkwdn running runs wells park graph frames http bit lyf as w reference stack overflow questions park graph x requirement failed invalid initial capacity http stack overflow com questions spark graph x requirement failed invalid initial capacity graph x invalid initial capacity running triangle count
244,0,run following spark says task serializable task serializable group by keymap groups map
245,0,since spark following py spark snippet fails analysis exception second argument first boolean literal restricted python similar analysis exception first last aggregation
246,0,current implementation poisson glm seems allow positive values see correct since support poisson includes origin override def initialize double weight double double require color red color response variable poisson family positive got fix easy change require color red color response variable poisson family generalized linear regression wrong value range poisson distribution
247,0,following command fails sparks park files spark jars passed driver yarn mode
248,0,following error message points random column actually using query making hard diagnose ccm arm brus misleading error message aggregation without window group by
249,0,purpose trying breaks park sql codegen uncover potential issues creating arbitrate ly complex data structures using primitives strings basic collections map seq option tuples case classes first example nested case classes gen base mutable projection target mutable row row info mutable row row info return info info info provide immutable access last projected row info public internal row current value info return internal row mutable row info info info public java lang object apply java lang object info internal row internal row info info info info object obj expression references eval null info org apache spark sql expressions aggregator value org apache spark sql expressions aggregator obj info info boolean is null is null at info utf string value is null null get utf string info info boolean is null is null info final java langstring value is null null java langstring value tostring info is null value null info boolean is null false is null info final com tres at a spark sql struct value is null null com tres at a spark sql struct value finish value info is null value null info info boolean is null false info internal row value null info false is null info info final internal row value null info is null true info value value info else info info boolean is null false info values new object info is null info throw new runtimeexception errmsg info info info boolean is null false info final com tres at a spark sql struct value is null null com tres at a spark sql struct value info is null value null info boolean is null false info internal row value null info false is null info info final internal row value null info is null true info value value info else info info boolean is null false info values new object apply info apply info final internal row value new org apache spark sql catalyst expressions generic internal row values info values null info is null is null info value value infoinfo is null info values null info else info values value infoinfo is null info throw new runtimeexception errmsg info info info boolean is null false info final com tres at a spark sql struct value is null null com tres at a spark sql struct value b info is null value null info boolean is null false info internal row value null info false is null info info final internal row value null info is null true info value value info else info info boolean is null false info values new object apply info apply info final internal row value new org apache spark sql catalyst expressions generic internal row values info values null info is null is null info value value infoinfo is null info values null info else info values value info info final internal row value new org apache spark sql catalyst expressions generic internal row values info values null info is null is null info value value infoinfo is null is null info value value info info copy results mutable row infoinfo is null info mutable row update value info else info mutable rowset null at info info info return mutable row info info no format broken spark sql codegen
250,0,create table name text id integer insert values mike val df sql context read jdbc jdbc url new properties df filter id show df filter id show error cause org postgresql util p sqlexception error column id exist position org postgresql core v query executor impl receive error response query executor impl java org postgresql core v query executor impl process results query executor impl java org postgresql core v query executor impl executequery executor impl java org postgresql jdbc pg statement execute pg statement java org postgresql jdbc pg statement execute with flags pg statement java org postgresql jdbc pg statement executequery pg statement java org apache spark sql execution data sources jdbc jdbc rdd compute jdbc rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark rdd map partitions rdd compute map partitions rdd scala working fix issue submit pr soon jdbc data source read fails quoted columns eg mixed case reserved words source table used filter
251,1,plan mark explicit spark support aforementioned environments deprecated remove supports park also see mailing list discussion http apache spark developers list n nabble com straw poll dropping support things like scala tpp html officially deprecate support python java scala
252,0,run sql distinct spark git hub master branch throw unresolved exception example run test cases park branch master sql rewrite distinct aggregates unresolved exception u daf foldable type check
253,0,error src main java org apache spark util collection unsafe sort unsafe external sorter java sizes line length line longer characters found warning checkstyle check violations detected fail on violation set false spark branch spark release publish failed style check failed
254,0,code logic looks like spark generated code causes compile exception group by key reduce groups map used
255,1,whenever aggregate data event time want consider data late order terms event time since keep aggregate keyed time state state grow unbounded keep around old aggregates attempt consider arbitrarily late data since state store memory prevent building unbounded state hence need watermarking mechanism mark data older beyond threshold late stop updating aggregates would allow us remove old aggregates never going updated thus bounding size state design doc https docs googlecom document zpazsvraazvmyhuxwqanqlzl isxhkfcqeditusp sharing observed delay based event time watermarks
256,0,querying global temp view throws table view found exception hive support enabled test case reproduce problem test needs run hive support enabled cause hive session catalog lookup relation check global temp views unable query global temp views hive support enabled
257,0,conf option value enclosed example multivalue config like quotes park driver extra java options dlog j configuration file conf log jserver properties dal lux io user file write type default quote without next all ux io config treated spark submit options cause error mesos cluster scheduler generate bad command options
258,0,executors ends ask permission to commit output driver failed retry another sending driver receives ask permission to commit output messages handles executor ignores first response true receives second response false task attempt number partition authorized committers by stage locked forever driver enters infinite loop h driver log sending ask permission to commit output failed driver enter task dead loop
259,0,multiple records minimum value answer approximate percentile wrong suppose table records partitions values column col partitions query percentile approx col array current answer far correct answer test case wrong approximate percentile answer multiple records minimum value
260,0,find insert overwrite statement running spark sql spark shell spends much time hive client start apache hive bin bin hives park costs ten minutes hive client costs less seconds steps took test sql insert overwrite table login game partition pt mix end t select distinct account name role id server rec date mix platform mix pid mix devtbllogloginptmixendt lines data tbl log login partition pt mix end tps sure must insert overwrite costing lot times park may overwrite need spend lot time io something else also compare executing time insert overwrite statement insert statement insert overwrite statement insert statements park insert overwrite statement costs minutes insert statement costs seconds insert statements park insert statement hive clients park costs seconds hive client costs seconds difference little ignore insert overwrite statement runs much slower spark sql hive client
261,0,recently changes spark handle jar conflict issue uploading distributed cache default yarn client upload files archives assembly hdfs staging folder throw file appears files archives exception know whether uncompress leave file compressed spark distributed cache throw exception file specified dropped files archives
262,0,behavior variables sql shell changed specifically hive varname value set hive varname value longer work queries worked correctly either fail produce unexpected results think regression addressed hives park work like command line args hive conf hive var used set session properties hive conf properties added had oop configuration set add shive configuration property set hive var add shive var hive vars substituted queries name configuration properties substituted using hive conf name hive conf spark conf conf variable prefixes removed value sql conf rest key returned set adds properties session config according comment https git hub com apache spark blob master sql core src main scala org apache spark sql runtime config scala l had oop configuration regression hive variables longer works park
263,0,many parts behavior would unless default locale changed currently default en affects sql datetime functions moment sql function lets user specify language country sentences consistent hive affects dates passed json api affects strings rendered ui potentially although correctness issue may argument letting vary affects bunch instances dates formatted strings things like ids filenames far less likely cause problem worth making consistent occurrences tests downside change also upside behavior depend default jvm locale also affected default jvm locale example wanted parse dates way depended non us locale format string would longer possible means specifying example sql functions parsing dates however controlling globally changing locale exactly great either purpose change make current default behavior deterministic fixed prcomingcchyukjinkwonfix default locale used date format number format locale us
264,0,following spark shell snippet reproduces issue reason treat two struct type incompatible even differ field null ability analysis exception may thrown union two dfs whose struct fields different null ability
265,0,example following throws exception exception thread dag scheduler event loop java lang stack overflow error java lang exception exception java org apache spark serializer serialization debugger serialization debugger visit serializable with writeobject method serialization debugger scala org apache spark serializer serialization debugger serialization debugger visit serializable serialization debugger scala org apache spark serializer serialization debugger serialization debugger visit serialization debugger scala org apache spark serializer serialization debugger serialization debugger visit serializable with writeobject method serialization debugger scala org apache spark serializer serialization debugger serialization debugger visit serializable serialization debugger scala custom partition coalesce r cause serialization exception
266,0,spark enable hive support in it sql context throw already exists exception message database default already exists https www mail archive comdev spark apache org msg html spark enable hive throw already exists exception message database default already exists
267,1,upgrade latest release mima order include fix bug led flakiness mima checks https git hub com typesafe hub migration manager issues upgrade mima
268,1,known complaints cribs history server application list updating quickly enough event log files need replay huge currently fs history provider design causes entire event log file replayed building initial application listing refer method merge application listing file status file status process replay involves line event log read string parsing string json structure converting json corresponding scala classes nested structures particularly part involving parsing string json scala classes expensive tests show majority time spent replay work replay performed building application listing two events code really cares spark listener application starts park listener application end since listener attached replay listener bus point application eventlistener means processing event log file huge number hundreds thousands events work done deserialize event replay needed two events interested used ensure replay performed purpose building application list make effort replay two events others tests show drastically improves application list load time mb event log user events load time local mac comes secs second using approach customers typically execute applications large event logs thus multiple large event logs present speed soon history server ui lists apps considerably updating pull request take fixing remove unneeded heavy work performed fs history provider building application listing ui page
269,0,deploys park thrift server yarn tried execute beeline following commands how databases got error message quote beeline connect jdbc hive localhost connecting jdbc hive localhost info utils supplied authorities localhost info utils resolved authority localhost info hive connection try open client transport jdbc uri jdbc hive localhost connected spark sql version driver hive jdbc versions park transaction isolation transaction repeatable read jdbc hive localhost show databases java lang illegalstateexception overwrite cause java lang classcastexception org apache spark sql catalyst expressions generic internal row can not cast org apache spark sql catalyst expressions unsafe row java lang throwable initcause throwable java org apache hive service cli hive sqlexception to stack trace hive sqlexception java org apache hive service cli hive sqlexception to stack trace hive sqlexception java org apache hive service cli hive sqlexception to cause hive sqlexception java org apache hive service cli hive sqlexception hive sqlexception java org apache hive jdbc utils verify success utils java org apache hive jdbc utils verify success with info utils java org apache hive jdbc hive query result set next hive query result set java org apache hive beeline buffered rows buffered rows java org apache hive beeline beeline print beeline java org apache hive beeline commands execute commands java org apache hive beeline commands sql commands java org apache hive beeline beeline dispatch beeline java org apache hive beeline beeline execute beeline java org apache hive beeline beeline begin beeline java org apache hive beeline beeline main with input redirection beeline java org apache hive beeline beeline main beeline java caused org apache sparks park exception job aborted due stage failure task stage failed times recent failure lost task stage tide dw java lang classcastexception org apache spark sql catalyst expressions generic internal row can not cast org apache spark sql catalyst expressions unsafe row org apache spark sql executions park plan a non fun apply spark plan scala org apache spark sql executions park plan a non fun apply spark plan scala org apache spark rdd rd dan on fun map partitions internal a non fun apply apply rdd scala org apache spark rdd rd dan on fun map partitions internal a non fun apply apply rdd scala org apache spark rdd map partitions rdd compute map partitions rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark scheduler result task run task result task scala org apache spark scheduler task run task scala org apache spark executor executor task runner run executor scala java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java driver stack trace sun reflect native constructor access or impl new instance native method sun reflect native constructor access or impl new instance native constructor access or impl java sun reflect delegating constructor access or impl new instance delegating constructor access or impl java java lang reflect constructor new instance constructor java org apache hive service cli hive sqlexception new instance hive sqlexception java org apache hive service cli hive sqlexception to stack trace hive sqlexception java error error retrieving next row state code quote add jar command also error occurred quote add jar udf jar java lang illegalstateexception overwrite cause java lang classcastexception org apache spark sql catalyst expressions generic internal row can not cast org apache spark sql catalyst expressions unsafe row java lang throwable initcause throwable java quotes park sql thrift error
270,0,rdd zip with index generate wrong result one partition contains int maxvalue records rdd contains partition records error occurs example partition records index became operation repartition coalesce possible generate big partition bug fixed rdd zip with index generate wrong result one partition contains records
271,0,looks like https git hub com apache spark pull broke parquet log output redirection patch querying parquet files written parquet mrs park prints torrent harmless warning messages parquet reader happens execution planning matter log levels park context set regression noted something needed fix follow pr feel responsible going expedite fix suspect pr broke spark parquet log output redirection premise going spark prints avalanche warning messages parquet reading parquet files written older versions parquet mr
272,0,following spark shell snippet creates series query plans grow exponentially th plan created using cached copies th plan significantly affects usability issue fixed introducing checkpoint method dataset truncates query plan lineage underlying rdd query planning slows dramatically large query plans even subtrees cached
273,0,reported similar bug two months ago fixed spark https issues apache org jira browse spark find new bug insert na fill call outer join inner join workflows park get wrong result calling outer join na fill inner join miss rows
274,1,new tungsten executionengine robust memory management speed simple data types however suffer following user defined aggregates hiv eu dafs dataset typed operators fairly expensive fit tungsten internal format aggregate functions require complex intermediate data structures unsafe raw bytes good programming abstraction due lack structs idea introduce jvm object based hash aggregate operator support aforementioned use cases operator however limit memory usage avoid putting much pressure gc eg falling back sort based aggregate soon number objects exceeds low threshold internally data bricks prototyped version customer poc observed substantial speedups existing spark introduce jvm object based aggregate operator
275,0,greetings currently process migrating project working spark project uses spark streaming convert thrift structs coming kafka parquet files stored conversion process works fine think may bug paste stack trace org codehaus jan in o jan in o runtimeexception code method lorg apache spark sql catalyst expressions generated class ljava lang object v class org apache spark sql catalyst expressions generated class specific unsafe projection grows beyond kb org codehaus jan in o code context make space code context java org codehaus jan in o code context write code context java org codehaus jan in o unit compiler write short unit compiler java org codehaus jan in o unit compiler write ldc unit compiler java also later error us park uncaught exception handler uncaught exception thread thread executor task launch worker run main group java lang outofmemoryerror java heap space seen similar issues posted always query side hunch happening write time error occurs batch duration write snippet stream flat map case success row thrift parse success row case failure ex thrift parse error s logger error error deserialization ex none for each rddrddvalsql context sql context get or create rdd context transformer sql context create data framerd d converter schema coalesce coalesce size write mode append partition by partitioning parquet parquet path please let know assistance anything help best justin code generator failed compile org codehaus jan in o jan in o runtimeexception code method error
276,0,sql show table extended like tablename works park works spark error org apache spark sql catalyst parser parseexception missing functions extended line pos sql show table extended like test state code failed run sql show table extended like table names park
277,1,existing code three layers serialization involved sending task scheduler executor task object serialized task object copied bytebuffer also contains serialized information additional jars files properties needed task execute bytebuffer stored member variable serialized task task description class task description serialized addition serialized task jars task description class contains task id metadata sent launch task message necessary two layers serialization jar file property info deserialized prior deserializing task object third layer deserialization unnecessary results park eliminate layer serialization moving jars files properties task description class task scheduler unneeded serialization
278,0,reproducer expected behavior case probably choose one side cast compare string string long long instead using data type less precision filter join expressions return incorrect results comparing strings longs
279,1,code generation get data columnvector columnar batch becoming pervasive generation part trait ease reuse refactor code generation get data columnvector columnar batch
280,0,logical plan built containing following somewhat nonsensical filter filter is not null f optimization filter converted condition always fail filter is not null f is not null f appears caused following check null intolerant https git hub com apache spark commit dfbebdebbbbdbcbfdiffacce be ccfr recurses expression extracts nested is not null calls converting is not null calls attribute root level https git hub com apache spark commit dfbebdebbbbdbcbfdiffacce be ccfr results nonsensical condition is not null converted always false condition is not null is not null
281,0,hi hope known issue luck finding anything similar jira mailing lists could searching wrong terms started experiments park sql seeing appears bug using spark sql join two tables three column inner join first column join ignored example data please let know provide details best regards eli sql based three column join loses first column
282,0,method count chisq ure test result ml lib feature chisq selector scala line wrong features election method chisquare selector based chisquare test result statistic chisq ure value select features select features largest chisq ure value degree freedom df chisq ure value different statistics chisq test rdd different df can not base chisq ure value select features wrong method count chisquare value features election results strange take testsuite ml feature chisq selector suite scala example use select k best select feature selected use select fpr select feature selected strange use sci kit learn test data parameters use select k best select feature selected use select fpr select feature selected result make sense df features ci kit learn plan submit pr problem mlml lib chisquare selector based statistics chisq test rdd wrong
283,1,generate bit masks grouping sets parsing process use analysis bit masks difficult work practice lead numerous bugs suggest remove use actual sets instead however would need generate offsets grouping id use bit masks parsing analysis cube roll up grouping sets
284,0,select distinct work order clause
285,0,spark submit support jar url http protocol url contains query string s worker driver runner download user jar method throw see expected jar exception method checks existance downloaded jar whose name contains query string s problem jar located web service requires additional information retrieve file example download jar bucket via http url contains signature datetime etc query string worker look jarnamedsparkjobjarxamz algorithm awshmacshaxamz credential us east aws request xamzdatezxamzexpiresxamz signed headers host x amz signature instead spark job jar hence query string removed checking jar existance created pr fix anyone reviews park worker throw exception uber jar http url contains query string
286,0,migrations park observed offset out of range exception reported kafka client scenario create singled stream union multiple d streams one d stream one kafka cluster multi dc solution kafka clusters topics number partitions quick investigation found class direct kafka input d stream keeps offset state topic partitions aware different kafka clusters every topic singled stream created union configured kafka cluster send offsets one kafka cluster overwrite offsets second one fortunately offset out of range exception thrown off sets kafka clusters significantly different kafka offset out of range exception d streams union separate kafka clusters identical topic names
287,0,had oop rdd fail job files corrupted eg corrupted gzip files note new had oop rdd issue reported bilal aslam had oop rdd swallow eofexception
288,0,spark sql grouping sets throws nullpointerexception problem re created using following lines code case class point string b string cstring val data seq point point points c parallelize data to df register temp table tables park sql select b count c table group b grouping sets show grouping set throws npe
289,1,classifier get num classes support non double types classification algos relying support non double label col like navi ebay es suggested set hah reasonable way datatype cast everywhere make cast happen predictor yan bo liang joseph kbs rowen move label col datatype cast predictor fit
290,1,ansi sql uses following specify frame boundaries window functions improve window function frame boundary api data frame
291,1,creating example code spark realized pretty convoluted define frame boundaries window functions partition column ordering column reason provide way createwindow spec directly frame boundaries trivially improve adding rows between range between window object data frame api simplify defining frame boundaries without partitioning ordering
292,0,current implementation iterating polling removing kafka commit queue needs drained
293,0,jvm object tracker obj map used track jvm objects spark r however observed jvm objects used anymore still trapped map prevents object get gc ed seems makes sense use weak reference like persistent rd dss park context jvm object tracker obj map may leak jvm objects
294,0,filing based email thread reynold xin docs https park apache org docs latest sql programming guide html running thrift jdbc odbc server jdbc connection url thrift server looks like specified database jdbc url ignored connecting thrift server
295,0,calling repartition py spark rdd increase number partitions results highly skewed partition sizes rows repartition method evenly spread rows across partitions behavior correctly seen scala side please reference following issue highly skewed partitions result severe memory pressure subsequent steps processing pipeline resulting oom error spy spark rdd repartitioning results highly skewed partition sizes
296,0,stack trace see concurrent modification exception json serial zation accumulators failing concurrent modification exception
297,0,reproduces park r can not parallelize data frame na null date columns
298,0,data frame struct need rename fields lowercase saving cassandra turns possible cast boolean field struct another boolean field renamed struct quote case class class with boolean flag boolean case class parent cwb class with boolean val struct cwb datatype struct type seq struct field flag boolean type true seq parent class with boolean true to df with column cwb cwb cast struct cwb collect scala match error boolean type class org apache spark sql types boolean type quote workaround temporarily cast field integer back quote val struct cwb tmp datatype struct type seq struct field flag integer type true seq parent class with boolean true to df with column cwbcwbcaststructcwbtmp with column cwb cwb cast struct cwb collect quote scala match error boolean typecasting struct
299,0,attempting create data frame using binary type field fails python underlying pyro lite library dates park appears using pyro lite issue fixed pyro lite see original bug report https githubcomirmenpyrolite issues patch https githubcomirmenpyrolite commitee cbc eae be a ee test case output attached python guy really sure builds park classpath magic test works correctly updated pyro lite binary type fails python due outdated pyro lite
300,0,without saving parquet works fine change type c column integer type also works fine incorrect result work data parquet
301,1,remove redundant experimental annotations sql streaming package
302,0,issue subject happens attempt transform data frame parquet format or cdf contains sparse vector dense vector data sources https git hub com apache spark blob vml libsrc main scala org apache spark ml lib linalg vectors scala l looks like serialization issues happens org apache spark ml lib linalg vector udt can not cast org apache spark sql types struct type
303,1,current internal row hierarchy makes difference immutable mutable rows practice can not guarantee immutable internal row immutable always pass mutable object one elements lets make internal rows mutable reduce complexity simplify internal row hierarchy
304,0,somehow saw failed test org apache spark distributed suite caching memory serialized replicated log shows spark master asked worker launch executor worker actually got response registration master knew worker registered worker know self registered seems worker launch executor master may ask worker launch executor worker actually got response registration
305,0,run following rs park home session in for version patched r platform x apple darwin bit running macos sierra spark returns incorrect result collecting cached dataset many columns
306,1,dataset always eager analysis thus spark sql eager analysis used thus need remove removes park sql eager analysis
307,0,given table create table date sts timestamp following view creation sql failes failed analyze canonicalized sql possible bugs park create view test dates select ts interval day dates can not create view includes interval arithmetic
308,0,following complicated example becomes stuck infer filters from constraints rule never runs however fail stack overflow hit limit optimization passes think sort non obvious infinite loop within rule attached your kits park process recorded stack trace s see attached screenshots showing distribution time hung query infer filters from constraints rule never terminates query
309,0,bug transposed sparse matrix is transposed true multiplication sparse vector bug present v org apache spark mlliblinalgblasmlliborg apache sparkmllinalgblasmllib local private gem v method signature bq gem v alpha double sparse matrixx sparse vector beta double dense vector bug verified running following snippets park shell using v erroneous computation multiplication transposed sparse matrix sparse vector
310,0,master maven build currently broken repl suite consistently fails class circularity errors see https spark tests app spot com jobs spark master test maven had oop timeline failure first build failed http sampl abcs berkeley edu jenkins jobs park master test maven had oop appears correspond https git hub com apache spark commit c be beedbddcdbbatestspasssbt build repl suite fails class circularity error master maven builds
311,0,production steps create scala spark application work long enough open application details webui runs park submit command standalone cluster like masters park localhost open running application details webui like localhost spark submit never finish kill process cause application creates thread infinite loop webui communication never stops application waiting thread finished instead even close web page webui prevents spark submit application finished
312,0,code generation including many mutable states exceeds jvm size limit extract values references fields constructor split generated extractions constructor smaller functions code generation including many mutable states exceeds jvm size limit
313,0,fixed insert failure data source tables schema comment field
314,0,spark scalar epl honors park repl class server port configuration user can not set fixed port numbers park repl class server port issues park since class removed spark scalar epl honors park repl class server port
315,0,https data stax oss atlassian net browse spark c brought attention following leads believe something wrong reused exchange reused exchange aggregations produce incorrect results
316,0,k application history history server backend take long time even get single application history page investigation found root cause following piece transforms map iterator uses find a pin instead map get operations park history server webui takes long single application
317,0,experien sing problems working parquet dataset api symptom problem tasks failing program runs jpspidlsofpgrephttpssee constant grow close waits way bypass problem use count addition seen df drop duplicates br dd is empty produce problem take is empty dataset leaks connections
318,1,list leaf files related functions spark listing filecatalog list leaf files calls had oop fs relation list leaf files in parallel number paths passed greater threshold lower serial version implemented had oop fs relation list leaf files called had oop fs relation list leaf files in parallel had oop fs relation list leaf files in parallel called listing filecatalog list leaf files actually confusing error prone effectively two distinct implementations serial version listing leaf files listing filecatalog since class needs keep one function listing files serial consolidate various list leaf files implementations
319,1,query one distinct necessary makes bunch unions slower bunch union all s followed distinct optimizer remove unnecessary distinct s multiple unions
320,0,using malformed urls cadd jars cadd file bricks executors forever executors try update dependencies url malformed always throw malformed url exception cluster unusable restart adding malformed urls cadd jars cadd file bricks executors
321,0,race condition fetch failed resubmit failed stage job job run different threads job failed times due fetch failed aborted job post resubmit failed stages be case failed stages dag scheduler empty failed stage never resubmitted due abort stage another thread
322,0,getting incorrect results data frame except method rows returned instead ones intersected calling subtract underlying rdd returned correct result tracked use coalesce following simplest example case created reproduces issue get result uses except one using coalesce returns instead data frame except returns incorrect results combined coalesce
323,0,h problem remainder expression returns incorrect result using expression eval calculate result expression eval called case like constant folding remainder expression eval returns incorrect result
324,0,filestream source used structured streaming first resolves globs creates listing filecatalog list files resolved glob patterns folder deleted glob resolution listing filecatalog list files run filenotfoundexception fatal exception streaming job however include warn message folder deletion globbing may fail structured streaming jobs
325,0,assert on query two apply constructor one accepts closure returns boolean another accepts closure returns unit actually confusing developers could mistakenly think assert on query always require boolean return type verifies return result indeed value last statement ignored one constructors assert on query condition consistent requiring boolean return type
326,0,parsing csv datetime column contains variant iso include colon offset casting timestamp fails simple example csv content quote time quote stack trace results processing data quote error utils aborting task java lang illegalargumentexception org apache xerces jaxp datatype xml gregorian calendar impl parser skip unknown source org apache xerces jaxp datatype xml gregorian calendar impl parser parse unknown source org apache xerces jaxp datatype xml gregorian calendar impl unknown source org apache xerces jaxp datatype datatype factory impl new xml gregorian calendar unknown source javax xml bind datatype converter impl parse datetime datatype converter impl java javax xml bind datatype converter impl parse datetime datatype converter impl java javax xml bind datatype converter parse datetime datatype converter java org apache spark sql catalyst util datetime utils string to time datetime utils scala org apache spark sql execution data sources csv csv typecast cast to csv infer schema scala quote somewhat related believe python standard libraries produce form zone offset system got data written python https docs python org library datetime html strftime strptime behaviors park sql catalyst handle isodate without colon offset
327,0,fix ddl bugs table management name temp view exists
328,0,py spark df take ends running single stage job computes one partition dfdf limit collect ends computing partitions df runs two stage job difference performance confusing think generalize fix spark dataset collect implemented efficiently python df take df limit collect perform differently python
329,0,run python application specify remote path extra files included pythonpath using py files spark submit py files configuration option yarn cluster mode get following error exception thread main java lang illegalargumentexception launching python applications spark submit currently supported local files xxxx app py org apache spark deploy python runner format path python runner scala org apache spark deploy python runner a non fun format paths apply python runner scala org apache spark deploy python runner a non fun format paths apply python runners ca lascala collection traversable like a non fun map apply traversable like scala scala collection traversable like a non fun map apply traversable like scala scala collection indexed seq optimized class for each indexed seq optimized scala scala collection mutable array ops of ref for each array ops scala scala collection traversable like class map traversable like scala scala collection mutable array ops of ref map array ops scala org apache spark deploy python runner format paths python runner scala org apache spark deploys park submit a non fun prepare submit environment apply spark submit scala org apache spark deploys park submit a non fun prepare submit environment apply spark submit scala scala option for each option scala org apache spark deploys park submit prepare submit environments park submit scala org apache spark deploys park submit submit spark submit scala org apache spark deploys park submit main spark submit scala org apache spark deploys park submit main spark submit scala sample commands would throw errors parks park app py requires a ppp y spark submit deploy mode cluster py files xxxxapppyxxxxsparkapppy works fine spark submit deploy mode cluster conf spark submit py files xxxxapppyxxxxsparkapppy working would work fine app py downloaded locally specified working correctly using py files option earlier versions park using spark submit py files configuration option work either ways following diff shows comment states work nonlocal paths yarn cluster mode specifically separate validation fail yarn client mode used remote paths https git hub com apache spark blob master core src main scala org apache spark deploys park submit scala l newly added stores hence validation gets triggered even specify file spy files option https git hub com apache spark blob master core src main scala org apache spark deploys park submit scala l also changed logic yarn client read values directly spark submit py files configuration instead py files earlier https git hub com apache spark commit bab ffeeceeabdfediffbdffbebr broken whether use py files spark submit py files validation gets triggered cases irrespective whether use client cluster mode yarn specifying remote files python based spark jobs yarn cluster mode working
330,0,trying reach launch multiple containers pool running executors count reaches goes beyond target running executors container released marked failed cause many jobs marked failed causing overall job failure patch soon completing testing panel title typical exception found driver marking container failed panel dynamic allocation race condition containers getting marked failed releasing
331,1,use multiple d streams coming different kafka topics streaming application settings like max rate backpressure enabled disabled would better passed config kafka utils create stream kafka utils create direct stream instead settings park conf able set different max rate different streams important requirement us currently work around problem using one receiver based stream one direct stream would like able turn back pressure one streams well set streaming max rate independently multiple streams
332,1,profiling job saw patten matching wrap function hive inspector consuming around time avoided similar change unwrap function made spark wrapping catalyst datatype hive datatype avoid pattern matching
333,0,h problem description following query triggers memory error collect gb unrolled gb input data processed memory leak memory store unable cache whole rdd memory
334,0,permanent tables views exist temporary view exists expected error no such table exception partition related alter table commands however always reports confusing error message example multiple bugs ddl statements temporary views
335,0,several default params spark rs park mlp wrong layers null to le step size seed make default params spark rs park mlp consistent multilayer perceptron classifier
336,0,create tables follows create table select str cast decimal num create table b select str select floor num returns select floor num join bstr bstr returns floor ceil decimal returns wrong result compact format
337,0,data sources without extending schema relation provider expect users specify schemas creating tables schema input users exception issued since spark data source avoid infer schema every time store schema meta store catalog thus reading cataloged data source table schema could read meta store catalog case also got exception example reading cataloged data sources without extending schema relation provider
338,0,memory store put iterator as bytes may silently lose values used k ryo serializer properly close serialization stream attempting deserialize already serialized values may cause values buffered k ryo internal buffers read root cause behind user reported wrong answer bug py spark caching reported ben leslie spark user mailing list thread title dpy spark persist memory vs memory disk memory store put iterator as bytes may silently lose values k ryo serializer used
339,1,logical plan serialize from object array always use generic array data destination unsafe array data could used primitive array simple approach solve issues addressed spark motivating example optimize serialize from object primitive array
340,1,task metrics ui data updated block statuses field assigned never read increasing memory consumption webui remove field remove unused task metrics ui data updated block statuses field
341,0,bug python udf work order limit python udf work sort limit
342,0,updating spark found seems memory leaks park streaming application head heap histogram application running hours shows scala collection mutable default entry java lang long unexpected big numbers instances fact numbers started growing streaming process began keep growing proportional total number tasks investigation found problem caused inappropriate memory management release unroll memory for this task unroll safely method class org apache spark storage memory store https git hub com apache spark blob branch core src main scala org apache spark storage memory store scala spark x release unroll memory for this task operation processed parameter memory to release https git hub com apache spark blob branch core src main scala org apache spark storage memory stores cal all fact task successfully unrolled blocks memory unroll safely method memory saved unroll memory map would set zero https git hub com apache spark blob branch core src main scala org apache spark storage memory stores cal al result memory saved unroll memory map released key part memory never removed hashmap hashtable keep increasing new tasks keep incoming although speed increase comparatively slow dozens bytes per task possible result oom weeks months inappropriate memory management org apache spark storage memory store may lead memory leak
343,0,check following concurrent modification exception even though accumulators thread safe concurrently read serializing executor heartbeats modified tasks running leading concurrent modification exception errors thereby leading missing heartbeats leading inconsistent data since individual fields multi field object might serialized different points time leading inconsistencies accumulators like long accum seems like pretty serious issue sure best way fix obvious fix would properly synchronize accesses fields accumulators synchronize writeobject write k ryo methods may adverse performance impact serialization accumulators heartbeats thread safe
344,0,dataset join with performing broadcast join table gigabytes size due dataset logical plan statistics size in bytes seq seq really long string to ds might remove private sql dataset logical plan get work val stats ds logical plan statistics yields stats org apache spark sql catalyst plans logical statisticsstatistics false causes join with perform with perform broadcast join even tho data gigabytes size course causes executors run memory settings park sql auto broadcast join threshold help logical plan statistics size in bytes large negative number thus less join threshold able workaround issue setting auto broadcast join threshold large negative number dataset join with broadcasts gigabyte sized table causes oom exception
345,0,using pivot multiple aggregations need alias avoid special characters alias help df group by cpivotaggavgcoldmaxbcolb show c bar avg cold bar max bc olbfooavgcoldfoomaxbcolb small two two large two one expected output c barco ldbarcolbfoocoldfoocolb small two two large two one one approach fix issue change class sql catalyst src main scala org apache spark sql catalyst analysis analyzer scala change output name method alias specified aggregates pivot honored
346,0,pool adjacent violators algorithm pava implementation currently ml lib taken time certain inputs worst case complexity n reproduce pulled private method pool adjacent violators ml lib regression isotonic regression benchmarking harness given input vary length input get timings input length time us tests performed using https githubcomsirthiasscala benchmarking template also confirm run issue real data set working trying calibrate random forest probability output partitions take hours run skew issue since largest partitions finish minutes assume partitions cause something approaching worst case complexity working patch borrows implementation used sci kit learn riso package handle particular input linear time quadratic worst case isotonic regression takes non polynomial time inputs
347,0,write df passes everything arguments underlying data source x passing header true spark example following additional arguments write df passed data source
348,0,alter table rename partition unable handle data source tables like alter partition commands issue exception instead issue exceptions alter table rename partition tries alter data source table
349,0,clock fly found following corner case returns wrong quantile value left column represents output using quantile summaries window function value right column represents expected result different left right column left column intermediate compression storage quantile summaries quantiles summaries returns wrong result compression
350,0,core info application master ui consider application info executor limit pretty confusing ui says unlimited executor limit set master ui show correct core limit application info executor limit set
351,0,select length select length sql return error shive ok error query can not resolve length due datatype mismatch argument requires string binary type however int type line pos error query can not resolve length due datatype mismatch argument requires string binary type however double type line poss park sql length return error
352,1,many users requirements use third party r packages executors workers spark r satisfy requirements elegantly example mess administrators cluster deploy r packages executors workers node in flexible think support third party r packages spark r users jar packages following two scenarios users install r package scran custom cran like repository executors users load local r packages install executors achieve goal first thing makes park r executors support virtual env like python cond a investigated found packrat http r studio gi thu bio packrat one candidates support virtual envr packrat dependency management system r isolate dependent r packages private package spaces park r users install third party packages application scope destroy application exit need bother administrators install packages manually would like know whether make sense spark r executors workers support virtual env
353,0,spark fix oom issue metadata super big cases may also trigger oom current implementation treenode to json recursively search print fields current treenode even field type type seqtype map safe seq map big converting json make take huge memory may trigger memory error userspace input may also propagated plan userspace input arbitrary type may also self referencing trying print userspace input json risky following example triggers stack overflow error calling to json plan user defined udf current treenode to json may trigger oom corner cases
354,1,buildings park r vignettes jenkins machines need r markdown r package available http scran r project org web packages r markdown index html think running something like r scripte install packages r markdown repos http cran stat ucla edu work install r markdown r package jenkins machines
355,0,spark currently assumes partitions less uses padding exceed sort logic reliable checkpoint rdd gets messed fails part files sorted compared strings leads filename order part part instead part part part reconstructing checkpointed rdd job fails possible solutions bump padding allow partitions sort part files extracting sub portion string verify rdd fix sorting part files reconstructing rdd partition checkpointed files
356,0,http sampl abcs berkeley edu jenkins views park qa test jobs park branch tests bt had oop console seems show df tests park sqlr broken branch broken test show df tests park sqlr
357,0,latest branch two test case failure due backport test alter view keep previous table properties comment create time etc tests park explain output ct as shows analyzed plan fix two test failures backport
358,0,demo case silently replace corrupted line null without error message improves error message fail sparse json file lines data frame reader
359,0,using scala value classes inner type datasets breaks spark x simple spark application demonstrates spark scala value classes create encoder problems break runtime
360,1,regulate pending running executors determine executors eligible kill kill iteratively rather loop rpc call synchronized leading lock contentions park listener bus side effect listener bus blocked iteratively remove executor skill multiple executors together reduce lock contention
361,0,spark query hive table starting number
362,0,using ml lib calling to json plan many level subqueries may cause memory exception stack trace like query plan stack trace j map distribution attached large metadata filed alias cause oom calling treenode to json
363,1,would useful jdbc rdd jdbc spark sql functionality usable outside jdbc rdd would make easier write test harnesses comparing spark output jdbc databases refactor jdbc rdd expose jdbc spark sql conversion functionality
364,0,spark updated version vis js class renamed like timeline vis timeline ticket care style broken style event timeline broken
365,0,numbers park r tests current failing run windows discussed https git hub com apache spark pull list tests fail right https gist githubcomshivaramdfbddce ecec full log build test appveyorhttpsciappveyor com project hyuk jin kwon spark build test fix spark r tests windows
366,0,investigating spark found incorrect results case subquery thought originally edge case investigation found general problem incomplete algorithm name resolution catalyst pa ser may lead incorrect result
367,1,another step get rid hive client hive session state meta store interactions external catalog interface however existing implementation insert into hive table still requires hive clients thus remove hive client moving meta store interactions external catalog remove direct usage hive client insert into hive table
368,1,publishing spark rcr an would nice vignette user guide describes big picture introduces use various methods important new users may even know method look add package vignettes park r
369,1,kolmogorov smirnov test popular nonparametric test equality distributions implementation ml lib nice exposes park r add kolmogorov smirnov tests park r
370,0,spark shell apparently example minimal removing cross one join causes issues park sql cross join two joins bug
371,0,better error message exceptions scala udf execution
372,0,http sampl abcs berkeley edu jenkins jobs park master tests bt had oop test report junit org apache spark util collection unsafe sort prefix comparators suite string prefix comparator could reproduce locally let add case regression tests explicitly test prefix comparators suite string prefix comparator failed input strings empty string s
373,0,stopping spark session recreate use hive context throw error steps reproduce sparks park session builder enable hive support get or create spark sql show databases sparks top sparks park session builder enable hive support get or create spark sql show databases java lang illegalstateexception can not call methods stopped spark context error occurs case py sparks park shell using hive context creating spark contexts park throws java lang illegalstateexception can not call methods stopped spark context
374,1,move create table shive strategies
375,0,following example fails classcastexception surprising error occurring query parsing hunch performing expression evaluation early need run analysis type promotion rules prior trying evaluate expressions performing arithmetic values lead classcastexception match errors query parsing
376,0,following test case produces classcastexception analyzer bug discovered trying run sqlite bug reports spark sql see https www sqlite org src tkt view name ec classcastexception outer reference can not cast named expression correlated subquery rhs operator
377,0,issue fixed spark seems client wrapper conf trying access thread local session state set npe thrown client wrapper conf
378,0,summary page spark history server webui keep displaying loading history summary time crashes browser k application history event logs hdfs investigation history page js files ends rest request a piv applications endpoint history server rest endpoint gets back json response k applications inside event log directory takes forever parse render page hundreds thousands application history running fines park history server summary page gets stuck loading history summary k application history
379,1,spark configurable l regularization parameter generalized linear regression important spark r users run ridge regressions park rs park glm configurable regularization parameter
380,0,trying run pivot transformation ran spark cluster namely sc parallelize seq to dfb cres org apache spark sql data frame int bin tc int scalar es group bypivot bag gcountcavgcnafillresorg apache spark sql data frame int count c bigint avg c double count c bigint avg c double scalar es group bypivot bagg count c avg cna fill show count ca vgc count ca vgc upgrade environments park got error executing na fill method scala sc parallelize seq to dfb cres org apache spark sql data frame int bint field scalar es group bypivot baggcountcavgcnafillorg apache spark sql analysis exception syntax error attribute name count c org apache spark sql catalyst analysis unresolved attribute e unresolved scala org apache spark sql catalyst analysis unresolved attribute parse attribute name unresolved scala org apache spark sql catalyst plans logical logical plan resolve quoted logical plan scala org apache spark sql dataset resolve dataset scala org apache spark sql dataset col dataset scala org apache spark sql data frame na functions org apache spark sql data frame na functions fill cold at a frame na functions scala org apache spark sql data frame na functions a non fun apply data frame na functions scala org apache spark sql data frame na functions a non fun apply data frame na functions scala scala collection traversable like a non fun map apply traversable like scala scala collection traversable like a non fun map apply traversable like scala scala collection indexed seq optimized class for each indexed seq optimized scala scala collection mutable array ops of ref for each array ops scala scala collection traversable like class map traversable like scala scala collection mutable array ops of ref map array ops scala org apache spark sql data frame na functions fill data frame na functions scala org apache spark sql data frame na functions fill data frame na functions scala org apache spark sql data frame na functions fill data frame na functions scala data frame fill pivot causing org apache spark sql analysis exception
381,0,hive index tables supported spark sql thus issue exception users try access hive index tables internal function table exists tries access hive index tables always gets error message hive index table supported message could confusing users since sql operations could completely unrelated hive index tables example users try alter table new name exists index tablename expected exception table already exists exception table existence checking index tablename exists
382,0,look event timeline stage chrome get total time bigger sum individual elements actually proportions calculated correctly due css issue expanded size full bar even timeline stage core bar timeline bar chrome
383,1,method sql context parse datatype datatype string string could removed uses park session parse datatype datatype string string instead require updating py spark method sql context parse datatype datatype string string could removed
384,0,broadcast join produces incorrect columns join result see example join without using broadcast gives correct columns running py spark yarn amazon emr broadcast join produces incorrect results compressed oops differs driver executor
385,0,exception executor logs park r zip distributed executors runs park rr studio
386,0,result compare two vectors using unit tests org apache spark ml libutil testing utils right some time example val vectors dense a rraryvalbvectorszerosbab stole result true comparing vector relative tolerance absolute tolerance unit tests error
387,1,since hive client used interact hive meta store hidden hive external catalog moving hive client hive external catalog hive shared state becomes wrapper hive external catalog thus removal hive shared state becomes straightforward removal hive shared state reflection logic directly applied choice external catalog types based configuration catalog implementation hive client also used invoked entities besides hive external catalog defines following two apis removal hive shared state
388,1,remove catalog table type index
389,1,refactor rml lib easier ml implementations
390,0,currently analyze table hives erde tables issue exceptions cases tables data source tables issued exception however tables memory cataloged tables issue exception issue exceptions analyze table memory cataloged tables
391,0,following end end test uncovered bug get external row field need update get external row field escape field names also need audit expressions make sure making mistake get external row field properly escape field names causing generated code compile
392,1,inline tables currently support sql generation result view depends inline tables would fail support sql generation in line tables
393,1,create hive table as select logical plan dead code refactoring removal useless create hive table as select logical plan
394,0,following example runs successfully spark fails current master bbbfcbebdfbb earlier note error occurs query execution analysis physical planning complex query triggers binding error hash aggregate exec
395,1,take advantage am rm client apis simplify logic yarn allocation handler
396,0,see discussion mailing list http apache spark developers list n nabble com aggregations scala pair std html implementation order preserving allow duplicate names relational grouped dataset agg order preserving allow duplicate column names
397,0,found case broken spark run successfully spark btw case also run successfully hive please see reproduces details spark sql cli usr libs park bins park sql spark sql use testdb spark sql drop database exists testdb cascade info executions park sql parser parsing command drop database exists testdb cascade error query drop current databasetest dbhivecliusrbinhivehive use testdb ok hive drop database exists testdb cascade ok time taken seconds failed drop database use databases park
398,0,consider following query innermost query left join condition false nevertheless number rows produced equal number rows table non empty since values null outer retain rows overall result query contain single row value instead current spark master ecbdfadaecdbff least returns rows looking explain appears logical plan optimizing local relations park even run query suspicion bug constraint propagation filter pushdown issue seem affects park think regression master analyzer incorrectly optimizes plan empty local relation
399,0,running select null fails select null returns null expected select null throws analysis exception select null works
400,0,seeing many job failure due executor oom following stack trace https git hub com si talked i as park blob master core src main java org apache spark util collection unsafe sort unsafe external sorter java l unsafe external sorter checking memory page used upstream baseobject case heap memory always null unsafe external sorters pill memory pages job failure due executor oom off heap mode
401,0,py spark task accesses cached data non locally throws stream corrupted exception like stack trace simplest way found reproduce running following py spark locality throw java iostream corrupted exception
402,0,currently logical relation new instance simply creates another logical relation object parameters however new instance method inherited multi instance relation return copy unique expression ids current logical relation new instance causes failure self join logical relation new instance follow semantics multi instance relation
403,0,py spark filtering udf derived column join types optimized logical plan results java lang unsupported operation exception could replicate scala fails join outer column expression left outer string column expression right outer string column expression passes join inner string column expression outer string made tests demonstrate run bins park submit test bug pypy spark filter udf column join gives java lang unsupported operation exception
404,0,random query generation uncovered following query returns incorrect results runs park sql original query uncovered generator since performed bit minimization try make understandable following tables based output adding contain four rows zero sure shrink straightforward way opening bug get help triaging incorrect result clause added group query
405,0,running works fine hunch uncovering bug ordering optimizer rules bug constant folding rule particular example probably unimportant may indicator problems select count null throws unsupported operation exception analysis
406,0,following failing test demonstrates bugs park mis encodes array struct fields whole stage codegen disabled path forgot copy somewhere round trip encoding array fields wrong whole stage codegen disabled
407,0,discovered bug working build master branch believe used work fine running spark data frame int datacolumn values like think duplicate splits generated quantile discretize r throws invalid argument exception parameter splits given invalid value valid data
408,1,acquiring allocations yarn launching containers spark currently waits seconds executors connect drivers park standalone nothing like happens wondering whether remove sleep entirely reason missing yarn different standalone regard least could something smarter like wait executors registered remove second sleep starting app yarn
409,0,encountering incompatible data source register better add instructions remove upgrade improve error message encountering incompatible data source register
410,1,sometimes simply need add property spark config mesos dispatcher option right created property file add conf mesos dispatcher process
411,0,hit consistent bug dataset columns raising blockers park returning wrong results rather error ing leading data integrity issues put together following test cases how issue runs park shell example joining dataset lots fields onto another dataset join works fine show dataset get expected result however run map step dataset end strange error sequence right data set contains last value whilst test may seem rather contrived example standard anal tical pattern original test case case classname name string case class small case class join key integer names seqname case class big case class field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer field integer val big cc seq big case class val small cc seq small case class seqname jamie name ian name dave name val big ccds spark create datasets parks park context parallelize big cc val small ccds spark create datasets parks park context parallelize small cc val joined test big ccds join with small ccds b field b join key left next step fine shows names wrapped array jamie ian dave joined test show false one ends repeating simple map step possible wrapped array joined test map identity show false one works less fields jamie ian dave joined test map show false incorrect results returned following join two datasets map step total number columns
412,0,suggest someone called yarn scheduler backend do request total executors one three functions coarse grained scheduler backend kill executors coarse grained scheduler backend request total executors coarse grained scheduler backend request executors hold lock coarse grained scheduler backend yarn scheduler backend do request total executors send request executors message yarn scheduler endpoint wait reply someone send remove executor yarn scheduler endpoint message received endpoint request ex ex ut or message sent received endpoint endpoint would first handle remove executor request executor message handling remove executor would send message driver endpoint wait reply driver endpoint request lock coarse grained scheduler backend handle message lock occupied would cause deadlock found issue deployment would block driver make handle messages two message went timeout potential deadlock driver handling message
413,0,group order ordinal throw analysis exception instead unresolved exception
414,0,found https git hub com apache spark pull files rs park parses negative numeric literals unary minus positive literals introduces problems edge cases parsed decimal instead bigint negative numeric literal parsing
415,0,issues park branch missing profile scala build release builds h missing hive thrift server scala
416,0,using data set containing rows containing null array ints observe following performance local mode select column explode column extremely slow
417,0,treenode exception thrown executing following minimal examples park crucial column q generated lit expr exception org apache spark sql catalyst errors package treenode exception binding attribute tree x possible workaround write data frame disk grouping mapping treenode exception flat mapping relational grouped dataset created data frame containing column created lit expr
418,0,found strange behaviour using full outer join combination inner join seems inner join match values correctly full outer join reproducible examples park full outer join followed inner join produces wrong results
419,0,spark seems problems reading parquet dataset generated originally posted user mailing list last discoveries clearly seems like bugs park unable infer schema parquet data written spark
420,1,ccmgummelttnachenskon to think fairly easy would beneficial work goes mesos separate module like yarn principle really also means anyone need mesos support build without entirely willing take shot collect mesos support code module profile
421,0,submitting application name bins park submit name my application test verbose executor cores num executors master yarn deploy mode client class org apache spark examples spark k means examples target original spark examples snapshot jar hdfs localhost lr big txt history server ui appid application appname cd cb be a eb appname random uuidcdcbbeaeb since spark appname my application test application org apache spark examples spark k means invoke appname appname random uuid even spark appname exists
422,1,execution package meant internal result make sense mark things private sql private spark simply makes debugging harder spark developers need inspect plans runtime remove private sql private spark sql execution package
423,0,comment catalog table returned hive always empty store table property creating table however try retrieve table metadata hive meta store rebuild comment always empty table comment catalog table returned hive meta store always empty
424,1,could subquery within single query could reuse result without running multiple times reuse subqueries within single query
425,0,following queries work using ordinals order causes analysis error query group clause using ordinals
426,0,kafka utils create direct stream work python setparameter from offsets starting offsets stream kafka long type removed python py j maps numeric variables java lang integer java lang long depending number size causes classcastexception small offsets variables behaviour noticed tests functionality disabled python https git hub com apache spark blobefcfbcfbcdaepythonpy spark streaming test spy l from offsets parameter kafka direct streams work python
427,0,source table data source table table generated create table like non empty expected table empty create table like generates non empty table source data source table
428,1,function related hive external catalog apis enough verification logic spr hive external catalog in memory catalog become consistent error handling example exception got calling rename function verification function related external catalog apis
429,1,update logistic cost aggregator serialization code make consistent linear regression update logistic cost aggregator serialization code make consistent linear regression
430,0,spark standalone cluster runs single applications park task repeatedly fails causing executor jvm exit zero exit nonzero whereas think always call schedule even clean executor shutdown since schedule always safe calls park tasks cause jvm exit zero exit code may cause app hang standalone mode
431,0,query used works park fails executor oom stack trace query broadcast hash join fails due execu to rooms park
432,1,remove test hive shared state otherwise really testing reflection logic based setting really testing reflection logic based setting catalog implementation removal test hive shared state
433,0,dataset typed selection analysis error dataset typed selection
434,0,greatest least function friendly error message data types error sql statement analysis exception can not resolve greatest cast decimal due datatype mismatch expressions type got greatest array buffer decimal type stringtype line pos report human readable datatype instead rather array buffer stringtype improve error message greatest least
435,0,wide table columns try fitting train data columns fatal error occurs caused org codehaus jan in o jan in o runtimeexception code method lorg apache spark sql catalyst internal row lorg apache spark sql catalyst internal row class org apache spark sql catalyst expressions generated class specific ordering grows beyond kb org codehaus jan in o code context make space code context java org codehaus jan in o code context write code context java org apache spark sql catalyst expressions generated class specific ordering grows beyond kb
436,0,right constructors time window expression catalyst incorrectly uses window duration place slide duration cause incorrect windowing semantics time window expressions analyzed catalyst relevant code https git hub com apache spark blob branch sql catalyst src main scala org apache spark sql catalyst expressions time windows cal all time window incorrectly drops slide duration constructors
437,0,avg metrics summed across folds instead averaged easy fix cross validator fit function py spark cross validator reports incorrect avg metrics
438,0,bins park r launching java spark submit command users m wang spark wsb in spark submit spark r shell var folders bs gvjklkwqstvftpmgnrtmpqxj giz backend port ee de using spark default log j profile org apache spark log j defaults properties setting default loglevel warn adjust logging level uses cset loglevel new levels cset loglevel info error could find functions cset log levels cset loglevel exists park rsc set loglevel work
439,1,remove max of min of
440,0,hello using cx large instance sec core slots parse url url hosts park sql unfortunately seems internal thread safe cache instance send idle view thread dump executors executor threads blocked state however switch executor cores executor scores throughput almost x higher cpus back use thanks java util hashtable limits throughput parse url
441,0,happens files can operator take account partition pruning implementation same result result executions may incorrect self joins base file relation minimal test case reproduce exchange reuse incorrectly reuses scans different sets partitions
442,1,catalyst package meant internal result make sense mark things private sql private spark simply makes debugging harder spark developers need inspect plans runtime remove private sql private spark catalyst package
443,1,spark i loop get added jars useful method use programmatically get list jars added opens park i loop get added jars
444,0,root history server rendered dynamically javascript honor application web proxy base https git hub com apache spark blob master core src main resources org apache spark ui static history page template html l links history server honor https git hub com apache spark blob master core src main scala org apache spark uiu i utils scala l means links history serverroot page broken deployed behind proxy history server main page honor application web proxy base
445,0,useful log timezone query result match especially build machines different time zone a mplab jenkins log timezone query result match
446,0,correlated subqueries limit could return incorrect results rule resolve subquery analysis phase moves correlated predicates join predicates neglect semantic limit example correct result contains rows correlated subqueries containing nondeterministic operators return incorrect results
447,0,hello little similar spark https issues apache org jira browse spark reopened would recommend give another full review hashed relations cala particularly new long to unsafe rowmap joins long to unsafe rowmap crashes array index out of bounds exception
448,0,issue occurs run map data set containing case class list self contained test case case class test cc key int letters list string list causes issues eq array works fine simple test data val dssc maker dd seq list list h list fhlistllmapxxlengthxtodf key letters test cc fail val test ds map key test show error caused org codehaus commons compiler compile exception file generated java line column applicable constructor method found actual parameters int scala collection seq candidates test cc int scala collection immutable list seems internally converting list sequence cant convert back change list string seq string array string issue doesnt appear dataset containing case class list type causes compile exception converting sequence list
449,0,behaviors park context add file changed slightly introduction netty rpc based file server introduced spark disabled default became default file servers park prior calling spark context add file twice path would succeed would cause future tasks receive updated copy file behavior never explicitly documented spark behaved way since early x versions relevant lines executor update dependencies existed since netty file server enabled second add file call fail requirement error netty stream manager tries guard duplicate file registration believe change behavior unintentional propose remove require checks park matches x default behavior problem also affects add jar subtle way file server add jar call also fail exception exception logged ignored due code added order ignore errors caused missing spark examples jars running yarn cluster mode afaik spark context add file fail called twice file
450,0,launching spark system multiple java s installed options choosing j reuse setting java home straightforward however py spark internal py j launches java gateway always invokes java directly without qualification means get whatever java first path necessarily ones park java home could see npy j issue point view fix easy make sure java want first path figure way make reliably happen py spark executor launch path seems like something would ideally happen automatically set java home launching spark would expect java used throughout stack java launched py spark gateway may java used spark environment
451,0,reproduces park sql hive happen change name cte guess catalyst get caught infinite recursion loop cte source tablename infinite recursion loop org apache spark sql catalyst trees treenode tablename collides
452,1,jira upgrade derby version sean figured use derby tests initial pull request include jars folders park believe required based comments pull request dependency upgrade upgrade due already disclosed vulnerability cve derby used https www version eye com search checking problems variety libraries investigating set jenkins job check pom regular basis stay ahead game matters like raised mailing list http apache spark developers list n nabble com vote release apache spark rct pp html stephen hellberg replied sean owen checked impact previous spark releases particular version derby relatively recent without vulnerabilities version checked branch ideally backport impacted spark releases marked critical ticked important checkbox going impact every user security component add one hence build tag upgrade derby
453,0,ml gaussian mixture training failed due feature column type mistake feature column type ml linalg vector udt got ml lib linalg vector udt mistake bug easy reproduce following unit tests complain errors estimators transformers missed calling transform schema dataset schema firstly fit transform also add function estimators transformers missed ml gaussian mixture training failed due feature column type mistake
454,0,errors thrown udfs cause treenode exception query order clause
455,0,hello crash spark sql joins minimal reproducible test case interestingly seems happen reading parquet data added crash true variables how left outer example also crashes regular inner join joins long to unsafe rowmap crashes negative arraysize exception
456,1,codes subexpression elimination for whole stage codegen never used actually remove using jira remove unused codes subexpression elimination for whole stage codegen
457,1,use struct type catalog table remove catalog column
458,0,spark x possible use int string functions perform typecast functionality broken sparks park longer falls back hive functions spark breaks various hive cast functions
459,0,spark currently throws exceptions invalid casts data types except date type somehow date type returns null consistent throws analysis exception wells park throw analysis exception invalid casts date type
460,0,subexpression elimination suite semantic equals hash assumes default attribute reference expr id wont expr id however depends test runs may happen use expr id fix potential expr id conflict subexpression elimination suite semantic equals hash
461,0,spark parsefloat literals decimals however introduces side effect described fail create decimal arrays literals different inferred precession s scales
462,0,yarn rolling upgrade happens spark yarn shuffle service initializing tokens soon enough causes running applications fail nullpointerexception s rather ioexception s causes clients retry turn causes application totally fail retried succeeded shuffle server error server transport request handler error invoking rp chandler receiver pc id java lang nullpointerexception password can not null sasl enabled orgs park project guava base preconditions check not null preconditions java org apache spark network sasl sparks asl server encode password sparks asl server java org apache spark network sasl sparks asl server digest callback handler handles park sasl server javacom sun security sasl digest digest md server validate client response digest md server javacom sun security sasl digest digest md server evaluate response digest md server java org apache spark network sasl sparks asl server responses park sasl server java org apache spark networks as lsas lrp chandler receives as lrp chandler java org apache spark network server transport request handler process rpc request transport request handler java org apache spark network server transport request handler handle transport request handler java org apache spark network server transport channel handler channel read transport channel handler java org apache spark network server transport channel handler channel read transport channel handler java i one tty channel simple channel inbound handler channel read simple channel inbound handler java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java i one tty handler timeout idle state handler channel read idle state handler java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java i one tty handler codec message to message decoder channel read message to message decoder java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java org apache spark network util transport frame decoder channel read transport frame decoder java i one tty channel abstract channel handler context invoke channel read abstract channel handler context java i one tty channel abstract channel handler context fire channel read abstract channel handler context java i one tty channel default channel pipeline fire channel read default channel pipeline java i one tty channel nio abstract nio byte channel nio byte unsafe read abstract nio byte channel java i one tty channel nio nio event loop process selected key nio event loop java i one tty channel nio nio event loop process selected keys optimized nio event loop java i one tty channel nio nio event loop process selected keys nio event loop java i one tty channel nio nio event loop run nio event loop java i one tty util concurrent single thread event executor run single thread event executor java java lang thread run thread java yarn shuffle service in it properly yarn rolling upgrade
463,0,hello found issue testing codebase rc struct types park accepts python type handy rc throws error know intended advocate behaviour remain map type probably wasteful key names never change switching python tuples would cumbersome minimal script reproduce issue thanks struct type accept python dicts anymore
464,1,move bucket spec catalyst module use catalog table
465,0,summary reproduce bug create data framed f sample fixed seed collect data frame result call particular udf data frame result would expect results use rows df appear note result result deterministic see attached notebook details cells notebook executed order dataset sample seed result seems depend downstream usage
466,0,users allowed issue load data view currently users got strange runtime error example strange error issuing load table view
467,1,similar https issues apache org jira browse spark currently jdbc utils save partition type based dispatch row write appropriate values appropriate writers created first according schema apply row approach similar catalyst write support avoid per record type dispatch jdbc writing
468,0,calling persist data frame columns removing data data frame issues park works issues spark following test case demonstrates problem please let know need additional information thanks spark persist call data frames columns wiping data
469,0,spark add ability blacklist entire executors nodes deal w faulty hardware however without displaying ui hard realize executor bad tasks getting scheduled certain executors first step show nodes executors blacklisted entire application needs how blacklisting tasks stages also ensure blacklisting events get event logs history server ui show blacklisted executors nodes
470,0,following simple sqlquery reproduces issue exception thrown bug regressions park issue last value false throws index out of bounds exception
471,0,query fails works constraints propagation may fail query
472,0,case https git hub com apache sparkblobbaefffbbeecsql catalyst src main scala org apache spark sql catalyst analysis analyzers cal all shown case triggered even function unresolved functions like lead used may see errors like window frame range unbounded preceding current row must match required frame rows following following wrongly set frame specification resolve window frame triggered unresolved functions
473,1,elt function support codegen execution better provide support add codegen elt function
474,0,fail analysis query fails condition contains grouping column
475,0,please see attached notebook seems lag lead somehow fail recognize offset row exist generate wrong results lag lead using constant input values return default value offset row exist
476,0,vectorized parquet reader fails read certain tables created hive tables type tinyint smallint catalyst converts byte type short type respectively hive writes tables parquet format parquet schema files contains int fields reproduce run commands hives hell beeline tries consolidate schemas chooses whatever parquet file primitive types see parquet read support clip parquet type vectorized reader uses catalyst schema comes hive meta store says bytefield tries read data byte data stored on heap columnvector null tested small change parquet read support clip parquet type fixes particular issue runtests wait others chime may be tell right place fix vectorized parquet reader fails read certain fields hive tables
477,0,unfortunately know little databases figure bug data frame following schema trying write oracle database like right driver rds instance engine type mysql works error message https issues apache org jira browse spark could long also translated wrong datatype thanks oracle jdbc table creation fails or a invalid datatype
478,0,currently check value returned called method invoke returned value null nullpointerexception thrown fix nullpointerexception returned value called method invoke null
479,0,suppose spark correct output app think bug expected see positive integers avoid zeros environments park version scala version rdd pipe returns values empty partitions
480,0,orc sources park sql writer option compression used set codec value also set orc compressor c confused codec however user set orc compress writer option use default value compression snappy codec instead respect value orc compress writing orc files orc compress overridden users set compression options
481,0,spark can not select datatable stored orc file created hive hives park supports steps use hive create table tb txt stored txt load data use hive create table tb orc stored orc insert data tablet b txt example create table tb orc stored orc select tb txt uses park select tb orc error occurs java lang illegalargumentexception field nid exists park can not select datatable stored orc file created hive hives park supports
482,0,spark error occurs execute sql statement includes nvl functions park supports error org apache spark sql analysis exception can not resolve nv lb new user due datatype mismatch input function coalesce type string int line pos state codes park error occurs execute sql statement includes nvl functions park supports
483,1,move regexp unit tests regexp expressions suite
484,0,multivariate online summarizer minmax method use judgement nnz weight sum cause numeri al problem make result unstable example add two vector weight e weight e using multivariate online summarizer minmax get min vector max vector right result min vector max vector bug reason eee double type floating rounding different accumulating merging order may cause different result weight e weight e lines data weight e using input data order listed get result min vector max vector input data order following weight e lines data weight e weight e result min vector max vector eee add times e double type ee add time see e double type potential numerical problem multivariate online summarizer minmax
485,0,creating data source table without explicit specification schema select clause silently ignore bucket specification clustered sorted example instead issue error message silent ignore bucket specification creating table using schema inference
486,0,caching multiple replicas blocks currently broken following examples show replication happen various use cases run using spark preview local cluster mode replicate data executors show classnotfoundexception examples worked fine showed expected results spark caching data replication replicate data
487,1,data frame drop supported multi columns spark api make python api also support data frame drop supported multi columns spark api make python api also support
488,0,calls park r sql error pops instance https git hub com apache spark blobfbccbddfaaecfrpkgr sql context rl can not use spark r sql
489,0,scenario launch jobq allow grow cluster utilization wait mins job complete cluster available takes hr job complete note wait less time issue sometimes occur appears job least complete launch jobq preemption occurs q shrinking job allow cluster utilization point job basically halts progress job continues execute normal finishes job either fails attempt restarts time attempt fails job already complete meaning second attempt full cluster availability finishes job remains current progress simply finish waited hrs finally killing application looking error log constant error message warn netty rpc endpoint ref error sending message message remove executor container container host ip number sec internal preempted x attempts observations led believe application master know container killed continuously asks container remove executor eventually failing attempt continue trying remove executor done much digging online anyone else experiencing issue come nothing spark application handling preemption messages
490,0,hive client impl method create database see parameters fieldset null hive client impl throws npe reading database custom meta store
491,0,spark applications running mesos throw exception upon exit follows applications result affected error issue simply reproduced launching spark shell exit running following commands root causes park context stop mesos coarse grained scheduler backend stop calls coarse grained scheduler backends to platters ends messages stop executors also stop driver endpoint without waiting actual stop executors mesos coarse grained scheduler backend stop still waits executors stop timeout wait mesos coarse grained scheduler backend status update generally called update executors status turn remove executor called time driver endpoint available mesos spark application throws exception exit
492,1,currently filters timestamp type decimal type pushed orc data source although orc filters support support pushing filters decimal timestamp types orc
493,0,run sql get transformation script error python script like error message query sql spark sql transformation script got failure python script
494,0,right yarn shuffle service swallow errors happen startup log causes two undesirable things happen block handler remain null error happens every request shuffle service cause npe nm running containers may assigned host fail register shuffle service example first yarn shuffle service throw errors fails start
495,1,move hive hack data source table hive external catalog
496,0,job generator always evaluates true checkpoint duration always set equal batch duration https post imgorgudyltijs park streaming job generator png width imageurl https post imgorgudyltijs park streaming job generator png batches might get marked fully processed job generator
497,0,create table pointing parquet json datasets without specifying schema describe table command show schema shows schema table inferred runtime describe table show schema table table schema inferred runtime describe table command show schema
498,0,doable spark bump master version snapshot
499,0,hello using apache spark executing bisecting k means algorithm specific data set dataset details k input vector kk memory assigned gb per node number node still kos working fine set k fails java util nosuchelementexception key found suspect failing lack resources somehow exception convey anything spark job failed please someone point root cause exception failing exception stack trace issue failing giving explicit message failed bisecting k means algorithm failing java util nosuchelementexception key found
500,1,seems equal null safe filter missed batch prune ing partitions cached tables supporting improve performance roughly vary running codes support partition batch pruning equal null safe predicate in memory tables can exec
501,0,ct as partition clause got wrong error message example wrong messages ct as partition clause
502,0,three broadcast variables created beginning word vec fit never deleted un persisted seems cause excessive memory consumption driver job running hundreds successive training undeleted broadcast variables word vec causing oom long runs
503,0,streaming query explains how sn data arrives pretty confusing improve streaming query explain data arrives
504,0,trying builds park fails maven gone https www apache org dyn closer lua action download filename maven maven binaries apache maven bin targ z something control saw latest builds updating exists various mirrors at least one shit missing http mirrors koehn com apache maven maven maven missing mirror breaks older builds
505,0,create table like schemas park sql error struct type sql spark sql failed create table due catalog string error
506,1,external shuffle service essentials park order better monitor shuffle service added various metrics shuffle service external shuffle service source metric system add metrics source external shuffle service
507,0,basically issues park https issues apache org jira browse spark linear regression coefficients features std unnecessarily serialized stages least squares aggregator linear regression serializes unnecessary data
508,1,users try implement data source api extending relation provider crea table relation provider hit error resolving relation data source apis extending relation provider crea table relation provider without schema relation provider
509,0,two configs relevant anymore spark removes park sql native views park sql native view canonical config
510,0,catches no such method error nosuchmethodexception thrown utils wait for process
511,0,spark webui wrong value num completed tasks assigned variable num skipped tasks see attachment references park webui wrong value num completed tasks assigned variable num skipped tasks
512,1,different leafnode s meta store relation simple catalog relation predefined alias used change qualifier node however based existing alias handling alias put subquery alias pr separate alias handling meta store relation simple catalog relation make consistent nodes example example query meta store relation converted logical relation note optimized plans simple catalog relation existing code always generates two subqueries thus change needed remove alias meta store relation simple catalog relation
513,0,following java retagrddtallskinnyqrrow matrix
514,0,df column struct type rows sample data https issues apache org jira browse spark gives sparks park change null behaviours park null clause gives false nested empty column
515,0,creating view common user error number columns produced select clause match number column names specified create view example given table columns error confusing strange errors creating view unmatched column num
516,1,currently reports spark query performance regression large queries issue speeds sqlquery processing performance removing redundant consecutive execute plan call data set of rows function dataset instantiation specifically issue aims reduce overhead sqlquery execution plan generation real query execution see results park webui please use following query script speed sqlquery performance removing redundant execute plan call dataset
517,1,logical plan insert into hive table useless thus remove codebase remove insert into hive table logical plan
518,0,query containing limit table sample statistics could zero results correct could cause huge performance regression example statistics dfdf zero statistics values never zero otherwise size in bytes binary node also zero product children incorrect statistics queries containing limit table sample
519,0,illegal inputs limit table sample
520,0,trying use custom classpath meta store jars spark sql hive meta store jars pointing filesystem path ran following issue issue mr version packaged anywhere spark code isolated client loader ever tries parent classloader loading had oop classes configuration even though class list files spark sql hive meta store jars spark never tries load isolated client loader ignores needed had oop classes presents park loader
521,0,weird corner case users may hit issue schema array field whose element type struct struct one one field field named element following spark shell snippets park reproduces bug exception thrown nested struct field name happens element used dedicated name element type container group standard level layout lead ambiguity currently spark x snapshot master chosen done fix issue giving standard level layout higher priority trying match schema patterns array struct single field name element decoded parquet files written spark
522,1,currently optimizer may reorder predicates run efficient nondeterministic condition change order deterministic parts nondeterministic parts may change number input rows example may call rand different times therefore output rows differ improve pushdown predicate rule pushdown predicates cur rectly nondeterministic condition
523,0,following works spark anymore sparks park result understand tables columns might supported sql case would say df no cols register temp table call fail descriptive error select temp table cols fails
524,0,update please start comment https issues apache org jira browse spark focused comment id page com atlassian jira plugin system issue tab panels comment tab panel comment assume problem results performance problem reading parquet files original issue description test parquet file many nested columns g partitions spark x slower spark mins park minx slower used basic profiler task cumulative times park secs park sec expect drop performance know prepare sample data show problem ideas public data many nested columns spark performance regression reading parquet using ppd non vectorized reader
525,0,underlying file changes confusing users see filenotfoundexception would great following append message filenotfoundexception workaround explicitly metadata refresh make metadata refresh worktemporary tables views make metadata refresh work datasets data frames introducing dataset refresh method improve metadata refresh
526,0,linkage error user codes park executors get killed immediately great user experience especially shared environments multiple applications multiplexed one context linkage error crash spark executor
527,0,running spark r unit tests randomly following error failed error piperddrddstestrddrorg apache sparks park exception job aborted due stage failure task stage failed times recent failure lost task stage tid localhost org apache sparks park exception r computation failed ignoring sigpipe signal calls source l apply l apply fun write raw write bin execution halted can not open connection calls source compute func fun system write lines file addition warning message file con w can not open file tmprtmpgraufiledeefcb file directory execution halted org apache spark api rr runner computer runner scala org apache spark a pir base rr dd computer rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark scheduler result task run task result task scala org apache spark scheduler task run task scala org apache spark executor executor task runner run executor scala java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java related daemon r worker mode defaults park r launches r daemon worker per executor forks r workers daemon necessary problem for king r worker forked r processes share temporary directory documented https state thz chr manual r devel library base html tempfile html forked r worker exits either normally caused errors cleanup procedure r delete temporary directory affect still running forked r workers temporary files created temporary directories removed together also future r workers forked daemon affected use tempdir tempfile get tempo a ray files fail create temporary files already deleted session temporary directory order daemon mode work problem circumvented current dameon rr workers directly exits skipping cleanup procedure r shared temporary directory deleted capture errors r workers daemon r avoid deletion r session temporary directory
528,1,spark sql currently falls back hive xpath related functions implement xpath user defined functions
529,0,sql context import data stream reader
530,1,duplicated code options simplify cleanup options data frame reader api python
531,1,patch removes blind fall back hive functions instead creates whitelist adds small number functions white liste ones intend support long run spark whitelist list hive fall back functions
532,0,koert kuipers identified pr https git hub com apache spark pull changed behavior load api change load api add value path options thank add option path back load api data frame reader users specify one one path load api add path option back load api data frame reader
533,0,query failed expected empty table created empty table remains create table select fails
534,0,quote select percentile cast id bigint cast double temp bla quote works quote select percentile cast id bigint temp bla quote throws quote error query handler hive udf org apache hadoophiveqludfudaf percentile org apache had oop hiv eql exec no matching method exception matching method class org apache hadoophiveqludfudaf percentile bigint decimal possible choices func bigint array func bigint double line pos quote percentile needs explicit cast double
535,0,currently csv data source write date type timestamp type would nicer write dates timestamps formatted string like json data sources also csv data source currently supports date format option read dates timestamps custom format might better option applied writing well csv data source write date timestamp correctly
536,1,collect set can not map typed data map type data implement equals find map type collect set queries fail improve type check collect set check analysis
537,0,turn hive support following query generates confusing error message unresolved operator creating table select without enabling hive support
538,0,spark streaming documentation recommends application developers create static connection pools clean pool add shutdown hook problems park shutdown hook executor called first submitted job second subsequent job submissions shutdown hook executor invoked problem seen using java problem seen using spark looks like bug caused modification https issues apache org jira browse spark steps reproduce problem installs park submit basics park application import org apache sparks park contexts park conf object my pool def print to file fjavaiofileopjavaio printwriter unit valp new java io printwriter f try opp finally pclose def myfunc def create evidence print to file new java io file var tmp evidence txt pp println evidence sys add shutdown hook create evidence object basics park def main args array string val spark conf news park conf set appname basic piva lsc news park contexts park conf sc parallelize for each println f my pool myfunc sc stop see var tmp evidence txt created delete file submit second job see var tmp evidence txt longer created second submission use java spark evidence file created second subsequent submits utils scala terminate process call process destroy forcibly process destroy fails
539,1,embed partitioning logic file source strategy apply making function long small refactoring move functions eventually would able move partitioning functions physical operator rather physical planning mover dd creation logic file source strategy apply
540,0,currently column related comment attribute stored metadata example compared existing sql interface user friendly add comment attribute defining struct field add new methods comments struct field struct type
541,1,earlier releases package grouping generated java apidocs see https park apache org docs api java index html however disappeared https park apache org docs api java index html rather fixing suggest removing grouping might take time fix manual process update groupings park build scala one complained missing groups since remove package grouping gen javadoc
542,0,env scala java recently deprecated setlabel colchis q selector model switching multiline workaround reported bug gen javadoc https git hub com typesafe hub gen javadoc issues single line scala doc deprecate annotation would break gen javadoc
543,1,need hashcode eu q als unsafe map data beha iv our unsafe map data different array based map data remove hashcode eu q als array based map data
544,0,heap heap variants columnvector reserve unfortunately overflow reserving additional capacity reads avoid negative arraysize exception reserving additional capacity vectorized column reader
545,0,listing filecatalog implementation list leaf files shown number user provided paths less values park session session state conf parallel partition discovery threshold use parallel listing different number children inner dir larger threshold use parallel listing listing filecatalog list parallel anymore
546,0,forgot getter drop last one hot encoder get drop last missing one hot encoder
547,0,cluster py spark python py spark driver python environment variables set needed using non system python eg usr bin anaconda bin python unable override per submission yarn cluster mode using spark submit case via livy submit overrides park submit master yarn deploy mode cluster conf spark yarn app master envp y spark driver python python conf spark yarn app master envp y spark python python probe py environment variable values override conf settings workaround unsetenv vars always possible eg submitting batch via livy pass parameters spark submit expectation conf values override environment variables fix change order application conf env vars yarn client related discussion https issues cloud era org browse livy backporting would great unblocking set python via spark submit yarn cluster mode py spark python py spark driver python set
548,0,get similar error using complex types aggregator sure issue something else gen generate mutable projection generate generate mutable projection scala org apache spark sql executions park plan new mutable projections park plan scala org apache spark sql execution aggregate sort aggregate exe canon fun do execute a non fun a non fun apply sort aggregate exec scala org apache spark sql execution aggregate sort aggregate exe canon fun do execute a non fun a non fun apply sort aggregate exec scala org apache spark sql execution aggregate aggregation iterator generate process row aggregation iterator scala org apache spark sql execution aggregate aggregation iterator aggregation iterator scala org apache spark sql execution aggregate sort based aggregation iterator sort based aggregation iterator scala org apache spark sql execution aggregate sort aggregate exe canon fun do execute a non fun apply sort aggregate exec scala org apache spark sql execution aggregate sort aggregate exe canon fun do execute a non fun apply sort aggregate exec scala org apache spark rdd rd dan on fun map partitions internal a non fun apply apply rdd scala org apache spark rdd rd dan on fun map partitions internal a non fun apply apply rdd scala org apache spark rdd map partitions rdd compute map partitions rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark rdd map partitions rdd compute map partitions rdd scala org apache spark r ddr dd compute or read checkpoint rdd scala org apache spark r ddr dd iterator rdd scala org apache spark scheduler shuffle map task run task shuffle map task scala org apache spark scheduler shuffle map task run task shuffle map task scala org apache spark scheduler task run task scala org apache spark executor executor task runner run executor scala java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java no format aggregator fails tungsten error complex types used results partial sum
549,0,application yarn application state finished final application status failed invoking spark submit command line got exception submit spark launcher got state finished means app succeeded also fact test yarn cluster suite assertfalse condition fail test yarn cluster mode return consistent result command line spark launcher
550,0,since uses spark context update set job group clear job group cancel job groups park rap i requires c
551,1,vector udt matrix udt private apis user defined type private spark however order let developers implement transformers estimators expose types public api simply implementation transform schema transform etc otherwise need get data types using reflection note mean expose vector udt matrix udt classes method static value returns vector udt matrix udt instance datatype return type two ways implement following data types java sql java users need extra define data types scala expose vector udt matrix udt public api
552,0,several bugs found caused integer overflows tungsten jira taking final pass release reduce potential bugs issues least following raise exception early instead later throwing negative arraysize slow might cause silent errors document clearly largest arraysize support data frames reproduce one issues sufficient arraysize checks avoid integer overflows tungsten
553,0,python u dts work well one example py spark sql python u dts work well
554,0,jdbc source wrong results lower bound larger upper bound column partitioning
555,1,issue adds read orc write orcs park rap i parity add read orc write orcs park r
556,1,to do generic array data class eliminate boxing unboxing primitive array described https git hub com apache spark blob master sql catalyst src main scala org apache spark sql catalyst util generic array data scala l would good prepare generic array data implementation specialized primitive array eliminate boxing unboxing view runtime memory footprint performance prepare generic array data implementation specialized primitive array
557,1,debug versions park tutorials debugging streaming apps would nice text based socket source similar ones park streaming clearly marked debug users try run production applications type source can not provide ha without storing lot states park add debug socket source structured streaming
558,1,add new api method called g apply collects park data frame g apply spark data frame collect result back r compared g apply collect g apply collect offers performance optimization well programming convenience schema needed provided similar d apply collect add g apply collects park data frame
559,0,data frame read json string removed covered data frame read json string varargs breaks compilation compiler needs method single arg example works compile data frame read json path compatibility broken spark
560,0,logistic regression aggregator class used collect gradient updates ml logistic regression algorithm class stores reference coefficients array length equal number features also stores reference array standard deviations length num features also task completed serializes class also serializes copy two arrays arrays need serialized gradient updates aggregated causes issues performance issues number features large trigger excess garbage collection executor much excess memory results serializing num features excess data multiclass logistic regression implemented excess num features num classes num features ml logistic regression aggregator serializes unnecessary data
561,0,observed debugging https issues apache org jira browse spark fix disable default serialization debugger run infinite loop
562,0,notebook https data bricks prod cloud front cloud data bricks com public ececeaaafbcfc latest html demonstrates bug obvious issue nested u dts supported udt python looking exception thrown seems encoder java end tries encode udt java class exist python only udtpysparksqlpythonudts support nested types
563,1,interface method file format prepare read added pr https git hub com apache spark pull handle special case libs vm data source however semantics interface method intuitive returns modified version data source options map considering libs vm case easily handled using schema metadata inside infer schema remove interface method keep file format interface clean remove file format prepare read
564,0,issues current reader behavior text without args returns empty df columns inconsistent expected text always return df value stringfield text file without args fails exception reason expected df returned text value field orc varargs inconsistent others json single arg removed caused source compatibility issues spark user specified schema respected text csv used args spark solution implementing following format single argument method var arg method json parquet csv text means adding json string etc orc means adding orc varargs remove special handling text csv etc returns empty data frame fields rather pass empty sequence paths data source let data source handle right eg text data source return empty df schema value string harmonize behavior data frame reader text csv json parquet orc
565,1,would good additional implementation uses dense format unsafe array data reduce memory footprint current unsafe array data implementation uses sparse format useful unsafe array data created method from primitive array null value introduce additonal implementation dense format unsafe array data
566,0,document contract encoder serializer expressions
567,1,api parity scala api refer https issues apache org jira browse spark add varargs type drop duplicates functions park r
568,1,rights park load hive site xml based users feedback seems make sense still load conffile originally file loaded load hive conf class settings retrieved create hive conf instances let avoid using way load hive site xml instead since hive site xml normal had oop conffile first find url using classloader use had oop configuration add resource add hive site xml default resource configuration add default resource load confs please note hive site xml needs loaded had oop confused create metadata hive bring back hive site xml supports park
569,1,sometimes make sense specify partitioning parameters eg write data datasets data frames jdbc tables streaming for each writers probably add checks data frame writer add assert not partitioned check data frame writer
570,1,expose coda hale metrics codegen source text size long takes compile size particularly interesting since jvm hard limits large methods get metrics codegen size perf
571,1,sparks bt build currently uses forks bt pom reader plugin depends fork via sbt subproject cloned https git hub com scrap codes sbt pom reader tree ignore artifact id unnecessarily slows initial build fresh machines also risky risks build breakage case git hub repository ever changes deleted order address issues propose publish prebuilt binary forked sbt pom reader plugin maven central orgs park project namespace publish spark forked sbt pom reader maven central
572,1,corresponding python api k means summary would nice add k mean summary k mean spy spark
573,1,data frame writer insert into includes analyzer stuff move analyzer move analyzer stuff analyzer data frame writer
574,1,making default params writable default params readable public help users transformers save load pipelines making public safe even change internal formats make default params readable writable public apis
575,0,better error message writing bucket ing data
576,1,use latest spark session replace existing sql context ml lib replace sql contexts park session ml lib
577,0,updates park mls park ml lib migration guide mlq a migration guide update
578,1,see parent tasks park python conver age py spark ml linalg
579,1,see parent tasks park python conver age ml regression module
580,1,see parent tasks park python conver age ml classification module
581,1,see parent tasks park python conver age ml recommendation module
582,1,see parent tasks park bryan c component python coverage ml feature
583,1,issue replaces deprecated sql context occurrences spark session mlml libmodule except following two classes two classes use sql context function arguments readwrite scala tree models scala replace sql contexts park session mlml lib
584,1,see containing jira details spark mlq a scala apis audit feature
585,1,see containing jira details spark mlq a scala apis audit evaluation tuning
586,0,fix slow tests
587,1,support partitioned parquet format file stream sink added sparkle t add support partitioned csv json text format add support writing partitioned csv json text formats structured streaming
588,1,several classes methods deprecated creating lots build warnings branch issue identify fix items with sgd classes change make class deprecated object deprecated public class constructor deprecated public use require deprecated api need keep non deprecated private api since can not eliminate certain uses python api streaming algs examples use python ml lib api change using private constructors streaming algs warning sun deprecate classes examples deprecate change ones use deprecated apis multiclass metrics fields precision etc eliminate ml lib build warnings deprecation s
589,0,made several changes als necessary runtests avoid performance regression test synthetic datasets million ratings billion ratings cc ml nick holden k time run large scale performance tests links results spreadsheet https docs googlecom spreadsheets ixl is fxczstchpvpoozecovszdtze ks edit usp sharing raw results spark https docs googlecom document tlwfcvzwjuxvgfahdtkurvyr ykfvflllpneditusp sharing raw results spark https docs googlecom document qllxdgxjagosqzmbbsncjthh gjjcqnedieeditusp sharing performance test als spark
590,0,currently explain query whole stage codegen looks like problem plan looks much different logical plan make us hard understand plan especially logical plan showed together improve explain whole stage codegen
591,1,umbrella ticket list issues found apis releases park sql api audit
592,1,removed classes spark user uses incompatible library may see classnotfoundexception better give instruction ask people using correct version display better message finding classes removed spark
593,1,open apis converting new old linear algebra types spark ml lib linalg vector asml vectors from ml sparse dense matrices made private originally useful users transitioning workloads make ml lib ml linalg type conversion apis public
594,1,using asml convert ml lib vector matrix ml vector matrix using correct given conversion actually shares underline data structure result pr to breeze changed as breeze private api result affect user application change to breeze as breeze vector matrix
595,1,parent jira track work building kafka source structured streaming design doc initial version kafka source https docs googlecom document rwextqeaofrsmqbbruvfelpq red it usp sharing old description structured streaming support kafka yet personally feel like time based indexing would make much better interface pushed back kafka https c wiki apache org confluence display kafka kip add time based log index structured streaming support consuming kafka
596,1,since ml evaluation supported save load scala side supporting python side straightforward easy py spark ml evaluation support save load
597,1,pickle rs new old vectors implemented python ml lib api separates park ml libs park ml implements park ml python instead settarget since private apis implement python pickler sml vector ml matrix spark ml python
598,0,spark use vector implicit sas ml example consider update qahttpsgithubcomdbtsais park blobebacfbabfffac examples src main scala org apache spark examples ml data frame example scala example code use vector implicit sas ml from ml
599,1,spark makes k means model store clusters one perrow k means model load method needs updated order load models saved spark makes park mlk means model load backwards compatible
600,1,generate currently support code generation lets add support cg important generators explode json tuple implement code generation generate
601,1,add interface glr summaries python feature parity python api generalized linear regression summary
602,1,lazy file region created create file descriptor sendfile see https issues apache org jira browse spark change pushed back netty support things default file region https git hub com net tyne tty issues https git hub com net tyne tty commit bcf b feb looks like went final believe time created lazy file region final using final able usenet ty class directly remove lazy file region
603,1,current data set register temp table actually materialize data considered creating temp view deprecate create new method called dataset create temp view replace if exists boolean default value replace if exists false register temp table call dataset create temp view replace if exists true deprecate register temp table add dataset create temp view
604,1,implement repartition by column data frame allow us run r functions partition identified column groups d apply methods park r implement repartition by column data frame
605,0,upgrading test that package version new warnings new failure found spark r test cases warnings multiple packages produce warning test client r deprecated spark jars spark packages comma separated strings test context r deprecated spark surv reg test ml libr deprecated date functions data frame tests park sqlr deprecated please use expect gt instead date functions data frame tests park sqlr deprecated please use expect gt instead date functions data frame tests park sqlr deprecated please use expect gt instead method data frame synonym collect tests park sqlr deprecated failure show df tests park sqlr produced output changes releases test that found https git hub com hadley test that blob master news md fix warnings failures park r test cases test that version
606,1,spark rs park k means take data frame double columns different ml methods implemented support r model formula add support well support formulas park k means spark r
607,0,see following repro can not run aggregate function explode result
608,0,request adding some things park rap i like version added py spark api since scala java api add since tag r oxygen documentation spark rap i methods
609,1,validation metrics train validation split model also supported py spark ml tuning py spark train validation split model support validation metrics
610,1,json schema inference spend slot time infer field number techniques speed including eliminating unnecessary sorting use inefficient collections improve performance json schema inference infer field step
611,1,yarn shuffle service currently picks directly yarn local dirs store level dbfile yarn added interface had oop get recover path get location storing change use get recovery path mean use reflection similar check existence though since exist had oop yarn shuffle service use yarn get recovery path level db location
612,1,currently whole stage codegen version tungsten aggregate support subexpression elimination support subexpression elimination whole stage codegen version tungsten aggregate
613,1,issues aims add new foldable propagation optimizer propagates foldable expressions replacing attributes aliases original foldable expression optimizations take advantage propagated foldable expressions eg eliminate sorts optimizer handle following case case previous implementation literals foldable expression eg order abc foldable ordinals eg select abc order foldable aliases eg select x abc z order xz add foldable propagation optimizer
614,1,currently way users propagate options underlying library rely had oop configurations work example various options parquet mr users might want set data source api expose per job way set patch propagates user specified options also had oop configuration propagate data source options had oop configurations
615,1,since spark breaks behavior hashing tf try enforce good practice removing native hashing alg options park mlp y spark ml leaves park mllibpysparkmllib alone removes park ml hashing tf hashing alg option
616,1,looks like headmaster branch spark uses quite old version jetty v announcement security vulnerabilities notably versions address recently left webui port openserver compromised within days albeit upgrade security improvement made current version clearly vulnerable upgrade jetty latest version
617,1,break sqlquery suite smaller testsuites
618,1,spark switched use generic array datastore indices values vector matrix u dts however generic array data specialized primitive types might hurt ml lib performance badly consider either specialize generic array data use different container cc cloud f any huai vector udt matrix udt take primitive arrays without boxing
619,1,current master ml methods spark r make change might want avoid name collisions different signature use mlk means ml glm etc sorry discussing api changes last minute think would better consistent signatures spark rcc shivaram joseph k by an bo liang make mla piss park r consistent
620,1,umbrella ticket reduce difference sql core sql hive ultimately difference ability run hive serdes well udfs merge functionality hive module sql core module
621,1,spark two parsers anymore single one merge hives q last builders parks q last builder
622,0,test cases hive compatibility suite subquery enable better coverage enable tests hive compatibility suite subquery
623,1,current benchmark framework runs code block several iterations reports statistics however way exclude per iteration setup time overall results allow custom timing control micro benchmarks
624,1,rename up streams input rd ds whole stage codegen
625,1,provide apis vm algorithm data frames would recommend using owl qn rather wrapping spark ml lib sgd based implementation api mimic existing spark ml classification a piss park ml api linear svm
626,0,spill size aggregate sort broken data size actual peak memory sql metrics broken whole stage codegen enabled
627,1,issue aims implement asserttrue function hive generic udf function https git hub com apache hive blob master ql src java orgapachehadoophiveqludf generic generic udf asserttrue java since following function description hive please note hive false values false null empty string spark intentionally designed use implicit typecasting boolean type add asserttrue function
628,1,issue aims exposes calabro und function python rap ib round function implemented spark extending current round function used following semantics hive https git hub com apache hive blob master ql src java orgapachehadoophiveqludf generic round utils java add b round function python r
629,1,goal ticket simplify externalinterface internal implementation accumulators metrics unnecessarily convoluted able simplify quite bit umbrella ticket iteratively create new tasks investigation goes high level would like create better abstractions internal implementations well creating simplified accumulator v externalinterface involve complextype hierarchy simplify accumulators task metrics
630,1,issue aims add bound function aka banker round extending current round implementation hive supports b round since language manual https c wiki apache org confluence display hive language manual udf add b round function
631,0,poking around noticed places still referred spark assembly jar updated made sense also updated usage sections park submit since use run example argument run examples park submit mostly undocumented minor doc usage changes related removals park assembly
632,1,error r code using rd dapi always get error message error return status argument length zero useful think might better catch r exceptions how instead improve error messages rd dapi
633,1,right filter push works project aggregate generate join pushed many plans improve filter push
634,1,currently disable codegen case when number branches greater case when max num cases codegen would better value nonpublic config defined sql conf spark sql codegen max case branches config option
635,1,currently union takes intersect constraints children others dropped try merge together improve constraints propagation union
636,1,great sql functions if null null if nvl nvl meaning functions could found oracle docs sql function if null null if nvl nvl
637,1,spark data frame alias dataset row ml lib api actually works types dataset accept dataset instead maps dataset java source compatible change accept dataset instead data frame ml lib apis
638,1,refactor object operator framework make easy eliminate serialization s
639,0,right string data sources can had oop files xxx without information tablename path since kind regression show tablename path string data sources can
640,0,according spark code style guide https c wiki apache org confluence displays parks park code style guides park code style guide indentation issue add new scala style rule prevent followings add new scala style no scala doc prevent scala doc style multiline comments
641,1,order upgrade kryoneedshadekryo custom hive fork shade k ryo custom hive fork
642,1,parser utils parse utils utility collections use parsing process name used similar think merge also original unescape sql string method may fault u style character literals passed method unescaped successfully merge parser utils parse utils
643,1,currently many functions show usages like followings exceptions cube grouping grouping id roll up window functions show usages command desc function
644,1,right operations existing functions session catalog really check function exists add check avoid check command session catalog needs check function existence
645,0,concept partitioning associated physical tables disable supports partitioned views defined following three commands hiv eddl manual https c wiki apache org confluence display hive language manual ddl language manual ddl create drop alter view exception thrown users issue three ddl commands throw exceptions ddls partitioned views create view alter view
646,1,unit test k means summary spark ml items could fixed add since version k means summary class modify cluster sizes method match gmm method robust empty clusters case support some time see prs park unit tests park mlk means summary
647,1,time windowing function window added datasets jira track status rpython sql api date set time windowing api python r sql
648,1,decouple deserializer expression resolution object operator
649,1,to local iterator rdd super slow optimized implementation dataset data frame add to local iterator dataset
650,1,use single objects park r wrappers https git hub com apache spark blob master ml libsrc main scala org apache spark mlrs park r wrappers scala wrap method calls glm k means spark r quite hard maintain refactor separate wrappers like a ft survival regression wrapper naive bayes wrapper package name spa kr mlr instead spark mla pir refactor gl ms codes park r wrappers
651,1,use single objects park r wrappers https git hub com apache spark blob master ml libsrc main scala org apache spark mlrs park r wrappers scala wrap method calls glm k means spark r quite hard maintain refactor separate wrappers like a ft survival regression wrapper naive bayes wrapper package name spa kr mlr instead spark mla pir refactor k means codes park r wrappers
652,0,spark tests compilation java sources using lambdas guarded behind java tests maven profile currently build runtests result tests longer compile fix tests set automated ci break fix java tests profile runtests jenkins
653,1,running spark job spilling lot data reduce phase see significant amount cpu consumed native snappy array copy method please see stack trace stack trace org x erial snappy snappy native y jp array copy native method or gx erial snappy snappy native array copy snappy native java org x erial snappy snappy array copy snappy java org x erial snappy snappy input stream raw read snappy input stream java org x erial snappy snappy input stream read snappy input stream java java io datainputstream read fully datainputstream java java io datainputstream read long datainputstream java org apache spark util collection unsafe sort unsafe sorters pill reader load next unsafe sorters pill reader java org apache spark util collection unsafe sort unsafe sorters pill merger load next unsafe sorters pill merger java org apache spark sql execution unsafe external row sorter next unsafe external row sorter java org apache spark sql execution unsafe external row sorter next unsafe external row sorter java reasons pill reader lot small reads underlying snappy compressed stream snappy input stream invokes native jni array copy method copy data expensive fix snappy java use non jni based system array copy method case significant amount cpu consumed snappy native array copy method
654,1,reimplement typed aggregate expression declarative aggregate
655,1,currently method submit stage waiting stages called every iteration event loop dag scheduler submit waiting stages necessary related stage status case try submit waiting stages parent stages successfully completed elimination improved ag scheduler performance eliminate unnecessary submit stage call
656,1,execute multiple python udfs single batch
657,0,would helpful network performance investigation log time spent connecting resolving host add logs help investigate network performance
658,0,spark adds program atic way dump generated add sqlcommand printing generated code debugging
659,1,remove trait query able
660,1,remove consume child always create code unsafe row variables simplify whole stage codegen interface
661,1,parse create function command order support native drop function command need parse parse drop function ddl command
662,1,naive bayes expect inputs individual observations practice people may frequency table instead useful us support instance weights handle case support weighted instances naive bayes
663,1,would nice refactor memory store unit tested without constructing full block manager needing mock tons things refactor memory store testable independent block manager
664,1,improves park status tracker also track executor information
665,1,kept sql context implicit s object binary backward compatibility spark xseries makes sense api sql implicit s since single class defines sql implicit s move string to column implicit class sql implicit s
666,1,currently key fit within long build hashmap unsafe hashed relation converted bytes to bytes map serialization deserialization build bytes to bytes map directly better memory efficiency build bytes to bytes map hashed relation
667,1,currently spark history server rest api provides functionality query applications application start time range based min date maxdate query parameters lacks support query applications end time jira proposing optional min end date max end date query parameters filtering capability based parameters spark history server rest api functionality used following queries applications finished last x minutes applications finished time applications finished x time time applications started x time finished time backward compatibility keep existing min date maxdate query parameters continue support filtering based start time range add functionality spark history sever api query applications end time
668,1,per discussion mailing list please see http mail archives apache org mod mbox spark devmboxccagfavrbhwyyknvb slcmptsduulgewwdnmnvysxg mail gmail come would nice specify custom coalescing policy current coalesce method allows user specify number partitions can not really control much need feature popped wanted merge small files coalescing size add support custom coalesce rs
669,1,make subquery holder inner class
670,1,sort order contain reference effect sorting remove noop sort order optimizer remove noop sort order sort
671,1,per non gli suggestions things remove non vectorized parquet reader code support remaining types big decimals move logic determine parquet reader used planning complex types fall back parquet mr reader cleanup extend vectorized parquet reader
672,1,add support caching serialized data heap within process e using direct buffers sun misc unsafe expand jira later detail filing placeholder add support heap caching
673,1,block persisted memory store serialized storage level current memory store put iterator code unroll entire iterator java objects memory turnaround serialize iterator obtained unrolled array inefficient doublespeak memory requirements instead think incrementally serialize blocks unrolling downside incremental serialization fact need deserialize partially unrolled data case enough space unroll block block can not dropped disk however hoping memory efficiency improvements outweigh performance losses result extra serialization hopefully rare case incrementally serialize blocks unrolling memory store
674,0,hive tests fail sql generation failed
675,0,patch spark added number mima excludes cases missed due old way programatically generating excludes generate mima ignores park need audit additional excludes added make sure none represent unintentional incompatibilities fixed audit mima excludes added spark make sure none unintended incompatibilities
676,1,support order position sql eg controlled config options park sql group by ordinal support group ordinal sql
677,1,separate linear algebra standalone module without spark dependency simplify production deployment call new module ml lib local might contain local models future major issue remove dependencies user defined types package name changed ml lib ml example vector changed org apache spark ml lib linalg vector org apache spark ml linalg vector return vectortype new ml pipeline one ml package however existing ml lib code touched result potentially break api also vector loaded ml lib vectors park sql vector automatically converted one ml package separate local linear algebra standalone module without spark dependency
678,1,recently fast serialization introduced collecting data frame dataset technology used collect limit operator apply fast serialization collect limit
679,1,logging made private spark move users would able create logging trait avoid changing code alternatively also provide compatibility package adds logging move org apache spark logging org apache spark internal logging
680,1,class tags available constructing shuffled rdd use automatically use k ryo shuffle serialization rdd types guaranteed compatible kryoegrdds whose key value combiner types primitives arrays primitives strings likely result large performance gain many rd dapi workloads automatically use k ryo serializer shuffling rd ds simple types
681,1,continues works parks parks park exposer like model summary family link functions exposer like summary statistics spark r glm family link functions
682,0,common many sql operators care reading null values correctness currently achieved performing is not null checks relevant columns perrow basis pushing null filters parquet vectorized reader bring considerable benefits especially cases underlying data contain nulls contains nulls filter rows null attributes parquet vectorized reader
683,1,instead storing serialized blocks individual byte buffers block manager capable storing serialized block multiple chunks occupying separate bytebuffer change help improve efficiency memory allocation accuracy memory accounting serializing blocks current serialization code uses bytebuffer output stream doubles allocates backing byte array increase speak memory requirements serialization since need hold extra memory expanding array addition currently account extra wasted space end bytebuffer backing array megabyte serialized block may actually consume megabytes memory switching storing blocks multiple chunks able efficiently trim backing buffers space wasted change also prerequisite able cache blocks larger gb although full support depends several changes bee implemented yet store serialized blocks multiple chunks memory store
684,1,currently spark allows cluster managers viz yarn mesos standalone spark used newer different use cases need allowing cluster managers manages park components one use case embeddings park components like executor driver inside another process may datastore allows colocation data processing another requirement stems use case executors driver take parent process go components relaunched inside process jira requests two functionalities support external cluster managers allow cluster manager clean tasks without taking parent process add support pluggable cluster manager
685,1,code go session state catalog brings two small benefits reduces internal dependency sql context removes another public method java java obey package private visibility importantly according designs park need claim catalog function user facing public functions rather internal field remove sql context catalog internal method
686,1,general better internal classes depend external class case sql context reduce coupling user facing apis internal implementations remove internal classes dependency sql context
687,1,describe command take table identifier ask metadata catalog table information remove describe command dependency logical plan
688,1,introduced local operators org apache spark sql execution local package never fully wired engine actually use still plan implement full local mode probably going fairly different current iterator based local mode would look like let remove always introduced future looking branch remove org apache spark sql execution local
689,1,generate code join copy output row could multiple output row single input row could avoid copy join join generate multiple output rows single input row avoid copy whole stage codegen joins
690,1,reading data disk store attempting cache back memory store guard race conditions multiple readers attempting cache block memory guard race condition caching spilled bytes memory
691,0,given two task result take seconds fetch results rpc may designed handle large block use block manager currently based spark rpc message maxsize usually large safe large handling results also counting time fetch direct result also deserialize schedule delay also make sense fetch much smaller blocks via direct result fetch large directly result executors low
692,1,generated code accesses columnar batch object possible get values column columnvector instead calling get row direct consume columnvector generated code columnar batch used
693,1,window window expression useless might happen column pruning eliminate unnecessary window
694,1,using select dummy table table used sql statements table reference required contents table important example case see useless project whose project list empty executing column pruning rule remove project project list empty
695,1,random sampler sample currently accepts iterator input output another iterator makes inappropriate use whole stage codegen sampler operator add non iterator interface random sampler add non iterator interface random sampler
696,1,push predicate window operator jira predicates pushed window following conditions satisfied predicate involves one one column part window partitioning key window partitioning key sequence attribute references en one expression predicate must deterministic predicate push window operator
697,1,project list useless remove class window simplifies codes analyzer optimizer remove project list windows
698,1,today memory store disk store implement common block store api feel api inappropriate abstracts away important distinctions behavior two stores instance disk store notion storing deserialized objects confusing expose object based apis like put iterator get values instead exposing binary apis pushing responsibilities serialization deserialization client part larger block manager interface clean up like remove block store api refine memory store disk store interfaces reflect narrow sets responsibilities components remove block store interface cleanly reflect different memory disk store responsibilities
699,1,cached block spilled disk read back serialized for me bytes current block manager implementation attempt insert serialized block memory store even block storage level requests deserialized caching behavior adds complexity memory store think offers many performance benefits like remove order simplify larger refactoring patch therefore propose change behavior disk store reads cache bytes memory store blocks serialized storage levels two places request serialized bytes block store get local bytes called reading local copies torrent broadcast pieces broadcast pieces always cached using serialized storage level lead mismatch serialization forms spilled bytesread disk cached bytes memory store non shuffle block branch get block data called netty block rpc server responding requests read remote blocks caching serialized bytes memory benefit us cached bytesread evicted likelihood happening seems low since frequency remote reads non broadcast cached blocks seems low caching bytes low probability read bad risks eviction blocks cached expected serialized deserialized forms since blocks seem likely read local computation therefore think safe change cache memory disk blocks bytes memory store reading spills
700,1,query plan expressions always include expressions
701,1,right use physical rdd existing rdd data sources becoming much different use different physical plans use different physical plan existing rdd data sources
702,1,majority spark sql queries likely run though had oop fs relation however currently several complexity performance problems path simplify speed up had oop fs relation
703,1,preparation larger refactorings think remove confusing return values option block store put apis returning value useful one place caching situations block replication simpler put get remove return values blocks to reap is
704,1,need submit another prs park call task failure callbacks spark calls close function various output streams example need intercept exception call task context mark task failed calling close following changes spark include unit tests make sure always work future invoke task failure callbacks calling output stream close
705,1,broadcast left semi join without joining keys already supported broadcast nested loop join implementation left semi join bnl remove remove left semi join bnl
706,1,spark add python api generalized linear regression python api generalized linear regression
707,1,already internal apis hive sql context merge code paths one day track current database sql hive context
708,1,remove deterministic conditions filter contained child prune filters based constraints
709,1,clean task today fields sql context organized particular way however since sql context session many fields actually isolated per session minimize size context files provide logical grouping makes sense propose move fields class called session state refactor move sql context hive context per session state separate class
710,1,spark add python api max abs scaler python api max abs scaler
711,1,parts park want create stable api foundation dataset become main user facing apis park ticket tracks various tasks related main high level changes merge dataset data frame create natural entry point dataset sql context hive context ideal name sql hives park context ideal heavy dependency rd ds first class support sessions first class support system catalog see design doc details dataset oriented api evolution spark
712,1,column pruning rule optimizer introduce redundant project cases prevent remove redundant project colum pruning rule
713,1,task context supports task completion callback gets called regardless task failures however way listener know error ticket proposes adding new listener gets called task fails add task failure listener task context
714,1,following spark add wrapper naive bayes spark rr naive bayes implementation package e signature easy us match parameters naive bayes wrappers park r
715,1,think model summary interface available spark scala java r interfaces also available python interface similar spark https issues apache org jira browse spark expose ml summary function py spark classification regression models
716,1,support queries join tables using clause select table join table using support using clause join
717,1,currently sbin start stop mesos dispatcher scripts assume one mesos dispatcher launched potentially users like run multi tenant dispatcher might want launch multiples also helps local development ability launch multiple ones add support launching multiple mesos dispatchers
718,0,add benchmark codes encoder compress compression scheme benchmark
719,1,broadcasted table could used multiple times query cache avoid duplicate d broadcasts
720,1,current implementation statistics unary node considering output example project considering better guess considering output statistics logical plan
721,1,bug reported st uti awasthi https www mail archive com users park apache org msg html loss sum possibility infinity standardize feature fitting model support feature standardization another benefit standardization improve convergence rate aft survival regression support feature standardization
722,0,per discussion https git hub com apache spark pull discussion r improve error message better error message path specified
723,1,union distinct two distinct generate two aggregation plan remove extra distinct union
724,1,lots sql operators metrics input output number input rows exactly number output rows child could metrics output rows improve performance using whole stage codegen overhead sql metrics trivial anymore avoid necessary operator sql metrics add operators number rows input output example projection may need remove duplicated sql metrics
725,1,new mutable projection fall back interpreted mutable projection failed compile since remove configuration codegen heavily reply codegen also tungsten aggregate require generated mutable projection update unsafe row remove fall back could make user confusing see discussions park remove fall back codegen
726,1,spark sql collapse adjacent repartition operators keep last one collapse adjacent repartition operations
727,0,try avoid suffix unique id remove multiple empty lines generated improve readability generated code
728,1,currently delegated dls directly hive native placeholder hiv eql scala spark want provide native implementations ddls sql context hive context first step properly parsed dls create logical commands encapsulate actual implementation still delegate hive native command example define command rename table proper fields delegate implementation hive native command might need track original sqlquery order run hive native command remove sqlquery future next step flush internal persistent catalog api switch implementation newly added commands use catalog api create native ddl commands
729,0,best times table average time benchmark together average time could show information use best time average time micro benchmark
730,0,use lowercase change long prefixes something shorter case changing one tungsten aggregate agg make whole stage codegen variable names slightly easier read
731,1,define class inside package object name ends something like org my company project package my class however reflect try load org my company project my class support classes defined package objects
732,1,remove generic internal row with schema
733,0,dataset aggregators complex types fail unable find encoder type stored data set though datasets complex types supported encoder implicit s seq primitive
734,1,sparks park sql internally limited catalog support ddls umbrella ticket introduce internal api system catalog associated ddl implementations using api native database table system catalog
735,0,improve test coverage whole stage codegen
736,1,stated stream currently provide batch time input state update function required cases behavior depends batch start time con viva patching manually past several spark versions thought might useful others well add api update state by key provide batch time input
737,0,fix random generator map type
738,1,implement simple wrappers park r support k means k means wrappers park r
739,1,implement simple wrapper aft survival regressions park r support survival analysis survival analysis spark r
740,1,parquet files benefit vectorized decoding columnar batches designed support means single encoded parquet column decoded single columnvector vectorize parquet decoding using columnar batch
741,1,benchmarked discussed https git hub com apache spark pull files r benefits codegen declarative aggregate function could much faster imperative one implement built in aggregate functions declarative one skewness kurtosis need benchmark make sure declarative one actually faster imperative one reimplement stat functions declarative function
742,0,add testsuite eliminate subqueries
743,0,adding new tests followup pr https git hub com apache spark pull labels dangerous ground logistic regression without intercept converge glm netsupport case exit glm train warning message saying algorithm converge add tests make sure ml classification logistic regression returns meaningful result labels without intercept
744,0,visualization metrics generated operators
745,0,simplify bucket tests add comments
746,1,investing ating progress spark noticed train validation split class py spark ml tuning module java scala examples spark use org apache spark ml tuning train validation split available python blocks spark class different name py spark may be also find jira task saying need implemented design train validation split estimator ported py spark estimator need sporting would like contribute train validation split missing py spark ml tuning
747,0,add comment hive type coercion type widening
748,1,cache manager directly calls memory store unroll safely logic handling graceful fall back disk cached data fit memory however logic also exists inside memory store appears unnecessary duplication thanks addition block level readwrite locks refactor code remove cache manager replace atomic get or else update block manager method remove cache manager replace new block manager get or else update method
749,1,spark ml lib provides logistic regression linear regression ll elastic net regularization want expand support generalized linear models glm seg poisson gamma families link functions spark implements glm solver case number features small also need design interface gl ms spark r simply follow glm glm net python scala java side interface consistent linear regression logistic regression eg generalized linear model estimator interface generalized linear models gl ms
750,1,prerequisite heap caching blocks need mechanism prevent pages blocks evicted read heap objects evicting block read merely leads memory accounting problems assume evicted block candidate garbage collection true read heap memory lead either data corruption segmentation faults address add reference counting mechanism track blocks pages read order prevent evicted prematurely propose two phases first add safe conservative approach block manager get calls implicitly increment reference count blocks tasks references automatically free dupont ask completion correct may adverse performance impacts prevent legitimate block evictions phase two incrementally add release calls order fix eviction unreferenced blocks latter change may need touch many different components propose separately order make changes easier reason review use reference counting prevent blocks evicted reads
751,1,would easier fix bugs maintain ec script separately spark releases information see https issues apache org jira browse spark moves park ec scripts a mplab
752,1,right use filter follow sort merge join broadcast hash join conditions result projection join could expensive generate lots rows could reduce mostly condition sort merge join broadcast hash join support condition
753,1,current intersect physical operator simply delegates rdd intersect remove intersect physical operator simply transform logical intersect semi join way take advantage benefits join implementations eg managed memory code generation broadcast joins rewrite intersect phy is cal plan using semi join
754,1,add hash function spark rs park r support hash function
755,1,beneficial code testability point viewable exercise individual components also makes easy benchmark would able read data without need create al associate had oop input split etc components expose api unsafe row record reader run files
756,1,https git hub com apache spark pull submit job would create separate thread wait job result submit job threadpool workaround receiver tracker run waiting job result threads https git hub com apache spark pull merged master resolved blocking issue submit job threadpool removed remove submit job threadpool since submit job create separate thread wait job result
757,1,ml lib transformer uses deprecated call udf api remove use deprecated call udf ml lib
758,1,right num fields passed point to bit set width in bytes calculated making point to little bit heavy part constructor unsafe rownum fields unsafe row changed point to
759,1,support window functions sql context
760,1,support intersect except hive sql
761,1,cc non gli please attach design doc bucket ed table support
762,1,provides option choose json parser enabled accept quoting character example json file includes listed json backslash quoting specification returns corrupt record issue similar hive hive add option accept quoting character backslash quoting mechanism
763,1,right java users can not use actor helper uses special scala syntax patch refactored codes provide java api add example refactor actor receiver support java
764,0,run sqlquery spark sql execution page sql tab always blank jdbc server blank sql pages park sql always blank
765,1,input select jdbc tablecol xxx current plan implement jdbc relation unhandled filters removing unnecessary spark filter
766,1,update latest version zinc order match sbt version upgrade zinc
767,1,removes park deploy mesos zookeeper dir use existing configurations park deploy zookeeper dir mesos cluster mode removes park deploy mesos zookeeper dir uses park deploy zookeeper dir
768,1,removes park deploy mesos zookeeper url use existing configurations park deploy zookeeper url mesos cluster mode removes park deploy mesos zookeeper url uses park deploy zookeeper url
769,1,removes park deploy mesos recovery mode uses park deploy recovery mode configuration cluster mode removes park deploy mesos recovery mode uses park deploy recovery mode
770,1,add sql user defined type support encoder add sql user defined type support encoder
771,1,csv common data format small data world often first format people want try see spark single node making built common source provide better experience first time users consider inlining https git hub com data bricks spark csv built csv data source implementation
772,0,points park packages org find improve error messages data sources found
773,1,creating actual logical physical operator range matching performance rdd range apis compared old range api new version times faster old version improve performance range apis via adding logical physical operators
774,0,add ml examples park r add ml examples park r
775,1,discussed https git hub com apache spark pull might need implement unhandled filter remove duplicated spark side filtering implement unhandled filter interface
776,1,distinct unique drop duplicated rows columns drop duplicates drop duplicated rows selected columns implement drop duplicates method data frames park r
777,1,like shuffle file encryption sparks pills data also encrypted support shuffle spill encryption spark
778,1,is not null filter pushed jdbc data source looks sql standard according sql sql sql sql x believe databases support is not null operator pushed jdbc data source
779,1,use sql context ml libtest spark context rather creating new ones park ml test cases use sql context ml libtest spark contexts park ml testsuites
780,1,continuing requests eg spark allowing users extend modify ml lib models algorithms user needs changes please comment specifically needs modified use case remove final classes spark ml trees ensembles possible
781,1,support unsafe row local tables can
782,1,support unsafe row coalesce except intersect
783,1,support unsafe rowmap partitions map groups co group
784,1,still spark plan support unsafe row support well support unsafe rows park plan possible
785,0,need document new heap memory limit configurations added spark add simple configuration validation instance able enable heap execution heap memory limit zero alias old confusing spark unsafe off heap configuration something lives spark memory namespace documents park heap memory configurations add config validation
786,1,similar dataset transform data frame transform function
787,1,mutated ply r package supports adding new columns replacing existing columns currently implementation mutates park r supports adding new columns also make behavior mutate consistent dpl yr throw error message duplicated column names data frame mutated duplicated column names specified columns arguments last column name takes effect enhance mutate support replace existing columns
788,1,starting hive derby meta store use memory backend since execution hive fake meta store use memory mode reduce time used creating execution hive use memory execution hive derby meta store
789,1,implement drop method data frames park r
790,1,created new private variable bound t encoder shared multiple functions rdd select collect replaced query execution analyzed function call logical plan api comments using wrong class names eg data frame parameter name seg napi descriptions wrong eg map partitions sqlcode refactoring comment correction dataset apis
791,1,kafka already released introduce new consumer api compatible old one added new consumer api made separate classes package org apache spark streaming kafka v changed api remove old classes backward compatibility user need change old spark applications up rga de news park version please re wie w changes update kafka d streams new kafka consumer api
792,0,umbrella issue addressing spark r related issues corresponding spark planned spark r
793,0,far using comma separated decimalformat output encoded contents way rare data binary could common issue use data set api example sql display binary encoded values
794,1,currently support read df write dfj son file parquet file sql context support external data source api read json read parquet read orc read jdbc read csv exist api deprecated removes park also deprecate spark r note refer data frame reader data frame writer apis park sql defined r like styledata frame reader api https park apache org docs latest api scala index html org apache spark sql data frame reader data frame writer api https park apache org docs latest api scala index html org apache spark sql data frame writer support external data source apis park r
795,1,replace shuffle manager class short shuffle mgr names external shuffle block resolver
796,1,change num partitions rdd get num partitions consistent py spark scala
797,1,think upgrade tachyon order get fix https tachyon atlassian net browse tachyon upgrade tachyon dependency
798,1,add isnan columns park r column three related variable functions isnan is null is not null replaced at a frame is nan data frame is nan spark r side data frame is nan deprecated removed spark add is null data frames park r data frame two related functions isnan is null fix usage is nan isnan
799,1,change cumed is tcu me dist dense rank dense rank percent rank percent rank row number row number two reasons make change follow naming convention ruler httpwww insider org nodes park data frame deprecated old convention cume dist removes park better fix issue release otherwise make breaking api change rename window rank function names spark r
800,1,spark sql aggregate function stddev stddev pop stddev samp variance var pop vars amp skewness kurtosis collect list collect set support column name arguments like aggregate function max min count sum stddev variance etc support column name arguments
801,1,unify get struct field get internal row field
802,1,repartition returns new data set exactly num partitions partitions coalesce returns new data set exactly num partitions partitions similar coalesce defined rdd operation results narrow dependency eggo partitions partitions shuffle instead new partitions claim current partitions sql support coalesce repartition dataset apis
803,0,in vfunc argument reduce by key and window none checkpointing required scala implementation reduce by key and window handles correctly require checkpointing python version requires checkpoint directory specified bug py spark reduce by key and window in vfunc none requires checkpointing
804,0,right table contents gets generated page page basis makes hard navigate different topics project make use empty space left documentation put navigation menu picture worth thousand words add menu documentation ml lib
805,1,implement struct encode decodes park r documented https park apache org docs latest api scala index html org apache spark sql functions implement struct encode decodes park r
806,1,collection functions documented https park apache org docs latest api scala index html org apache spark sql functions size explode array contains sort arraysize explode already implemented array contains sort array implemented implement collection functions spark r
807,1,want support json serialization vectors order supports park json serialization vectors
808,1,consolidate expression encoder tuple encoders tuple
809,1,row encoder support user defined type add support database application important focus issue add user defined type support row encoder
810,0,show batch failures streaming ui landing page
811,1,random forests feature importance gbt would great add feature importance gbt well perhaps code random forests refactored apply types ensembles see https issues apache org jira browse spark feature importance gbt
812,1,add java api track state by key
813,1,returns text reuse spark sql provide whole text file data source directly convert text utf string without extra string decoding encoding whole text file rdd return text rather string
814,1,remove open hashset old aggregate
815,1,sufficiently test path work well remove option turn unsafe codegen
816,0,spark sql create table using command follwing create table tablename col char hive support creating table desc table desc table names park report error org apache spark sql types datatype exception unsupported datatype char struct field name special characters please use backticks quote field name eg x please note backtick supported field names park sql support column datatype char
817,1,implement python api bisecting k means python api bisecting k means
818,0,currently spark sql would flush command history exiting flush spark sql command line history history file
819,1,deprecated runs sparks park either remove runs make effect warning messages simplify implementation prefer latter better binary compatibility make runs effect k means
820,1,remove internal implicit conversion expression column functions scala
821,1,remove implicit conversion expression column
822,1,data frame internal implicit conversion turns logical plan data frame fairly confusing new contributors since buy us much remove implicit conversion remove internal implicit conversion logical plan data frame
823,1,umbrella ticket walk newly introduced apis make sure consistent sql api audits park
824,0,hp fortify opens source review team https wwwhp fod com open source review project reported handful potential resource leaks discovered using static analysis tool fix issues identified scan fix potential socket filehandle leaks identified via static analysis
825,1,since bytes number records beginning page address zero need bit set remove bit set bytes to bytes map
826,1,remove methods variance stddev skewness grouped data keep common first order statistics
827,1,two classes public since used public code make data frame holder dataset holder public
828,1,add rapist d dev variance
829,1,add java friendly api streaming listener add java streaming listener
830,1,since spark resolved map partition with prepare needed anymore remove preparer dd
831,1,distribute allows user control partitioning ordering dataset useful applications add data frame api provides functionality similar hiv eql distribute
832,1,order lay groundwork proper heap memory support sql tungsten need extend memory manager perform bookkeeping heap memory add support heap memory memory manager
833,1,use scala collection mutable bit set broadcast nested loop join uses park bit set uses park bit set broadcast nested loop join
834,1,runs introduces extra complexity overhead ml lib k means implementation seen much usage runs equal deprecate method remove void helps us simplify implementation deprecate runs k means
835,0,saving data eg saving parquet error query execution partial file generated failed task uploaded retries task throw file already exist error confusing users may think file already exist error error causing job failure find real errors park ui stage page provide informative error message direct parquet output committer used file already exists error
836,1,currently write ahead logs park streaming flushes data writes need made support flushing data data written stream actually closed case failure data last minute default rolling interval properly written therefore need flag close stream write achieve readwrite consistency flag close write ahead log writing
837,0,fairly old page docs contains bunch assorted information regarding running spark had oop clusters think page removed merged parts docs information largely redundant somewhat outdated https park apache org docs latest had oop third party distributions html three sections compile time had oop version information think removed favor buildings park page days advanced users building without bundling had oop sure giving bunch different had oop versions sends right message linking had oop seem add much beyond programming guide runs park redundant hardware provisioning guide inheriting cluster configurations think would better section end configuration page remove third party had oop distributions doc page
838,1,add text data frame reader data frame writer python api text data source
839,1,first cut implementation track state by key new improvement state management methods park streaming see epic jira details https issues apache org jira browse spark implement track state by key improved state management
840,1,mark stage result stage shuffle map stage internal state private
841,1,update tachyon client dependency new dependencies added spark facing apis changed upgrade tachyon dependency
842,1,part work implements park would nice network library efficiently stream data connection currently shuffle data protocol efficient large files requires whole file buffered receiver side receiver anything large files comes huge cost memory chunk large files requires client ask chunk separately instead similar approach allowing data processed arrives would lot efficient make easier implement file server referenced bug support streaming data using network library
843,1,right stack new url classloader user add jar sql add jar command approach introduce issues caused ordering added jars class jar depends another class another jar example case lookup class b able find class jar parent jar use single url classloader jars added sql add jar command
844,1,use kcl current master kcl added integration kinesis producer library kpl support auto de aggregation would great upgrade kcl latest stable version note latest version restored compatibility dynamo db streams kinesis adapter broken see https git hub com aws labs amazon kinesis client release notes tdasbrkyvz please recommend version upgrade upgrade kinesis client library latest stable version
845,1,also sql context new session add get or create spark context sql context python
846,1,variety reasons tagged bunch internal classes execution package sql developer api remove developer api annotation private classes
847,1,spirits park clean uses park home possible remove entirely need look use done use allowing applications run different versions spark standalone mode instance someone could submit application customs park home worker would launch executor using different paths park use case widely used may be removed existing constructors takes park home purpose deprecated explains park home longer used purpose legitimate reasons spark home keep around need audit uses clean clarify uses park home
848,1,use cases useful explicitly ban creating multiple root sql context shive contexts root sql context means first sql context gets created introduce mechanism ban creating new root sql contexts jvm
849,1,share sql tab across sessions sql tab shared across sessions
850,1,add method analogous spark ml lib clustering k means model compute costs park ml clustering k means model temp fix proper evaluators defined clustering add compute cost k means models park ml
851,1,transformer version k means model currently methods compute cost get centers cluster centroids could way get either exposing parent model adding method would make things easier add compute cost cluster centers k means models park ml package
852,1,new netty rpc still behaves much like akka requires client eg executor server eg driver listen incoming connections necessary since sockets full duplex rpcs able flow either way connection also semantics netty based rpc exactly match akka get weird issues like spark supporting client mode also reduces number ports spark apps need usenet ty based rpc env support client mode
853,1,serialize called actual size append use unsafe projection unrolling avoid serialization multiple times unrolling complex types
854,1,matches closely imperative aggregate rename expression aggregate declarative aggregate
855,1,typeid needed columnar cache confusing remove typeid columnar cache
856,1,support message handler direct kafka stream allows arbitrary output stream instead array byte useful function therefore exist kinesis well add message handler kinesis utils create stream similar direct kafka
857,1,currently try support multiple sessions sql within spark context broken complete isolate session current database hivesqlconfudfudafudtf temporary table added jar cached tables accessible sessions improve session management sql
858,1,spark uses network module implement rpc however configurations named spark shuffle prefix network module refactor make sure user control shuffler pc separately separate configs shuffler pc
859,1,refactoring instance case class l or lir also cleaning code refactoring instance l or lir also cleaning code
860,1,idea logic calling python actually nothing rdd really communicating socket nothing distributed currently depending rdd written way extract functionality apply area code depend rd ds also make easier test refactor python rdd decouple iterator computation python rdd
861,1,following method logistic regression model marked private prevents users creating summary given dataset check https git hub com feynman liang spark blob face fbcdmllibsrcmainscalaorg apache spark ml regression linear regression scala l method definitely necessary test model performance way name evaluate already pretty good meng xr could check thx make logistic linear regression model evaluate method public
862,1,spark add python api aft survival regression python api aft survival regression
863,1,bagel deprecated done changes need runtests remove bagel testsuites
864,0,see spark code added docs never updated update documentation instructions enable block manager wire encryption
865,0,saw many failures test recently eg http sampl abcs berkeley edu jenkins views park qa test jobs park masters bta mplab jenkins build profile had oop labels park test test report junit org apache spark network sasl sasl integration suite test no sasl client flaky test network sasl sasl integration suite test no sasl client
866,1,https issues apache org jira browse spark longer need use customs cp based mechanism archiving jenkins logs master machine superseded use jenkins plugin archives logs provides public viewing remove legacy log syncing code since blocker disabling worker master ssh jenkins remove legacy scp based jenkins log archiving code
867,0,job descriptions help distinguish jobs one batch jobs stages pages spark ui set meaningful job descriptions streaming related jobs
868,1,hive already supports according https issues apache org jira browse hive currently spark sql still supports primitive types collect list collect set accept struct types argument
869,1,block matrix multiply sends block corresponding columns right block matrix even though might corresponding block multiply optimizations perform simulate multiplication driver figure blocks actually need shuffled send block partition join inside partition rather sending multiple copies partition decrease communication block matrix multiply increase performance
870,1,name weights becomes confusing supporting weighted instanced discussed https git hub com apache spark pull want deprecate weights use coefficients instead deprecate remove weights make changes spark ml deprecate weights use coefficients instead ml models
871,1,really simple request upgrade fast util x current version x minor api object xx open hashmap structures used many places spark marked deprecated plus conflict another library using saddle http saddle gi thu bio uses newer version fast util happy send pr guess bigger question want keep using fast utils parks park uses hash maps probably requires another discussion remove fast util
872,0,document options public apidoc otherwise hard find options without looking code document libs vm data source options public doc minor improvements
873,1,implemented dsp r sparse vector support row matrix method also used weighted least squares places would useful move linalg blas move row matrix dsp rbl as
874,1,spark need generate random data follow weibull distribution add weibull generator random data generator
875,1,currently convert to safe python udf needed actually python udf could process unsafe row
876,1,recently released many improvements especially following quote python side ides interactive interpreters ipython get help text autocompletion java classes objects members make spy j ideal tool explore complex java apis eg eclipse api thanks jonah kich wa coders quote normally wrap a piss park ones would make easier offroad using java proxy objects upgrade py spark use py j
877,1,currently method join right data frame using columns seq string supports inner join convenient support join types support specify join type calling join using columns
878,1,bring much value since better unit test coverage hash joins also help reduce test time remove hash join compatibility suite
879,1,python since defined py spark sql would nice move py spark shared components move since annotator py spark shared components
880,1,ml pipelines transformer estimator appends new columns input data frame example might produced at a frames like following columns bc raw input budfbcudfcbudfcudfs could expensive however materialize c udf bud fc triggered twice evalue c used would nice detect pattern use intermediate values optimize sequential projections
881,0,sometimes dependency changes pretty unclear transitive set things changing enumerate dependencies put source file repo make explicit changing enumerates park dependencies file diff new pull requests
882,0,improve ml guide replace ml dataset data frame simplify abstraction remove links scala apidoc main guide change ml algorithms pipeline components improves park ml user guide
883,1,codegen consider null ability expressions considering avoid lots null check reduce size generated code also improve performance double check correctness null ablity expressions schema hit npe wrong results consider null ability expression codegen
884,0,clean user guides address minor comments https git hub com apache spark pull https git hub com apache spark pull code examples introduced create data frame switch update user guide address minor comments code review
885,0,hive comparision test print dependent tables
886,1,convenient implement data source api libs vm format better integration data frames ml pipeline api jira covers following read libs vm data data frame two columns label double features vector accept num features option implementation live org apache spark ml source libs vm implement sqldatasource api reading libs vm data
887,1,addcolumn function data frame like ifelse rs park r guess could implement combination otherwise h example dfx true return otherwise return add ifelse column functions park r
888,1,right qualified tablename table identifier seq string represent table identifiers one form looks table identifier best one provides methods get tablename database name return unquoted string return quoted string todos spark places need updated consolidate different forms table identifiers
889,1,ml evaluator currently requires metrics maximized bigger better counterintuitive metrics currently hack ily negate metrics regression evaluator weird instead return metric expected eg rmse return rmse negation provide indicator whether metric maximized minimized model selection algorithms use indicator needed ml evaluator indicate metric maximized minimized
890,1,make multilayer perceptron classifier layers weights public make multilayer perceptron classifier layers weights public
891,1,currently spark r collect data frame collects data within data frame local data framer users used using data frame however collect currently collect data nested types data frame serializer jvm backend support nested types collect r side assumes column simple atomic type com binded atomic vector improve implementation collect data frames park r
892,1,stat functions defined https park apache org docs latest api scala index html org apache spark sql data frame stat functions currently crosstab supported functions supported include corr cov freq items add support data frame stat functions spark r
893,1,add python api ml lib fpm prefix span add python api prefix span
894,0,prepare architecture documentation
895,0,had oop backported branch also benefit whitespace trimming configuration backport had oop branch
896,1,jira define commands had oop token scope task highlighted following token in it authenticate request identity token persist token token cache later reuse token display show existing token info attributes token cache token revoke revoke token token longer valid can not used later token renew extend lifecycle token expired had oop token command
897,1,jar finder getjar invoked client app would really useful destroy generated jar jvm destroyed setting temp jar delete on exit order preserve backwards compatibility configuration setting could implemented eg test builddir purge exit jar finder getjar delete jar file upon destruction jvm
898,0,had oop common project had oop common src main docs release notes html shows modified even though touched check reset previous version make go away thing neutralize put dummy commit every time switch branches rebase appears began release notes commit c bbb dcccd dd must due line endings change use svn eol style native html prevent line ending issues
899,0,system uncommon get m blogs map reduce job would helpful possible configure had oop daemons write logs major things happen conf options could find increasing amount output disk really bottleneck us believe short jobs would run much quickly less disk usage also believe high disk usage might triggering kernel bug machines causing crash m blogs went kb would probably still information needed thanks huge log files
900,0,add documentation building txt describing dependencies instructions building windows add instructions building txt describing build windows
901,1,currently different ipc servers had oop use config variables names starting ipc server makes difficult confusing maintain configuration different ipc servers support per server ipc configuration
902,1,given example data node disk definitions applicable configuration list disks provided defined multiple local disks defined data node dfs datadir data dfsdndatadfsdndatadfsdn data dfsdndatadfsdndatadfsdn true one disks breaks unmounted mount point data example becomes regular directory valid permissions possible directory structure had oop expecting situation happens data node fails restart actually enough disks ok state proceed way around alter configuration omit specific disk configuration opinion would practical let had oop daemons start least disks partition provided list usable state prevents roll custom configurations systems temporarily disk therefor directory layout missing might also configurable least x partitions available ones ok state allow daemon startup least configurable disk ok state
903,1,jira make handling improperly constructed file uris windows local paths rigorous eg reject file c windows valid file uri syntax explained http blogs msdn combi e archive file uris windows aspx also see https issues apache org jira browse had oop reject invalid windows uris
904,1,quant cast released qfs http quantcastgithubcomqfsc distributed file system based kosmos filesystem kfs qfs comes various feature performance stability improvements kfshadoopfsshim needs added support qfsqfsurisneedaddfsshim use qfs
905,1,currently file util untar spawn star utility work tar may present platforms default eg windows changing use java api would help make cross platform file util unzip uses approach change untar use java api windows instead spawning tar process
906,0,change website reflect new user had oop apache org mailing list since merged user lists per discussion general http apache or ghv change website reflect new user had oop apache org mailing list
907,0,code changes make had oop branch work natively windows done jira intended track work needed achieve test pass dev tests stabilize branch win
908,1,reasons listed http findbugs sourceforge net bug descriptions html se comparator serializable comparators serializable make deserialization work required superclasses arg constructor http findbugs sourceforge net bug descriptions html se suitable constructor simply add arg constructor writable comparator writable comparator must implement arg constructor
909,1,had oop streaming use sold had ooprapijiraportsnewrapi port stream input format newmap reduce api
910,1,track changes restoring security branch restore security had oop branch
911,1,had oop introduces configure flag prevent potential status inconsistency z kfc name node making auto manual failover mutually exclusive however described section design doc hdfs allow manual auto failover coexist adding rpc interfaces z kfc manual failover shall triggered ha admin handled z kfc auto failover enabled auto ha allow manual failover invoked z kfc
912,1,keep initial patches manageable kerberos security currently supported z kfc implementation jira support following important pieces security integrate zk authentication kerberos password based allow user configure acls relevant z nodes add keytab configuration login z kfc daemons ensure rpcs made health monitor failover controller properly authenticate target daemons security support zk failover controller
913,1,currently zk session expires results fatal error sent application callback best behavior example case haz k goes would like current state maintained rather causing either nn abort zk clients able reconnect sort correct leader based normal locking schemes improve active standby elector behavior session expires
914,1,known failures test patch bail soon sees causes pre commit builds potentially find real issues patch tests would fail might come known failure add fn mvn test command test patch get full list failures test patch runtests fn avoid masking test failures
915,1,one thing original patch had oop address need curated jars visible final tarball make had oop client set curated jars available distribution tarball
916,1,sync able sync deprecated remove remove deprecated sync able sync method
917,1,access control support nonsecure deployment had oop windows
918,1,able start kdc server unit tests security could turned greatly improve coverage unit tests add capability turn security unit tests
919,1,given locked using xml configuration administrators need manage unless tool manages used would good also validate provided config xml site xml file stool like xmllint may be xerces somehow running command least starting daemons use relevant tool available optionally silent env requests validate xml s relevant tool available using scripts
920,1,hdfs added new public api sequence file sync fs need forward port compatibility looks like might introduced apis need forward porting well eg local ted blocks set file length data node get block info forward port sequence file sync fs friends had oop x
921,1,currently check done has sufficient time elapsed method hardcoded mins wait wait time driven configuration default value clients min kerberos re login interval user group information configurable
922,0,docs version hardcoded updated automatically automatically update doc versions
923,1,rpc layer improvements support protocol compatibility
924,1,create script setup application order create root directories application h base hcat hive etc
925,1,see thread http mark mail org thread cxtzlvztfgfxn need get things running top level had oop tools module dist cpv first resident new home things need module top level pom appropriate dependencies integration patch builds new module integration post commit nightly builds new module set things top level had oop tools module
926,1,had oop common src main bin had oop config sh needs updated post maven ization eg still refers build classes etc had oop config sh needs updated post maven ization
927,1,had oop common src main bin had oop config sh needs updated post m reg layout map red home changed had oop config sh needs updated post mr
928,0,hudson pre commit tests presently capable testing patch trunk nice could extended automatically run correct branch make pre commit checks run correct branch
929,1,nutshell ls needs ability list directory contents w impossible list root directory owner permissions etc see original hdfs bug details had oo pdf sls expand directories hdfs
930,1,had oop hdfs agreed filesystem list status throw filenotfoundexception instead returning null target directory exist however local file system implementation today filesystem list status still may return null target directory exists grant read permission causes npe many callers reasons cited had oop hdfs see had oop linked issues examples filesystem list status throw ioe upon access error
931,1,ipc wire compatibility
932,1,older had oop version tries contact newer had oop version across ipc protocol version bump client currently gets non useful error message like eofexception instead ipc server code speak enough priori pc protocols send back fatal message indicating version mismatch send back nicer error clients using outdated ipc version
933,1,key value class non writable forget attach io serializers npe thrown tasks information missing led think better exception thrown serialization factory instead npe class found accepted loaded ones serializer class missing return null thrown pe
934,1,setting compression code cmr job full classname codec must used ease usability compression codecs resolved codec name ie gzip deflate zlib bzip instead full codec classname besides easy use had oop users would use codec alias instead full codec classname could simplify h base resolves loads codecs add capability resolve compression codec based codec name
935,1,create script hudson uses execute test patch source control modifications test patch sh arguments do new updating hudson script would execute following take password argument create test patch script hudson
936,1,deprecate metrics v
937,1,fs shell many chains else chains instantiating running commands dynamic mechanism needed registering commands fs shell requires changes adding new commands add command factory fs shell
938,1,project pig exposes fs shell functionality end users shell command want use command modifications make sure whether work hdfs had oop pig get identical semantics main concern recently raised users way ignore certain failures consider benign instance removing nonexistent directory asks related issue meaningful error code returned fs shell use java class take different actions different errors unix like ways tell command ignore certain behavior commands would like expanded implemented rmf rmdir ignore fail non empty mkdir p extensions fs shell
939,1,daemon factory class defined hdfs util common would better place class daemon factory moved hdfs common
940,0,fix had oop patch testing using jira cli tool
941,1,new file system api had oop implement security features currently provided filesystem apis critical requirement map reduce components migrate use new apis internal filesystem operations map reduce security implementation new file system file context api
942,0,number subprojects left had oop yet website fully updated reflect update website recent subproject departures
943,1,allow users use compact form xml elements example could allow old format would also supported allow compact property description xml
944,1,working had oop noticed metrics naming style place capitalized camelcase eg files created name node metrics rpc metric sun capitalized camelcase eg threads blocked jvm metrics rpc metrics lowercased underscored eg bytes written data node metrics map reduce metrics let make consistent un capitalized camelcase main reason camelcase backends limits name length underscore wasteful consistent naming style metric number inodes created mutable counter long files created instead redundant metric files created number inodes created mutable counter long files created make metrics naming consistent
945,1,had oop widespread s matures number tools utilities users keeps growing bundled had oop core had oop contrib full fledged servers example named is tcp streaming pipesharpighiveoozie today standard mechanism making tools available users neither standard mechanism tools integrate distributed lack common foundation creates issues developers users common foundation had oop client tools
946,1,whenever new patch submitted verification test patch process make sure none herriot bindings broken test patch needs verify herriot integrity
947,0,create wiki document herriot test framework test development guide
948,0,add documentation interface classification scheme the w common site update had oop common sites
949,1,per discussions a run chris hong rajiv et al concluded current metrics framework needs overhaul allow multiple plugins different monitoring systems simultaneously see also had oop refresh metrics plugin config without server restart including filtering metrics per plugin support metrics schema plugins jira resolved core had oop components hdfs map reduce updated use new framework updates external components use existing metrics framework tracked different issues current design wiki http wiki apache org had oop had oop metrics v overhaul metrics framework
950,0,created new logo had oop security topics wanted share community create new logo had oop security related uses
951,0,currently had oop command guide http had oop apache org common docs r commands manual html lists available command line options description include examples command clarity had oop commands guide include examples
952,0,user document added secure impersonation feature document cover code example user group information do as used secure impersonation user document user group information do as
953,1,currently maven pom file generated template file includes versions libraries depend versions libraries also present ivy libraries properties library updated must updated two places error prone instead specify library versions single place versions dependencies specified single place
954,0,current documentation rack awareness http had oop apache org common docs r cluster setup html had oop rack awareness augmented include sample script improve documentation rack awareness
955,1,adding serialized form delegation tokens http interfaces include version information add version serialization delegation token
956,1,token used authentication rpc information username may needed access authorization information typically specified token identifier especially true block tokens used clientdata node accesses authorization based access permissions specified token identifier username block tokens used called access tokens one think capability tokens see had oop info add authenticated token identifier sugi used authorization
957,1,user group information contain authentication method subject used hdfs issue delegation tokens kerberos authenticated client sugi contain authentication method
958,1,need configurable mapping full username seg omalley apache org local username seg omalley many organizations sufficient use prefix however case shared clusters may duplicated prefixes configurable mapping let administrators resolve issue need mapping long principal names local os usernames
959,1,way metrics currently exposed jmx name node helpful since current counters record fetched without context number mean little example number files created equal means last period files created new period end unknown fetching either mean another files fetching time period one solutions problem jmx context accumulated at a child class abstract metrics context expose different records jmx custom mbeans way information fetched jmx represent state things meaningful way jmx context metrics
960,1,common tests functional tests end end makes sense mock ito framework convenience true unit tests development add unit tests framework mock ito
961,1,filesystem mkdir create file apis create parent path use case illustrated https issues apache org jira browse hdfs focused comment id page com atlassian jira plugin system issue tab panels a comment tab panel action filesystem mkdir create file apis create parent path
962,1,configuration objects send debuglevel log message every time instantiated include full stack trace appropriate trace level logging renders debug logs hard read configuration sends much datalog j
963,1,hdfs need use unix domain sockets jira include library common add sh net unix package based code android apache license add support unix domain sockets jni libs
964,1,possibility contributors commiters might checking patches version findbugs differs one installed hudson test patch script verify version findbugs correct test patch verify findbugs version used verification correct one
965,0,new had oop common site set site initial pass may need add content may need update links new had oop common site
966,0,runtime test patch increased min test patch takes min
967,1,currently quota turned user can not call rmr large directory causes quota besides error message unfriendly handled handling trash quota
968,1,need mechanism rpc calls get exceptions automatically retry call certain circumstances particular often end calls rpcs wrapped retry loops timeouts able make retrying proxy call rpc retry circumstances need rpc retry framework
969,1,adding option fs shell stat get file block location information useful print block location information format block idx xxxx byte range yyyy zzzz location dnd n block idx xxxx byte range yyyy zzzz location dnd n add command fs shell stat get file block location information
970,1,following tests org apache had oop fs package moved hdfs subdirectory had oop related hdfs eg test ftp file system files moved hdfs use hdfs codes testing hdfs features eg test sticky bit defined org apache had oop hdfs package fs tests placed hdfs
971,0,had oop core site page http had oop apache org core add chuk wa list related projects menu bar related jira https issues apache org jira browse chuk wa had oop core site add chuk wa related project menu
972,0,add link training videos getting started section like pig see http had oop apache org pig add link training website
973,1,effort simplify capacity scheduler would reintroduce possibly revisions original design incompatible change objections remove preemption capacity scheduler codebase
974,0,add link distributions wiki page releases page
975,0,git save empty directories would nice make placeholder touch file would cause git create directory builds git checks fail src contrib chuk wa opt
976,0,since ant javadoc generate core javadocs javadocs eg hdfs javadocs checked hudson core javadoc checked hudson
977,1,specific subcase general priority inversion problem noted had oop many lower priority jobs submitted waiting mappers free even though actually done work assigned free reducers higher priority job submitted priority inversion results due lower priority tasks midst completing also due ones yet started claimed free reducers simple workaround require job complete useful work assigning reducer done tunable backwards compatible manner adding minimum map progress percentage assigning reducer option job conf setting would eliminate common case setting would technically eliminate inversion had oop though likely unacceptably high cost job conf option minimum progress threshold reducers assigned
978,0,would great collect analytics visitors website need create privacy policy tells visitors collect create privacy policy had oop website
979,1,increase productivity current project makes heavy use had oop wrote small eclipse based gui application basically consists views hdfs explorer adapted eclipse filesystem explorer example includes following features classical tree based browsing interface directory content detailed columns table filename filesize file type refresh button delete file directory confirm dialog select file street able click delete button rename file directory simple click file table type new name validate open file system editor select file table click open button works windows linux internal dragdrop external dragdrop local file system hdfs opposite work map reduce simple job launcher select job xml configuration file run job kill job visualize map reduce progress progress bars open browser had oop job tracker web interface installation notes eclipse jdk import archive eclipse copy had oop conffile had oop default xml src folder step moved gui later right click project run eclipse application enjoy eclipse based guid fs explorer basic map reduce job launcher
980,1,already quotas hdfs namespace had oop had oop implements similar quotas disk space hdfs jira propose sport had oop port hdfs space quotas
981,1,watchdog watching chuk wa agent every minutes point retrying every mins practice watchdog able automatically restart agent take minutes get ops restart also ops want us limit number communications had oopchukwaminuteschukwa agent controller retry register longer period frequent
982,1,optimize hudson build had oop nightly sh script
983,1,split build script building core hdfs map red separately
984,1,data need reprocessed database currently manual method reload chuk wa sequence files database minor tweaks metrics data loader possible create command line utility add utilities load chuk wa sequence file database
985,1,would useful implement jni based runtime had oop get access native os runtime would allow us stop relying exec ing bash get access information usergroups process limits etc features chown chgrp org apache had oop utils hell implement native os runtime had oop
986,1,amongst things jun it better support class wide set tear via before class after class annotations flexible assertions http junit sourceforge net doc release notes html would nice able take advantage features tests write junit runtests written junit without changes upgrade junit
987,1,file tailing adaptor trying readfile file exist permission denied exception throws file log removed file tailing adaptor unable readfile take action instead trying times
988,1,add had oop native library java library path compression
989,0,configuration directories specified either setting had oop confdir using config command line option however had oop daemons h script start daemons unless pid directory separate configuration issue code generating pidfile names dependent configuration directory pid directory changed had o open vsh seems little unnecessary restriction startup scripts start instances had oop daemons w different config sw setting separate pid directories
990,1,ability test end to end chuk wa pipeline log file data sink data sink demux output chuk wa test framework
991,1,external application ops script cli based tool change configuration capacity scheduler change capacities various queues example updating config file application needs tell capacity scheduler config changed causes scheduler read configuration possible capacity scheduler may need interact external applications similar ways capacity scheduler needs read configuration
992,1,create utility classes dump archive chuk wa records files utility classes archive chuk wa record files
993,1,order reduce number file hdfs need rolling mechanism demux output avoid immediate merging already file time range create spill file instead merge raw files every hours merge hourly files every days rolling mechanism demux output
994,1,update chuk wa parsers update chuk wa parsers
995,1,simplify parsers implementation add map reduce side demux add dynamic link record type parsers using configuration file alias encapsulate data files creation location naming convention inside core demux classes sort data time partition machine timestamp default update chuk wa demux process
996,1,problem need object serialization deserialization support t file object file to pt file
997,1,besides default jt scheduling algorithm work going least two schedulers had oop had oop had oop makes easier plug new schedulers jt place source files various schedulers easy users choose scheduler choice deployment easy developers add schedulers framework without inundating had oop core support source files for multiple schedulers
998,0,would good test case had oop metrics could use file context derive something null context check values returned via metrics correct create tests had oop metrics
999,1,sequence file block compression format complex requires codecs compress decompress would good file format needs new binary file format
1000,1,change hash function text something handles non ascii characters better http bailey svn sourceforge netview vc bailey trunk src java org apache bailey util hash java view markup replace text hashcode better hash function non ascii strings
1001,1,utility collapse contents directory small number files compaction utility directories
1002,1,currently way find files replica corrupt read files instead fsck report using corrupted blocks found periodic verification fsck show checksum corrupted files
1003,0,had oop added new logo core released documenation logo also used core project website add new logo project site
1004,1,checkpoint verify fs image time creates new one keep two generations fs image
1005,1,something implemented application layer may useful had oop long term log storage systems often keep data sorted sortkey future computations files often benefit sort order job requires grouping sortkey possible reduction map stage natively supported had oop except degenerate case map file per task since splits span sortkey however aligning data read map task sortkey boundaries straightforward would useful capability had oop definition sortkey left application necessarily key field sequence file generic interface otherwise sequence file text file readers use extracted sortkey align map task data key boundaries align map splits sorted files key boundaries
1006,0,think add community section tlp website deeplinks corresponding pages attach generated pdf site tlp site community section
1007,1,jira intended enhance ipc scalability robustness currently ipc server easily hung due disk failure garbage collection can not respond clients promptly caused lot dropped calls delayed responses thus many running applications fail time outside busy clients send lot requests server short period time many clients communicate server simultaneously server may swarmed requests can not work responsively proposed changes aim provide better client server coordination server able throttle client burst requests slow client affect server serving clients temporary hanging server cause catastrophic failures clients client server detect remote side failures examples failures include remote host crashed remote host crashed rebooted remote process crashed shut operator fairness client able make progress improve scalability robustness ipc
1008,0,currently powered by page hard find think add link left navigation bar add link powered by page left navigation bar
1009,1,move h base had oop co remove jira issues move svn https svn apache org repos asf had oop core trunk src contrib h base https svn apache orgreposasfhadoophbase trunk move h base had oop core
1010,0,tests throughput scanning block compressed sequence files sustained throughput bounded mb sec per process cpu process maxed seems cpu consumption high throughput low scanning files reading sequence file consumes cpu maximum throughput mb sec per process
1011,0,had oop website link following pages httpwww apache org foundation thanks html httpwww apache org foundation sponsorship html done boilerplate site every page links pages requirement asf sites website link asf sponsor page
1012,1,currently block transfered data node client interleaves data chunks respective checksums requires creating extra copy original data new buffer interleaved crcs avoid extra copying data crc fed socket one another non interleaved checksums would optimize block transfers
1013,1,user moving invoking random writer config command line like bin had oop jar had oop examples jar random writer output conf xml worked ignores conf xml without complaining equivalent bin had oop jar had oop examples jar random writer conf conf xml output random writer complain many arguments
1014,1,metrics system job tracker defaulting every seconds computing counters jobs work substantial amount work showing running snapshots seen like lower default interval every seconds make low priority thread metrics system job tracker running often
1015,0,significant changes had oop configuration since had oop documentation needs completely overhauled exhaustive accurate javadocs including specific examples b update wiki http wiki apache org lucene had oop how to configure c importantly put page describing had oop configuration had oop website via forrest thing else folks think update documentation had oop configuration post had oop
1016,1,currently max queue size ipc server set handlers usually rpc failures observed eg had oop increase number handlers problem goes away think big part fix increase max queue size think make maxq size per handler configurable bigger default improvements also had oop server keeps reading rpc requests clients number flight rpcs larger maxq size earliest rpcs deleted main feedback server client often heard users had oop handle bursty traffic say handler count default server handler pcs sec quite conservative low typical server implies rpc wait sec dropped clients send rpcs around time rare heartbeats etc dropped stead dropping earliest rpcs server delays reading new rpcs feedback clients would much smoother file another jira regd queue management jira propose make queue size per handler configurable larger default may ipc server max queue size configurable
1017,1,hudson kill long running tests believe supposed quite seem job test really hung would nice timer goes hudson see section killing hung test http wiki apache org lucene had oop hudson build server hudson kill long running tests
1018,1,unlike gzip b zip file format supports splitting compression blocks k default blocks separated synchronization marker bit approximation pi would permit large compressed files split multiple map tasks currently possible unless using had oop specific file format want input format b zip files
1019,0,following user discussion map red system dir parameter needs documentation
1020,1,would nice utility looked first nbytes file picked decoder strings could hooked bin hadoopfscatwebuitextify sequence compressed files create utility convert binary sequence compressed files strings
1021,0,looking alpha website find link ali yun oss documentation had oop compatible filesystems header links azure blob adlsswiftaliyunossaliyun oss documentation missing website
1022,0,using had oop delegation token authentication functionality apache solr part integration testing found following issue delegation token cancelation functionality consider setup solr servers configured use delegation token functionality backed zookeeper invoke following steps step send request create delegation token delegation token dt created successfully step send request cancel dtd t canceled successfully client receives httpresponse step send request cancel dtd t cancelation fails client receives httpresponse step send request cancel dt point get two different responses dt cancelation fails client receives httpresponse dt cancelation succeeds client receives httpresponse also per current implementation server maintains memory cache current tokens updated using zk watch mechanism eg zk watch ensure memory cache synchronized step investigation found root cause behavior due race condition step firing zk watch whenever watch fires step get httpresponse expected case get httpresponse along following error message log client perspective server return http error cancel request sent invalid token ref relevant solr unit test reference https git hub com apache lucene so lrblobcdbceededbabcsolr core src test org apache solr cloud test solr cloud with delegation tokens java l synchronization issue delegation token cancel functionality
1023,0,issue found had oophadoophdfstesttestacl send to end sorry jenkins able catch had oop fixed issue kms client provider secure proxy user token use case nonsecure proxy user case affected new logic ticket open fix fix kms client provider nonsecure proxy user use case
1024,1,missing pom xml way mvn versions set work project missing had oop cloud storage project module pom xml
1025,1,tomcat performance tuning loaded cluster found accept count accept or thread count protocol useful let make configurable kms startup script since kms jetty x targeted branch make additional kms tomcat settings configurable
1026,0,found build issues test runs create releases h script fix release build issues
1027,1,people seeing yet unless explicitly exclude v common slang azure build had oop dependency declaration common slang v creating resolution conflict dependency needed local dynamo db tests propose fix guard explicitly declaring version used tests azure storage one excluding get free impact anything shipped production puts had oop build control versions common slang coming everywhere way commons config version declared had oop common explicitly declare common slang dependency
1028,0,let update website release notes alpha changes update project release notes alpha
1029,1,share had oop component template directories rpm built part build system longer happens files cause harm good since classpath let remove remove vesti gal templates directories creation
1030,1,adls multiple upgrades since version using change list https git hub com azure azure data lake store java blob master changes md update adls sdk
1031,0,mvn install fails trunk new environment following works existing dev setup likely jar cache fix compilation failure missing had oop kms test jar
1032,1,read adls credentials using had oop credential provider api see https had oop apache org docs current had oop project dist had oop common credential provider api html read adls credentials credential provider
1033,1,ftp transfer mode used ftp filesystem block transfer mode ftp data connection mode used ftp filesystem active local data connection mode jira makes configurable make ftp filesystem data connection mode transfer mode configurable
1034,0,had oop building javadoc had oop azure module failed though error related patch had oop directly fixed anyway building whole project fails jdk ref https builds apache org job pre commit had oop build artifact patch process patch javadoc had oop tools had oop azure txt build failure due errors javadoc build had oop azure
1035,0,java version used had oop controlled java home variable defined had o open vsh log information hdfs starts log file means printing java version current shell path jira proposes adding new simple command extension existing had oop version command current java version used had oop also printed avoids customer confusion looking java stack properly configured example checking jce installed correctly minor changed one modifying hdfs cmd hdfs shell script bin directory thanks sujit bringing attention command print java version used had oop hdfs
1036,1,currently one command get state name node good command give state name nodes add ha admin get all service state option get ha state name nodes resource managers
1037,0,n format tests run failures errors skipped time elapsed sec failure org apache hadoopfsadllivetestadl file context main operations live test get file context org apache hadoopfsadllivetestadl file context main operations live time elapsed sec error java lang runtimeexception java lang reflect invocation target exception org apache had oop fs abstract filesystem new instance abstract filesystem java org apache had oop fs abstract filesystem create filesystem abstract filesystem java org apache had oop fs abstract filesystem getabstract filesystem java org apache had oop fs file context run file context java org apache had oop fs file context run file context java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java org apache had oop fs file context getabstract file system file context java org apache had oop fs file context get fs of path file context java org apache had oop fsfs link resolver resolve fs link resolver java org apache had oop fs file context create file context java org apache had oop fs file context main operations base test test get file context file context main operations base test java sun reflect native method access or impl invoke native method sun reflect native method access or impl invoke native method access or impl java sun reflect delegating method access or impl invoke delegating method access or impl java java lang reflect method invoke method java org junit runners model framework method run reflective call framework method java org junit internal runners model reflective callable run reflective callable java org junit runners model framework method invoke explosively framework method java org junit internal runners statements invoke method evaluate invoke method java org junit internal runners statements run before s evaluate run before s java org junit internal runners statements run after s evaluate run after s java org junit runners parent runner run leaf parent runner java org junit runners block junit class runner run child block junit class runner java org junit runners block junit class runner run child block junit class runner java org junit runners parent runner run parent runner java org junit runners parent runner schedule parent runner java org junit runners parent runner run children parent runner java org junit runners parent runner access parent runner java org junit runners parent runner evaluate parent runner java org junit internal runners statements run before s evaluate run before s java org junit runners parent runner run parent runner java org apache maven surefire junit junit provider execute junit provider java org apache maven surefire junit junit provider execute test set junit provider java org apache maven surefire junit junit provider invoke junit provider java org apache maven surefire booter forked booter invoke provider in same classloader forked booter java org apache maven surefire booter forked booter run suites in process forked booter java org apache maven surefire booter forked booter main forked booter java caused java lang reflect invocation target exception null sun reflect generated constructor accessor new instance unknown source sun reflect delegating constructor access or impl new instance delegating constructor access or impl java java lang reflect constructor new instance constructor java org apache had oop fs abstract filesystem new instance abstract filesystem java org apache had oop fs abstract filesystem create filesystem abstract filesystem java org apache had oop fs abstract filesystem getabstract filesystem java org apache had oop fs file context run file context java org apache had oop fs file context run file context java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java org apache had oop fs file context getabstract file system file context java org apache had oop fs file context get fs of path file context java org apache had oop fsfs link resolver resolve fs link resolver java org apache had oop fs file context create file context java org apache had oop fs file context main operations base test test get file context file context main operations base test java sun reflect native method access or impl invoke native method sun reflect native method access or impl invoke native method access or impl java sun reflect delegating method access or impl invoke delegating method access or impl java java lang reflect method invoke method java org junit runners model framework method run reflective call framework method java org junit internal runners model reflective callable run reflective callable java org junit runners model framework method invoke explosively framework method java org junit internal runners statements invoke method evaluate invoke method java org junit internal runners statements run before s evaluate run before s java org junit internal runners statements run after s evaluate run after s java org junit runners parent runner run leaf parent runner java org junit runners block junit class runner run child block junit class runner java org junit runners block junit class runner run child block junit class runner java org junit runners parent runner run parent runner java org junit runners parent runner schedule parent runner java org junit runners parent runner run children parent runner java org junit runners parent runner access parent runner java org junit runners parent runner evaluate parent runner java org junit internal runners statements run before s evaluate run before s java org junit runners parent runner run parent runner java org apache maven surefire junit junit provider execute junit provider java org apache maven surefire junit junit provider execute test set junit provider java org apache maven surefire junit junit provider invoke junit provider java org apache maven surefire booter forked booter invoke provider in same classloader forked booter java org apache maven surefire booter forked booter run suites in process forked booter java org apache maven surefire booter forked booter main forked booter java caused java lang illegalargumentexception valued fsa dl so auth access token provider found conffile orgapachehadoopfsadladl filesystem get non empty val adl filesystem java orgapachehadoopfsadladl filesystem get custom access token provider adl filesystem java orgapachehadoopfsadladl filesystem get access token provider adl filesystem java orgapachehadoopfsadladl filesystem initialize adl filesystem java org apache had oop fs delegate to file system delegate to filesystem java org apache hadoopfsadladladljavasun reflect generated constructor accessor new instance unknown source sun reflect delegating constructor access or impl new instance delegating constructor access or impl java java lang reflect constructor new instance constructor java org apache had oop fs abstract filesystem new instance abstract filesystem java org apache had oop fs abstract filesystem create filesystem abstract filesystem java org apache had oop fs abstract filesystem getabstract filesystem java org apache had oop fs file context run file context java org apache had oop fs file context run file context java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java org apache had oop fs file context getabstract file system file context java org apache had oop fs file context get fs of path file context java org apache had oop fsfs link resolver resolve fs link resolver java org apache had oop fs file context create file context java org apache had oop fs file context main operations base test test get file context file context main operations base test java sun reflect native method access or impl invoke native method sun reflect native method access or impl invoke native method access or impl java sun reflect delegating method access or impl invoke delegating method access or impl java java lang reflect method invoke method java org junit runners model framework method run reflective call framework method java org junit internal runners model reflective callable run reflective callable java org junit runners model framework method invoke explosively framework method java org junit internal runners statements invoke method evaluate invoke method java org junit internal runners statements run before s evaluate run before s java org junit internal runners statements run after s evaluate run after s java org junit runners parent runner run leaf parent runner java org junit runners block junit class runner run child block junit class runner java org junit runners block junit class runner run child block junit class runner java org junit runners parent runner run parent runner java org junit runners parent runner schedule parent runner java org junit runners parent runner run children parent runner java org junit runners parent runner access parent runner java org junit runners parent runner evaluate parent runner java org junit internal runners statements run before s evaluate run before s java org junit runners parent runner run parent runner java org apache maven surefire junit junit provider execute junit provider java org apache maven surefire junit junit provider execute test set junit provider java org apache maven surefire junit junit provider invoke junit provider java org apache maven surefire booter forked booter invoke provider in same classloader forked booter java org apache maven surefire booter forked booter run suites in process forked booter java org apache maven surefire booter forked booter main forked booter java no format test adl file context main operations live test get file context runtime error
1038,0,adls test adl contract rootdir live test rm non empty rootdir nonrecursive failed
1039,0,discussed had oop comment https issues apache org jira browse had oop focused comment id page com atlassian jira plugin system issue tab panels comment tab panel comment following comments still dependencies removed had oop client jar current code builds obsolete snapshot jar found repository server changing project version something new exposes problem build currently dies had oop tools had oops ls seeing issues had oop client modules filing new bug reopen had oop modules dependencies had oop client jar removed had oop
1040,1,azure data lake store filesystem dependent azure data lake store sdk released need snapshot version dependency jira removes sdk snapshot dependency released sdk candidate functional changes dk impact live contract test remove snapshot version sdk dependency azure data lake store filesystem
1041,0,had oop introduced in compatible check disallowed principal like http host used spnego spn breaks following test trunk test web delegation token test kms test trash with secure encryption zones test secure encryption zone with kms used http localhost spnego spn assuming default realm ticket opened bring back support http host valid spnego spn kerberos name parsing bug discovered fixed included necessary part ticket along additional unit test cover parsing different form principals jenkins url https builds apache orgjobhadoopqbttrunkjava linux x test report https builds apache org job pre commit had oop build test report maintain http host spnego spn support fix kerberos name parsing
1042,1,view filesystem override filesystem get link target view file system used resolve symbolic links default filesystem implementation throws unsupported operation exception proposal define get link target view filesystem invoke target filesystem resolving symbolic links path thus returned preferred view fs qualified path used view filesystem handle implement get link target view filesystem
1043,1,current implementation was b supports azure storage keys sas key provided via org apache had oop conf configuration results secrets residing address space was b process providing complete access azure storage account containers added fact was b inherently support acl was b current implementation can not securely used environments like secure had oop cluster jira created add new mode was b operates azure storage sas keys provide fine grained timed access containers blobs providing segway supporting was b secure had oop cluster details issue proposal provided design proposal document azure add news askey mode was b
1044,0,found set spelling errors logging exception messages examples buf er buffer princi al principal existance existence spelling errors logging exceptions code
1045,0,yarn two new default values added yarn configuration two timeline service properties skipped test configuration fields base test configuration fields base fails mistakenly treats two newly added default values regular properties test configuration fields base fails fields default values skipped properties
1046,0,test grid mix memory emulation test total heap usage emulator plugin fails mac following tests fail mac well test resource usage emulators test cumulative cpu usage emulator plugin test resource usage emulators test cpu usage emulator test resource usage emulators test resource usage matcher runner test grid mix memory emulation test resource usage emulators fail environment linux windows
1047,0,ref https builds apache orgjobhadoopqbttrunkjava linux x test report junit org apache had oop fs test symlink hdfs filesystem test create link using part qual path test symlink hdfs filesystem test create link using part qual path fails had oop
1048,1,kms part please refer hdfs design doc add re encrypt encrypted key interface kms
1049,0,fs shell error handling assumes display error message argument null however case leads npe results suppressing actual error information since higher level error handling kicks dumps stack trace npe instead eg deeply unhelpful depending underlying error may stack dumped logged had oop provides since fs shell explicitly dump traces illegalargumentexception appears underlying cause issue line display error called illegalargumentexception handling catch clause log error fs shell suppress real error error message present
1050,1,kms http fs currently uses tomcat propose upgrade latest version upgrade tomcat
1051,1,currently sequence files put sync blocks every bytes would much better configurable much higher default mb distance sync blocks sequence files configurable
1052,1,shell java hardcoded path bin ls correct platforms egn ixos see had oop similar issue remove hardcoded absolute path ls
1053,0,command hadoopfsfsmkdirpusrhd user results stack overflow problem slash end ip address remove command executed correctly stack overflow scheme less default fs trailing slash
1054,0,had oop cleaned javadoc build result javadoc dirs created outside target folders thank saw finding issue had oop output javadoc inside target directory
1055,1,currently mutable rates metrics class serializes writes metrics contains use metrics registry add e even two increments unrelated metrics contained within mutable rates object serialize wr class used rpc detailed metrics may many hundreds threads contending modify metrics instead allow updates unrelated metrics objects happen concurrently let thread locally collect metrics snapshot aggregate metrics threads collected benchmark performance numbers had oop https issues apache org jira secure attachment benchmark results indicate bring significantly higher performance high contention situations make mutable rates metrics thread local write aggregate read
1056,0,shell check is bash supported creates bash shell command verify system supports bash however error message misleading logic updated shell command throws ioexception imply bash run successfully shell command process interrupted internal logic throws interrupted ioexception subclass ioexception example appeared recent jenkins job https builds apache org job pre commit had oop build test report org apache had oop ipc test rpc wait for proxy test interrupted wait for proxy test logic test rpc wait for proxy test interrupted wait for proxy starts thread wait second interrupt thread expecting thread terminate however methods hell check is bash supported swallowed interrupt therefore failed original design desirable swallowed potential interrupt causing test rpc wait for proxy test interrupted wait for proxy fail unfortunately java allow static method throw exception removed static member variable method throw interrupt exception node manager call static method instead using static member variable fix associated benefit tests could run faster longer need spawn bash process uses shell static method variable happens quite often checking operating system had oop running shell check is bash supported swallowed interruptedexception
1057,1,track user level connections many connections user busy cluster many connections server expose num open connections per user metric
1058,1,disk checker fail detect total disk controller failures indefinitely seen real clusters disk checker performs simple permissions based checks directories guaranteed is kio attempted simple improvement write data flush disk disk checker perform disk io
1059,1,disk checker class unused public methods remove cleanup disk checker interface
1060,1,make tests robust timing problems eventual consistent stores need spin wait state examples follow work reimplemented slightly a test utils eventually propose adding class test tree eventually success or replacement eventually wait for operation taking predicate throws exception evaluate exception tries evaluate answer operations tops raising exceptions ca latest plugin back off strategies sca latest lets exponential well linear option adding special handler generate failure exception eg run detailed diagnostics exception text etc java lambda expression friendly testable tested add lambda test utils class tests fix eventual consistency problem contract test set up
1061,1,run command code shell java get situation ignore interruptedexception s refuse shut down due waiting return value subprocess spawned need allow subprocess interrupted killed shell process get skilled currently jvm shutdown subprocesses orphaned killed ability clean subprocesses spawned shell process exits
1062,0,http server has administrator access uses had oop security authorization detect whether http authenticated correct enabling kerberos http spnego two steps kerberos enabled http spnego links can not accessed logs return error message quote http error problem accessing logs reason user dr unauthorized access page quote make sure httpservletrequest get authtype null invoke http server has administrator access get authtype means get authorization scheme request kerberos enabled http spnego configured links can not accessed
1063,1,per discussion had oop like revert had oop removes deprecated a pix line release new replacement api places burden downstream applications revert had oop remove unused trash policy getinstance initialize code
1064,1,add new instrumented readwrite lock had oop common hdfs use improve locking fs dataset impl add new instrumented readwrite lock
1065,0,think catch exception main method dump log error message instead throw stack may frustrate users loglevel main throws exception arguments provided
1066,0,looking maven output running p dist source plugin test jar jar goals invoked twice turned dist profile default outside release context important javadoc source jars think turn default attach javadoc sources jars nondist build
1067,1,generate source code line numbers inclusion javadoc jars given git hub online viewers seem useful days disabling link source option saves us mb had oop common javadoc jars top bundling html source code javadoc jars
1068,1,currently downstream projects want integrate different had oop compatible file systems like was b need list dependencies one creates ongoing maintenance burden projects need update build whenever new had oop compatible filesystem introduced issue proposes adding new artifact transitively includes had oop compatible filesystems similar had oop client new artifact consist pom xml listing individual dependencies downstream users depend artifact sweep everything picking new file system future version matter updating had oop dependency version provide unified dependency artifact transitively includes cloud storage modules shipped had oop
1069,0,had oop introduced safely option prevent accidental deletion large directories lots files rmr skip trash ticket opened track document usage feature filesystem shell html https had oop apache org docs ralph a had oop project dist had oop common filesystem shell html rm document safely option rm command
1070,1,newly added kafka module defines kafka dependency reduce kafka dependencies had oop kafka module
1071,0,current implementation was b correctly handle multiple threads clients calling delete file expected behavior scenarios one thread delete file return true threads receive false however current implementation even though one thread deletes file multiple clients incorrectly get true return delete call bug return value delete calls was b
1072,1,work continues had oop become evident need better hooks start daemons specifically configured users via command subcommand user environment variables x actually standardized way turn means makes bin scripts super functional bit updating consolidate start df ssh start secure dns shone script make starts h stops h know switch users run root un deprecate starts to psh could used root production purposes single user nonproduction users update scripts smarter running privilege
1073,0,maven project info reports pluginversion depends maven shared jar uses bcel work well new lamda expression depends maven shared jar works around problem using custom release bcel fix class format exception trunk build
1074,0,committing branch needed it changes txt however recent commits branch without editing changes txt need update changelog update changes txt reflect changes branch
1075,0,recent investigation turns kms throws exception tomcat logged anywhere see exception message client side stack trace logging stack trance would help debugging kms server log exceptions throwing
1076,0,fix typing mistake in line document had oop metrics properties also could add examples in line document easier understanding metrics tag related configuration fix typing mistake in line document had oop metrics properties
1077,0,sometimes node resource monitor tries read system utilization win utils exe return empty values triggers following exception java langstring index out of bounds exception string index range java langstring substring string java org apache had oop util sysinfo windows refresh if needed sysinfo windows java org apache had oop util sysinfo windows get physical memory size sysinfo windows java org apache had oop yarn util resource calculator plugin get physical memory size resource calculator plugin java org apache had oop yarn server node manager node resource monitor impl monitoring thread run node resource monitor impl java index range sysinfo windows
1078,1,currently pulling version incubating think upgrade latest incubating upgrade h trace version
1079,1,currently pulling version think upgrade latest upgrade commons configuration version
1080,0,reported arp it had oop quote info org apache had oop maven pluginversion info version info mojo get svn uri info string uses string index of string instead string index of into rg apache had oop maven pluginversion info version info mojo version info mojo java lines quote fix findbugs warning version info mojo java
1081,1,add metadata file giving f simpl swift remove entry core default xml swift fs add service load metadata file
1082,0,easily debugfs instantiation problems much detail going add logging can not simply switch file system log slf jclass used many places including tests cast instead add new private slf j logger logger switch logging working base filesystem class take opportunity clean javadocs comments add list exceptions including indicating base classes throw unsupported operation exceptions cut bits comments true outcome patch ides highlight file flawed way another clean filesystem javadocs logging improve diagnostics fs load
1083,0,packages related docker linux container runtime exceed char line length limit enforced checkstyle causes every build fail would like exclude rule causing failure alternatively could look restructure packages question value check really provides ignore package line length checkstyle rule
1084,0,fix warnings findbugs had oop maven plugin
1085,1,ugi background thread renew tgt exception terminates https git hub com apache had oopblobbeeffcafadecfdb dad had oop common project had oop common src main java org apache had oop security user group information java ll something temporarily goes wrong results ioe even recovered renewal done client eventually fail authenticate retry best effort tgt expires hope error recovers retry tgt expires even ugi renewal thread encountered exception
1086,1,based discussion yet us code go still useful release managers similar variant script used apache h base apache kudu imo jacc output easier understand j diff incorporate check compatibility script runs java api compliance checker
1087,0,reported chi wan park bq since release client get set ping interval changed public package private bq gira phone broken examples changes https git hub com apache gira ph blob release gira ph core src main java org apache giraphjobgiraphjobjaval fix source level compatibility had oop
1088,1,z standard https git hub com facebook z std used production months facebook v recently released create codec library add code cz standard compression
1089,0,http had oop apache org releases html shows released january rather august website shows incorrect january release date
1090,0,ugi check tgt andre login from keytab method checks certain conditions met invokes re login from keytab re login from keytab method fails ioexception login user from keytab must done first keytab associated ugi check tgt andre login from keytab method checks keytab is keytab ugi instance variable associated ugi one triggers call re login from keytab problem keytab file ugi instance variable null triggers mentioned ioexception root problem seems creating u givi aug i login user from subject subject method method uses user group information subject constructor constructor following determine keytab subject given keytab ugi instance is keytab set true set sugi instance would keytab subject keytab problems first set keytab file is keytab set true keytab fileset null triggers ioexception method re login from keytab second even first problem fixed still problem assumes subject key tabu gire login using keytab incorrect ugi created using ugi login user from subject subject method case owner subject ugi caller caller responsible renewing kerberos tickets ugi try user group information created subject incorrectly tries renew kerberos ticket
1091,0,retry invocation handler logs warning exception retry many exceptions client automatically handle like filenotfoundexception unresolved path exception etc everyone generates scary looking stack trace warning program continues normally retry invocation handler logs remote exceptions
1092,1,leveraging had oop allow non rpc calls added call queue intended support routing web hdfs calls call queue provide unified protocol independent qos support external calls rpc call queue
1093,1,patchadd sfs shell stat java missing options file status already contains get permission method required returning symbolic permissions fs permission contains method return binary short nothing present standard octal format unix admins base work standard octal permissions hence patch also introduces one tiny method translate to short return octal build already passed unit tests javadoc add formats fs stat command print permissions
1094,0,org apache had oop security ssl test reloading x trust manager checks keystore file last modified time decide whether reload avoid unnecessary reload keystore file changed maintains internal state last loaded whenever tries reload file also updates last loaded variable case exception failing reload retried keystore file last modified time changes chances reload happens keystore file written reload fails probably eofexception load keystore files last modified time changes short period keystore file closed update however last modified time may updated precision period eg second case updated keystore file never reloaded simple fix update last loaded reload succeeds reloading x trust manager keep reloading case exception thoughts reloading x trust manager keep reloading case exception
1095,0,fs permissions string constructor breaks valid permission strings like fs permission class navel y uses umask parser parsing permissions source code public fs permission string mode new umask parser mode get umask mode string umask accepts subtly different esp wrt sticky bit parsing umask parsing fs permission fs permission string constructor recognize sticky bit
1096,0,sasl rpcclient get server principal printed server advertised principal actual principal expect configuration quite useful debugging security related issues also logged improve sasl rpcclient failure logging
1097,0,bug discovered trying run impala lib hdfs java keystore credentials jni threads different classloader bootstrap fail load java keystore provider quote jni util cc java util service configuration error org apache had oop security alias credential provider factory provider org apache had oop security alias java keystore provider factory found java util service loader fail service loader java java util service loader access service loader java java util service loader lazy iterator next service loader java java util service loader next service loader java org apache had oop security alias credential provider factory get providers credential provider factory java org apache had oop conf configuration get password from credential providers configuration java org apache had oop conf configuration get password configuration java org apache had oop fsa filesystem get aws access keys a filesystem java org apache had oop fsa filesystem get aws credentials provider a filesystem java org apache had oop fsa filesystem initialize a filesystem java quote credential provider factory fails classloading lib hdfs jni
1098,0,looks like test failing since had oop committed https builds apache org job pre commit hdfs build test report org apache had oop tracing test tracing test tracing tracing ipc server broken
1099,1,introduce auto close able lock class thin wrapper r entrant lock allows using r entrant lock try resources syntax wrapper functions perform expensive operations lock acquire release path add auto close able lock class
1100,1,rpc layer supports qos protocol sex web hdfs completely unconstrained generalizing server call extensible simple changes handlers enable unifying call queue multiple protocols design server call extensible unified call queue
1101,1,shell java hardcoded path bin bash correct platforms pointed aw reviewing had oop remove hardcoded absolute path shell executable
1102,0,had oop raw local file system react changing umask file context react changing umask via configuration test directory collection fails inconsistent behavior file context react changing umask via configuration
1103,0,had oop common failed generate j diff need fix fix had oop common generate j diff
1104,1,cleaning hives park use filesystem exists often case see code exists open exists delete exists probe needless object stores expensive needless had oops et example stripping also show opportunities optimise things better improve reporting eliminate needless uses filesystem exists is file is directory
1105,1,umbrella jira optimizations reduce object allocations efficiently use proto buf apis unified ipc web hdfs call q enable qos etc ipc layer optimizations
1106,0,race globals non global apis zookeeper available yet stable zk version timeline availability would help make sm aware users global config zk delegation token secret manager jaas config work well zk users process
1107,1,currently kms audit log using log j write text format log refactor people easily add new format audit logs current text format log default behavior remain compatible allow pluggable audit loggers kms
1108,0,org apache had oop fs tests a temporary credentials test sts throws access denied run without aws credentials accesskey secret key config fails instance profile credentials provider credentials chain line used instance profile always provides temporary credential get session token requires long term temporary credential suggestion fix test case tests a temporary credentials test sts error using i am credentials
1109,0,per release instructions https wiki apache org had oop how to release need update had oop project src site markdown index m dvm reflect right versions new features big improvements put together notes had oop hdfs depending others yarn mr update release notes alpha
1110,1,branch later patches various child related bugs listed had oop recently including had oophadoophadoophadoophdf s eliminate use commons httpclient had oop subprojects except had oop tools had oop open stack see had oop however incorporating patches commons httpclient still listed dependency pom files had oop project pom xml had oop yarn project had oop yarn had oop yarn registry pom xml wish remove since commons httpclient still used many files had oop tools had oop open stack need add dependency had oop tools had oop open stack pom xml add note had oop undo commons httpclient removed had oop open stack mostly done had oop version info formerly inherited had oop project pom xml also needs added branch version patch projects undeclared transitive dependencies commons httpclient previously provided via had oop common had oop client may find incompatible change course also means project exposed commons httpclient cve needs fixed reason well remove unneeded commons httpclient dependencies pom files had oop subprojects
1111,0,h error message bq expected h stack trace quote java lang assertionerror expected org junit assert fail assert java org junit assert fail not equals assert java org junit assert assertequals assert java org junit assert assertequals assert java org junit assert assertequals assert java org apache had oop security test groups caching test background refresh counters test groups caching java quote h security test groups caching test background refresh counters seems flaky
1112,1,update was b driver use latest version sdk microsoft azure storage clients currently using version sdk version brings breaking changes need fix code resolve breaking changes certify everything works properly update was b driver use latest version sdk microsoft azure storage clients
1113,0,ioexception throws get password get password return null string cause set conf throws java lang nullpointerexception check is empty null string ldap groups mapping get pass ward return null ioexception throws
1114,0,fix license notice had oop additional fix license notice
1115,1,right git repo branch named master addition trunk branch since master commonplace name recent branch git repositories misleading new folks looks like branch months ago remove delete spurious master branch
1116,1,big features like yarn demonstrate even senior level had oop developers forget daemons need custom opts env var replace custom vars generic handling like username check example generic handling place old var new var had oop name node opts hdfs name node opts yarn resource manager opts yarn resource manager opts n yarn timeline reader optsnhadoopdistcpoptsnma preddistcpoptshadoopdn secure extra opts hdfs data node secure extra opts had oop nfs secure extra opts hdfs nfs secure extra opts had oop job history server opts map red history server opts makes consistent across entire project b consistent every subcommand c eliminates almost custom appending case statements worth pointing subcommands liked is tcp sometimes need higher normal client side heap size custom options huge win combined had oo prc dynamic subcommands means users easily customizations based upon needs without lot weirdo shell aliasing one line shell scripts side deprecate had oop servername opts replace command subcommand opts
1117,0,hive compile had oop snapshot looks like change had oop causes incompatible change sortedmap writable
1118,1,update maven enforcer pluginversion
1119,0,had oop seeing mvn install d skip tests failing branch branch branch failure caused following shado op project module depends had oop buildtools module had oop project module declare had oop buildtools submodule therefore had oop buildtools built building had oop project had oop buildtools pom jar uploaded snapshot repository https repository apache org content repositories snapshots org apache had oop had oop buildtools build failure occurs conditions satisfied add missing dependency setting maven remote resource plugin fix builds
1120,1,current view file system support storage policy related api throw unsupported operation exception view file system support storage policy related api
1121,0,unit test test text input format test split able codecs failed seed stack trace java lang assertionerror key multiple partitions org junit assert fail assert java org junit assert asserttrue assert java org junit assert assertfalse assert java org apache had oop map red test text input format test split able codecs test text input format java bzip compression input stream finds compression marker twice corner case causing duplicate data blocks
1122,1,had oop guava cache introduced allow refreshes name node group cache run background avoiding many slow group lookup seven change seen quite clusters issues due slow group lookups problem prevalent ha clusters slow group lookup hdfs user fail return seconds causing failover controller kill way current guava cache implementation works approximately initial load first thread request groups given user blocks returns subsequent threads requesting user block first thread populates cache key expires first thread hit cache expiry blocks blocked threads return old value feel blocking thread still gives name node issues slow group lookups call fc one blocks lookups slow cause nn killed guava ability refresh expired keys completely background first thread hits expired key schedules background cache reload still returns old value cache eventually updated patch introduces background reload feature two new parameters had oop security groups cache background reload default false keep current behaviour set true enable small threadpool background refresh expired keys had oop security groups cache background reload threads relevant set true controls many threads background refresh pool default likely enough reload cached groups background expiry
1123,0,retry invocation handler need wrap interruptedexception ioexception call thread sleep otherwise interruptedexception handled correctly components hdfs retry invocation handler need wrap interruptedexception ioexception call thread sleep
1124,0,truncate fail use view fs path like return res target filesystem truncate f new length return res target filesystem truncate res remaining path new length truncate fail use view filesystem
1125,1,jira address jing comments https issues apache org jira browse had oop focused comment id page com atlassian jira plugin system issue tab panels comment tab panel comment had oop async call handler use event driven architecture handle async calls
1126,1,current async dfs implementation file system calls invoked returns future immediately clients clients call future get retrieve final results future get internally invokes chain callbacks residing client name node protocol translator pb proto buf rpc engine ipc client callback path bypasses original retry layer logic designed synchronous dfs proposes refactoring make retry also works async dfs support async call retry failover
1127,0,single cluster setup document grep job fails grep job single cluster document fails
1128,1,follow jira had oop find bug warning discussed had oop bq committed findbugs errors rather adding necessary plumbing pom xml make go away add findbugs exclude file xml get rid given kerby rc release add kerby version had oop project pomxmlbqhadoop project pom xml contains dependencies libraries used modules had oop dependency management version mentioned had oop modules inherit had oop project submodules use version submodule version need mentioned pom xml make version management easier follow fixups upgraded mini kdc using kerby
1129,1,slaves sh slaves file get replace workers sh workers file replace slaves workers
1130,1,upgrade yet us wrapper passed vote upgrade apache yet us
1131,0,currently two ways achieve km sha first one documented one running multiple kms instances behind loadbalancer https had oop apache org docs stable had oop kms index html way make use load balancing kms client provider added had oop however usage undocumented think update kms document introduce load balancing kms client provider provide examples also update kms site xml explain mention load balancing kms client provider km sha documentation
1132,0,many as f projects include apache logo add had oop add apache had oop project logo
1133,1,had oo pant code ancient kludge unlikely users still delete trunk scream test x remove had oo pant had oop tools
1134,1,currently future returned ipc async call support future get future get timeout unit support latter well support future get timeout ipc async calls
1135,1,slim docker image removing jdk trunk longer supports remove jdk docker file
1136,0,had oop pulled dist copy native libs script external file call script failing running distro build windows windows distro build fails dist copy native libs
1137,1,dist cp copies file calls get file status get file status destination compare source update metadata necessary dist cp command run without option preserve metadata attributes additional get file status call wasteful dist cp prevent unnecessary get file status call preserving metadata
1138,1,want rename alpha first alpha release however version number also encoded outside pom xml need update change project version alpha
1139,0,noticed mini hdfs cluster fails start trunk blocking unit tests jenkins mini hdfs cluster fails start trunk
1140,0,hi trying use append function n ality existing sequence fileset compression none works file created file already exists nullpointerexception way works specify compression codec thansk mick al unable append sequence file compression none
1141,0,run had oop trace command kerberized name node failed following error hdfs we i chiu encryption root had oop trace list host we i chiu encryption vpc cloud era com war nipc client exception encountered connecting server java lang illegalargumentexception failed specify server kerberos principal name warn security user group information priviledged action exception hdfsvpcclouderacomauth kerberos cause java io ioexception java lang illegalargumentexception failed specify server kerberos principal name exception thread main java io ioexception failed local exception java io ioexception java lang illegalargumentexception failed specify server kerberos principal name host details localhost we i chiu encryption vpc cloud era com destination host we i chiu encryption vpc cloud era com org apache had oop net net utils wrap exception net utils java org apache had oop ipc client call client java org apache had oop ipc client call client java org apache hadoopipcprotobufrpc engine invoker invoke proto buf rpc engine javacom sun proxy proxy list span receivers unknown source org apache had oop tracing trace admin protocol translator pb list span receivers trace admin protocol translator pb java org apache had oop tracing trace admin list span receivers trace admin java org apache had oop tracing trace admin run trace admin java org apache had oop tracing trace admin main trace admin java caused java io ioexception java lang illegalargumentexception failed specify server kerberos principal name org apache had oop ipc client connection run client java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java org apache had oop ipc client connection handle sasl connection failure client java org apache had oop ipc client connection setup iostreams client java org apache had oop ipc client connection access client java org apache had oop ipc client getconnection client java org apache had oop ipc client call client java caused java lang illegalargumentexception failed specify server kerberos principal name org apache had oop security sasl rpcclient get server principal sasl rpcclient java org apache had oop security sasl rpcclient create sasl clients as l rpcclient java org apache had oop security sasl rpcclient select sasl clients as l rpcclient java org apache had oop security sasl rpcclient sasl connect sasl rpcclient java org apache had oop ipc client connection setup sasl connection client java org apache had oop ipc client connection access client java org apache had oop ipc client connection run client java org apache had oop ipc client connection run client java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java org apache had oop ipc client connection setup iostreams client java failing trace admin setproperty common configuration keys had oop security service username key fixing may require restructuring name node principal dfs name node kerberos principal hdfs property trace admin had oop common specify newcommand principal suggestions thanks trace admin support kerberized cluster
1142,0,oo zie job single shell action fails may important needs exact details provide error message coming node manager unsafe cast https git hub com apache had oopblobeffebcbedbchadoop common project had oop common src main java org apache had oop crypto key kms load balancing kms client provider java l classcastexception uncaught exception raised see exact caused exception message oo zie job fails yarn logs reported saved handle classcastexception authentication exception load balancing kms client provider
1143,0,user had oop secure mode login kdc user kinit start firefox enable kerberos access http localhost logs get authorization errors hdfs user could access logs would expect user able web interface logs link results using curl curl v negotiate u tester http localhost logs http user tester unauthorized access page either show links hdfs user able access provide mechanism add users web application realm note pass authentication issue authorization logs suspect logs path secure web descriptor suspect users default access secure paths add ability secure log servlet using proxy users
1144,1,ldap groups mapping currently set timeouts ldap queries create risk long infinite wait connection support timeouts ldap queries ldap groups mapping
1145,0,please look following pre commit build branch https builds apache org job pre commit hdfs build artifact patch process patch asf license problems txt fix asf license warnings branch
1146,0,generating lot javadoc warnings jdk right number limited enlarge limitation probably reveal problems one batch javadoc generation process number javadocs warnings limited
1147,0,hack age haskell org pretty unreliable switch fp complete mirror quiet output make jenkins debugging easier random fixes cleanup docker file
1148,1,currently filesystem statistics exposes following statistics bytesread bytes written read ops large read ops write ops turn exposed job counters map reduce frameworks logic within dfs client map operations counters confusing instance mkdir s counts write op proposed enhancement add statistic dfs client operation including create append create symlink delete exists mkdir s rename expose new properties statistics object operation specific counters used analyzing load imposed particular job hdfs example use identify job send creating large number files information available statistics object app frameworks like map reduce expose additional counters aggregated recorded part job summary add new interface retrieving fsf c statistics
1149,0,none bundled javascript dependencies mentioned license txt let fix add license txt entries bundled javascript dependencies
1150,0,noted had oop lost level db jni related notice license updates done yarn had oop committed let restore restore lost level db jni license notice changes
1151,1,jira proposes improvement had oop remove web hdfs dependencies adl filesystem client build standalone client high level approach would extend had oop filesystem class provide implementation accessing azure data lake scheme used accessing filesystem continue adl azure data lake net path file azure data lake cloud store continue provide web hdfs rest interface client access adls store using web hdfs rest ap is provided adls store refactor azure data lake store independent filesystem
1152,0,dist cp raw data using delete feature following bug appears issue distributed copy issue tries delete things target longer exist source re validates make sure none reserved raw domain dist cp delete feature raw data implemented
1153,0,hdfs showed hdfs return read buf data left stream java io says b qlen zero bytesread returned otherwise attempt read least one byte review implementations iostream buffer offset bytes necessary considered safe add fast exit length implementations input stream read buffer offset bytes exit bytes
1154,1,cross frame scripting xfs prevention u is provided common servlet filter filter set x frame options http header deny unless configured another valid setting number uis could add filters well yarn webapp proxy could add proxied u is appropriate add xfs filter uis had oop common
1155,0,work shown tests catching regressions read fully reviewing documentation shows specification could improved review spec review implementations add tests proposed seek contract streams support seek support positioned readable fix code differs significantly hdfs local fs specify positioned readable add contract tests fix problems
1156,0,example tests org apache had oop fs shell find occasionally time
1157,1,allows metrics collector ams collect metrics sink per user rpc call counts schedule decisions per priority response time useful detect troubleshoot had oop rpc servername node overload issues support metrics source interface decay rpc scheduler metrics
1158,0,update https had oop apache org docs current had oop project dist had oop common filesystem shell html information relative path current working directory suggested y zhang al had oop discussion filesystem shell doc explain relative path
1159,1,per comment https issues apache org jira browse had oop focused comment id page com atlassian jira plugin system issue tab panels comment tab panel comment wheat had oop need remove file util copy merge cc wheat remove file util copy merge
1160,0,creating key contains special character name result failure creating key fact created ok underlying key provide reg kms key names incorrectly encoded creating key
1161,0,saw oom appears caused phantom references introduced filesystem statistics management post details followup comment phantom reference filesystem statistics trigger oom
1162,1,async rpc callers read replies fast enough buffer storing replies could used propose limiting number outstanding async calls eliminate issue limit number outstanding async calls
1163,0,ref comment https issues apache org jira browse had oop focused comment id page com atlassian jira plugin system issue tab panels comment tab panel comment run had oop check native also failed got something like log fix bugs initialization is al library jni bindings
1164,0,andrew wang suggested current kms acl page user focused hard come without reading agree jira puts documentation explain current implementation improve documentation kms acls delegation tokens
1165,1,had oop added shutdown hook manager used different components instead jvm shutdown hook shutdown hook registered currently upper bound execution time seen name node failed shutdown completely waiting shutdown hook finish failover long period time breaks name node high availability scenarios ticket opened allow specifying timeout value registered shutdown hook shutdown hook manager timeout registered shutdown hook
1166,1,currently dfs test command supports ef z options would helpful add wr verify permission rw actual readwrite help script programming add wr options dfs test command
1167,1,umbrella converting had oop hdfs map red yarn allow dynamic subcommands see first comment details umbrella dynamic subcommands had oop shell scripts
1168,1,currently back policy had oop hardcoded base whether call queue full ticket open allow flexible back policies moving average response time rpc calls different priorities allow rpc scheduler call queue back off using response times
1169,1,discussed mailing list like introduce apache kerby had oop initially good start upgrading had oop mini kdc kerby offerings apache kerby https git hub com apache directory kerby apache directory subproject java kerberos binding provides simple kdc server borrowed ideas mini kdc implemented facilities existing mini kdc currently mini kdc depends old kerberos implementation directory server project implementation stopped maintained directory community plan replace implementation using kerby mini kdc use kerby simple kdc directly avoid depending full directory project kerby also provides nice identity backends lightweight memory based one simple json one easy development test environments upgrade had oop mini kdc kerby
1170,1,ipc client underlying mechanism already supporting asynchronous calls calls shares connection call requests sent using threadpool responses order indeed synchronous call implemented invoking wait caller thread order wait server response jira change ipc client support asynchronous mode asynchronous mode return request sent wait response server change ipc client support asynchronous calls
1171,1,update yet us update yet us
1172,0,had oop added support rpc congestion control sending re triable server busy exceptions clients however every back off results log message seen log messages slow name node already metric tracks number back off events log message adds nothing useful ipc server also allow services skip logging certain exception types altogether ipc server allow suppressing exception logging type log server busy messages
1173,0,javadocs authentication filter say however string implementation longer available had oop moved test artifact also mention anything file backed secret provider file signer secret provider javadocs signer secret provider date authentication filter
1174,0,had oop pulled dist layout stitching dist tar stitching scripts had oop dist pom xml external files appears change working correctly windows external distribution stitching scripts work correctly windows
1175,0,many bundled dependencies source binary artifacts license txt notice txt verify license txt notice txt
1176,1,use resource manager node manager instead job tracker task tracker remove mrv terms http authentication md
1177,0,attempting compile open stack fairly fresh maven repo fails due commons httpclient declared dependency fixed maven dependency analyze shows problems fix had oop open stack undeclared unused dependencies
1178,1,grid mix require raw java command line run add subcommand grid mix
1179,1,h record moved bin rcc never updated pull classes streaming jar remove bin rcc script
1180,1,discussed mailing list disable style checks class setters like following disable hiding field style checks class setters
1181,1,had oop tools grows biggerbigger becoming evident single directory gets sucked starting become big burden number tools grows let re work smarter rework had oop tools
1182,1,let pull shellcode had oop dist pom xmlpull shellcode had oop dist
1183,0,error message expected stack trace quote java lang assertionerror expected org junit assert fail assert java org junit assert fail not equals assert java org junit assert assertequals assert java org junit assert assertequals assert java org junit assert assertequals assert java org apache had oop fs symlink base test test set times dangling link symlink base test java org apache had oop fs test symlink local fs test set times dangling link test symlink local fs java sun reflect native method access or impl invoke native method sun reflect native method access or impl invoke native method access or impl java sun reflect delegating method access or impl invoke delegating method access or impl java java lang reflect method invoke method java org junit runners model framework method run reflective call framework method java org junit internal runners model reflective callable run reflective callable java org junit runners model framework method invoke explosively framework method java org junit internal runners statements invoke method evaluate invoke method java org junit internal runners statements fail on timeout statement thread run fail on timeout java quote happens recent builds https builds apache org job pre commit had oop build test report org apache had oop fs test symlink local fs filesystem test set times dangling link https builds apache org job pre commit had oop build test report org apache had oop fs test symlink local fs filesystem test set times symlink to file https builds apache org job pre commit had oop build artifact patch process patch unit had oop common project had oop common jdk txt test symlink local fs filesystem fails intermittently
1184,0,findbugs warnings branch fix findbugs warnings had oop common branch
1185,0,logging slow name resolutions would useful identifying dns performance issues cluster resolutions goorg apache had oop security security util get by name see attached call graph adding additional logging method would expose issues log slow name resolutions
1186,0,had oop collects node network usage linux jira windows collect network disk usage node running windows
1187,1,java support stlsvtlsvsecuretlsv supported java add default list had oops sl enabled protocols enable tls v
1188,1,h base h master port number conflicts had oop kms port number uses might use cases user need kms h base present cluster h base able encrypt h files user might need kms encrypt hdfs directories users would manually override default port either application cluster would nice different default ports kms h base could naturally coexist change kms server port number conflicts h master port number
1189,0,added updated changelog release notes based upon yet us snapshot update changelog release notes
1190,0,kms server encounters unexpected error resulting httpresponse log stack trace makes difficult troubleshoot client side exception can not provide details kms log detailed stack trace unexpected errors
1191,0,ldap groups mapping lots configurable properties thus fairly complex nature hdfs permissions guide minimal introduction ldap groups mapping reference information configuring group mapping service available javadocs however javadoc provides information configure core default xml descriptions property still lacks comprehensive tutorial without tutorial guide configurable properties would buried sea properties cloud era horton works information regarding ldap group mapping httpwww cloud era com documentation enterprise latest topics cmsg ldap grp mappings html http horton works com blog had oop group mapping ldap integration neither cover configurable features using ssl ldap posix group semantics write new group mapping service guide
1192,0,found had oop key command usage documented reviewing hdfs addition document uppercase allowed key name had oop key command usage documented
1193,1,typical ldap group name resolution works well typical scenarios however seen cases user mapped many groups extreme case user mapped groups way implemented makes case super slow resolving groups active directory current ldap group resolution implementation sends two queries active directory server first query returns user object contains dn distinguished name second query looks groups user dn member user mapped many groups second query returns group objects associated user thus slow studying user object active directory found user object actually contains member of field dn group objects user belongs assuming organization recursive group relation user member group g group g member group g use properties avoid second query potentially runs low propose add configuration enable feature users want reduce group resolution time recursive groups existing behavior broken faster ldap group name resolution active directory
1194,1,remove get acl status call no nacl commands get f acl remove get acl status call no nacl commands get f acl
1195,0,currently user uses h base enables client job classloader job fails load h base classes example h base classes org apache had oop h base meet system classes criteria supposed loaded strictly base classloader had oop provide h base dependency exclude h base classes system classes unless h base provided future version had oop h base classes fail load client job classloader enabled
1196,0,per discussion hdfs jira propose adding step release process https wiki apache org had oop how to release update release year webui footer creating rc release add step release process update release year webui footer
1197,1,various ssl security fixes needed seecvecvecvecve update apache httpclient version http core
1198,1,rolling filesystem sink rolls new directory new metrics record comes issue hdfs update filesize closed hdfs new metrics record comes filesize never updated jira add background thread sink eagerly close file top hour rolling filesystem sink eagerly rotate directories
1199,1,protect csr f attacks had oop introduces csr f filter require specific http header sent every rest api call affect api consumers webapps cl is curl since csr f primarily browser based attack try minimize impact non browser clients enhancement provide additional configuration identifying non browser user agents skipping enforcement header requirement anything identified non browser largely limit impact browser based put post calls configured appropriately extend csr f filter user agent checks
1200,1,ali yun oss widely used among china cloud users currently easy access data laid oss storage user had oops park application original support oss had oop work aims integrate ali yun oss had oop simple configurations park had oop applications read write data oss without code change narrowing gap user app data storage like done had oop incorporate ali yun oss filesystem implementation
1201,1,problem user job adds many dependency jars command line had oop classpath part addressed including using wildcards can not done lib jars argument today takes fully specified file paths may want consider supporting wildcards way help users situation idea handle way jvm expands list jars directory traverse child directory also probably would good idea lib jars e files archives support wildcard lib jars argument
1202,0,seeing error running test patch apache had oop quote confirming git environment error users r chiang dev ah patch process git repo quote follow email trivial bug yet us wrapper missing popdex traction simple fix run command twice since short circuit kicks yet us cached fix git environment check test patch
1203,0,user access permission local directory had oop fs put command prints confusing error message file directory incorrect error message fs put local dir without permission
1204,0,encountered npe trying use h base utility export snapshot azure target turns verify and convert to standard format returning null determining h base root added atomic renamed irs set java lang nullpointerexception org apache had oop fs azure azure native filesystem store is key for directory set azure native filesystem store java org apache had oop fs azure azure native filesystem store is atomic rename key azure native filesystem store java org apache had oop fs azure native azure filesystem prepare atomic folder rename native azure filesystem java org apache had oop fs azure native azure filesystem rename native azure filesystem java org apache had oop h base snapshot export snapshot run export snapshot java org apache had oop util tool runner run tool runner java co my ammer calm ie snapshot abstract snapshot util export snapshot abstract snapshot util java co my ammer calm ie snapshot abstract snapshot util run abstract snapshot util java org apache had oop util tool runner run tool runner java co my ammer calm ie snapshot snapshot azure blob util main snapshot azure blob util java npe trying rename directory windows azure storage filesystem
1205,0,fix had oop map reduce client native task unit test fails able open glibc bugs pill file fix had oop map reduce client native task unit test fails able open glibc bugs pill file
1206,1,checkstyle checks realistic like line length leading spurious pre commit let disabled is able spurious checkstyle checks
1207,0,test local fsf c statistics failing sometimes fails appears fc statistics base test test statistics thread local data cleanup test timing fails test local fsf c statistics test statistics thread local data cleanup times occasionally
1208,1,need metrics sink write metrics hdfs sink accept configuration directory prefix following put metrics get yyyymmdd hh current timestamp hdfs dir dir prefix yyyymmdd hh exist create close currently open file create new file called log new directory write metrics current log file add hdfs metrics sink
1209,0,seen several failures test kms provider failed following snippet error message quote values different actual k quote test kms test kms provider intermittently fails test rollover draining
1210,1,csr f prevention rest ap is provided common servlet filter filter would check existence expected configurable http header xx srf header fact csr f attacks entirely browser based means approach ensure requests coming either applications served origin rest api explicit policy configuration allows setting header xmlhttprequest another origin add csr f filter rest apis had oop common
1211,0,had oop resolved replacing return null throwing ioexception causes several filesystem operations fail possibly sites allows dist cp succeed filesystem operations stopped working correctly
1212,0,dead links compability mdlinks mr app master job history server rest api wrong https had oop apache org docs r had oop project dist had oop common compatibility html rest apis fix dead links compatibility md
1213,0,https builds apache org job had oop common trunk test report org apache had oop crypto key kms server test kms test kms restart simple auth seems introduced had oop fix test kms test kms restart failure
1214,0,start build env sh fails branch found map reduce https builds apache org job pre commit map reduce build console start build env sh fails branch
1215,1,currently value ipc client rpc timeout ms greater timeout overrides ipc ping interval client throw exception instead sending ping interval passed rpc timeout work without effectively disabling ipc ping rpc timeout override ipc ping interval
1216,1,currently embeded jetty server used across had oop services configured ssl server xml file respective configuration section however ssl tls protocol used jetty servers downgraded weak ciphersuites exclude ciphers supplied key support excluding weak ciphers http server ssl server xml
1217,1,h description jira describes new file system implementation accessing microsoft azure data lakes to read l within had oop would enable existing had oop applications mr hive h base etc use adl store input output adl ultra high capacity optimized massive throughput rich management security features details available https azure microsoft co menus services data lake store support microsoft azure data lake file system had oop
1218,0,another address use error saw test test kerberos authentication handler test name rules failed due porting binding error https builds apache org job had oop common trunk java test report org apache had oop security authentication server test kerberos authentication handler test name rules looking mini kdc implementation port constructor use serversocket find unused port assign port number member variable port close serversocket object later in it kdc server instantiate tcp transport object bind port appear sport may used throw exception mini kdc throws address use bind exception
1219,0,saw pre commit jenkins job https builds apache org job pre commit had oop build test report org apache had oop http test http server test bind address also appeared previously had oop common trunk java jenkins oct following case first server bound port second one bound port violated assertion test case second port supposed first test http server test bind address bind port range wider expected
1220,1,yet us release rip components makedev support replace wrappers wrappers default sane version allow version overrides via env var download patch process execute given parameters marking incompatible change since also remove filename extensions move bin directory better maintenance towards future replace dev support wrappers yet us
1221,1,currently was b implementation hdfs interface support append api jira added design implement append api support was b intended support append would support single writer adding append api support was b
1222,1,had oop added compile runtime dependence intel is al library add docker file could part docker based build environment start build env sh needs fixed intel is al libraries added docker file
1223,1,add config disable logs endpoint http server listing directory like dangerous security perspective keep enabled default compatibility though add config disable logs endpoints
1224,0,debugging nm retry connection rmnonhanmlogrm time misleading actually log client side retry network connection failure include info retry invocation handler real retry policy works add fail action reason much multiple retry policies addition keep consistent loglevel message retry attempts ipc client info retry invocation handler debug fail keep consistent could confusing retry policies failover on network exception retry put retry failed reason log rm proxy retry could misleading
1225,0,npe thrown hiding actual error throw number format expect ion since count exceeds integer range npe test sequence file
1226,0,failure seems exist november rd still tracing come https builds apache org job had oop common trunk test report org apache had oop fs shell find test find process arguments error message test timed milliseconds stack trace test find process arguments occasionally fails
1227,0,https builds apache org job had oop common trunk test report junit org apache had oop security authentication util test zk signer secret provider test multiple in it error message expected null stack trace java lang assertionerror expected null org junit assert fail assert java org junit assert fail not null assert java org junit assert assertnull assert java org junit assert assertnull assert java org apache had oop security authentication util test zk signer secret provider test multiple in it test zk signer secret provider java think failure introduced had oop likely root cause error zk signer secret provider unexpected exception occurred pulling data from zookeeper java lang illegalstateexception instance must started calling method com google common base preconditions check state preconditions java org apache curator framework imps curator framework impl get data curator framework impl java org apache had oop security authentication util zk signer secret provider pull from z kzk signer secret provider java org apache had oop security authentication util zk signer secret provider roll secret zk signer secret provider java org apache had oop security authentication util zk signer secret provider enhancer bymockitowithcglibfcglib roll secret org apache had oop security authentication util zk signer secret provider enhancer by mock ito with cg lib ffast class by mock ito with cglibfinvokeorgmockitocg lib proxy method proxy invoke super method proxy java org mock ito internal creation abstract mock ito method proxy invoke super abstract mock ito method proxy java org mock ito internal invocation real method c glib proxy real method invoke c glib proxy real method java org mock ito internal invocation real method filtered c glib proxy real method invoke filtered c glib proxy real method java org mock ito internal invocation invocation call real method invocation java org mock ito internal stubbing answers calls real methods answer calls real methods java org mock ito internal mock handler handle mock handler java org mock ito internal creation method interceptor filter intercept method interceptor filter java org apache had oop security authentication util zk signer secret provider enhance rbymockitowithcglibfroll secret org apache had oop security authentication util rollover signer secret provider run rollover signer secret provider java java util concurrent executors runnable adapter call executors java java util concurrent future task run and reset future task java java util concurrent scheduled threadpool executor scheduled future task access scheduled threadpool executor java java util concurrent scheduled threadpool executor scheduled future task run scheduled threadpool executor java java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java test zk signer secret provider test multiple in it occasionally fail
1228,0,users was braised complaints error message returned back was b trying connect azure storage anonymous credentials current implementation returns correct message encounter storage exception however scenarios like querying check container exists throw storage exception returns false uri directly specified anonymous access error message returned clearly state credentials storage account provided jira tracks fix error message return returned storage exception hit also correct spelling mistakes error message fix exception message was b connecting anonymous credential
1229,0,kms client provider create connection close stream twice one finally block may called exception thrown may throw exception need careful swallow exception exception may swallowed kms client provider
1230,0,observed test failure times past fails expected access time file link always less actual access time error message test symlink local fs file context test set times symlink to dir occasionally fail
1231,0,seen test failed times past error message test metrics system impl test qsize occasionally fail
1232,1,file context class currently annotated evolving however point really need treats table interface file context abstract filesystem annotated stable interface
1233,0,jenkins failing test compressor decompressor exception caught converted fail stack trace value test compressor decompressor failing without stack trace s
1234,0,found issue had oop quote tests run failures errors skipped time elapsed sec failure org apache had oop fs shell test copy preserve flag test directory cp with porg apache had oop fs shell test copy preserve flag time elapsed sec error java io ioexception mkdir s failed create exists false cwdtestptchhadoophadoop common project had oop common target test data test stat org apache had oop fs checksum filesystem create checksum filesystem java org apache had oop fs checksum filesystem create checksum filesystem java org apache had oop fs filesystem create filesystem java org apache had oop fs filesystem create filesystem java org apache had oop fs filesystem create filesystem java org apache had oop fs filesystem create new file filesystem java org apache had oop fs shell test copy preserve flag initialize test copy preserve flag java quote fix intermittent test failure test copy preserve flag
1235,0,jenkins found test failure had oop quote tests run failures errors skipped time elapsed sec failure org apache had oop metrics impl test ganglia metrics test ganglia metrics org apache had oop metrics impl test ganglia metrics time elapsed sec failure java lang assertionerror missing metrics test rec xxx org junit assert fail assert java org junit assert asserttrue assert java org apache had oop metrics impl test ganglia metrics check metrics test ganglia metrics java org apache had oop metrics impl test ganglia metrics test ganglia metrics test ganglia metrics java quote fix intermittent test failure test ganglia metrics
1236,1,writeable rpc en inge depends java serialization mechanisms rpc requests without proper checks shown lead security vulnerabilities remote code execution eg collections had oop current implementation migrated writeable rpc engine proto buf rpc engine jira proposes deprecate writeable rpc engine branch remove trunk deprecate writeable rpc engine
1237,0,test rpc test client back off failing test rpc test client back off failing
1238,0,looking identifier implementations eg abstract delegation token identifier others see get user method return null debug logging enabled npes get user expected return null either checked erred upon better error allowed happen would otherwise happen debug log path npe sasl rpc server
1239,1,hdfs fetch dt missing critical features geared almost exclusively towards hdfs operations additionally token files created use java serialization shard impossible deal languages replaced better utility common readwrite proto buf based token files enough flexibility used services offers key functionality append rename old version file format still supported backward compatibility effectively deprecated follow jira de pre crate fetch dt updated utility create modify token files
1240,1,would better had oop docker file could used yet us external dependencies owned project make had oop docker file usable yet us
1241,0,fix sprintf warnings domain socket c introduced had oop fix sprintf warnings domain socket c introduced had oop
1242,0,had oop mvn package pdi std skip tests fails jdk no format error exit code test ptchhadoophadoop tools had oo paws src main java org apache had oop fs blocking threadpool executor service java error invalid uri https git hub error inspired jdk fix javadoc error caused illegal tag
1243,1,would good could read creds source via java property had oop configuration option read creds credential provider
1244,0,open had oop share doc had oop api index html click classes click access control list page shows page displayed error dist cp impersonation provider default impersonation provider also javadoc generated trunk problem had oop javadoc broken links access control list impersonation provider default impersonation provider dist cp
1245,0,had oop added several new tests covering functionality resolving hostnames based alternate network interface tests failing windows test dns fails windows had oop
1246,1,make j dependency consistent parts had oop seeing weird rare failures older versions maven appear related make j dependency consistent
1247,0,had oop changed behavior azure storage lease violation deletes appears test azure filesystem instrumentation test client error metrics partly dependent old behavior simulating error tracked metrics system seeing intermittent failures test test azure filesystem instrumentation test client error metrics fails intermittently due assumption lease error thrown
1248,0,issue proposes implement had oop filesystem contract tests had oop azure was b contract tests define expected semantics file system running had oop azure likely catch potential problems improve overall quality run filesystem contract tests had oop azure
1249,1,had oop azure tests support execution live azure storage service developer specifies key azure storage account configuration works overwriting src test resources azure test xml file error prone process azure test xml file checked revision control show example risk tester could overwrite azure test xml containing keys accidentally commit keys revision control would leak keys world potential use attacker issue proposes use xinclude isolate keys separate file ignored g it never committed revision control similar setup already used had oo paws integration testing use xinclude had oop azure test configuration isolate azure storage account keys service integration tests
1250,1,had oop remove metrics v trunk remove metrics v
1251,0,osx jdk issues localization cause utils hell run fail fixed jdk jdk jdk still broken fix posix spawn error osx
1252,0,running bats docker container jenkins fails bash unit tests failing
1253,0,originally requested url contains query string gets translated original url query parameter without original query string cause redirect back requested resource in valid jwt redirect authentication handler retain original query string
1254,0,page toc documentation page maintained hand automatically generated do xi a macro supported do xia module markdown page toc documentation automatically generated do xi a macro
1255,1,kms dt currently token identifier class de support decoding kms delegation token identifier
1256,0,sample code tracing md problems compilation error importing tracer generic options reflected tracer initialized tool runner run may confusing use fs shell example embedded tracer fix tracing documention reflecting update h trace
1257,0,test text command use mkdir s rather mkdir create test directory test text command use mkdir s rather mkdir create test directory
1258,0,clean h trace integration issues clean h trace integration issues
1259,1,filesystem create nonrecursive deprecated however distributed filesystem create implementation throws exception parent directory exist limits clients migration away deprecated method h base io fencing relies behavior filesystem create nonrecursive variant create method added throws exception parent directory exist un deprecate create nonrecursive
1260,0,had oop kill command execution failure ubuntu nm restarts can not get process alive via pid containers can not kill process correctly rm tells nm kill container logs nm customized logs fix kill commandbehavior linux distributions
1261,0,java util regex classes performance problems certain wildcard patterns namely consecutive characters filename properly escaped literals cause commands had oop f sls filename consume cpu probably never return reasonable time timescales number example every string causes filename reproduces reliably glob pattern regex library performance issues wildcard characters
1262,0,jenkins trunk java saw failure test rpc test rpc interrupted simple interrupt picked race test surfacing bug rpc points interrupt exceptions picked test rpc test rpc interrupted simple fails intermittently
1263,0,test web delegation token failed jenkins port use looks like code searches freeport starts jetty may be enough race condition port location jetty start cause intermittent failures test web delegation token failing port use
1264,0,discovered big top had oop nfs module compilation broken looks like had oop root cause hdfs nfs builds broken missing compile time dependency netty
1265,0,had oop ipc tests a slr pc broken post had oop string coming exception detailed test expects emergency patch progress test failing had oop ipc tests a slr pc
1266,1,cli using command with destination java add copying tail filename copy blob store like swift create copying file rename expensive direct flag allow user avoiding copying file add direct flag option fs copy user choose create copying file
1267,0,warnings had oop data join module warnings had oo pant module fix findbugs warnings had oop tools module
1268,0,usage had oop fs expunge empty trash refer hdfs architecture guide information trash feature description confusing gives user impression command empty trash actually remove sold checkpoints user sets pretty long value fs trash interval command remove anything checkpoints exist longer value description hdfs expunge command confusing
1269,1,would useful rd party apps know locations things had oop running without explicit path env vars set expose calculated paths
1270,0,docs http had oop apache org docs r had oop project dist had oop common filesystem shell html get merge say addnl valid parameter had oop replaced nl docs updated had oop fs get merge doc wrong
1271,1,seen many cases customers deleting data inadvertently skip trash fs shell prompt user size data number files deleted bigger threshold even though skip trash used add safely flag rm prompt deleting many files
1272,0,cpu usage information windows computed incorrectly proposed patch fixes issue unifies interface linux fix computing cpu usage statistics windows
1273,0,logging around was b component limited disabled default improvement created add logging around reads writes deletes azure storage exception capture blobs hit exception information useful communicating azure storage team debugging purposes was blogging improve was blogging around deletes reads writes
1274,0,metrics system impl creates metrics source adapter wrong time unit parameter metrics source adapter expects time unit millisecond jmx cache ttl metrics system impl passes time unit second metrics source adapter constructor metrics system impl creates metrics source adapter wrong time unit parameter
1275,1,had oop mitigated problem h master aborting region server due azure storage throttling event h base wal archival way achieved applying intensive exponential retry throttling occurred second level mitigation change mode copy operation operation fails even retries e client side copy blob copy back destination operation subject throttling hence provide stronger mitigation however expensive hence case fail retries change mode copy operation h base wal archiving bypass azure storage throttling retries
1276,1,jira proposes add counter called rpc slow calls also configuration setting allows users log really slow rpcsslowrpcsrpcsfallth percentile useful troubleshoot certain services like name node freezes heavy load rpc metrics add ability tracklogs low rpcs
1277,1,new jvm pause monitor written start stop lifecycle already proven brittle ordering operations even had oop thread safe start stop potentially entrant also requires every class supports monitor add another field perform lifecycle operations lifecycle yarn services yarn app lifecycle implemented had oop common making monitor subclass abstract service moving in it starts top operations service in it service start services top methods fix concurrency state model issues make trivial add child yarn service subclasses composite service nmr mapps able hook monitor simply creating on ector adding child make jvm pause monitor abstract service
1278,0,debian machine seen node manager recovery containers fail signal syntax process group may work see errors checking process alive container recovery causes container declared lost node manager restart application fail error attempts retried applications fail nm restart linux distro nm container recovery declares container lost
1279,0,observed yarn tests failing test rm admin service nullpointerexception s build build failure https builds apache org job pre commit yarn build artifact patch process test run had oop yarn server resource manager txt npe jvm pause monitor calling stop start
1280,0,had oop fixed bug delegate to file system using wrong default port side effect patch file path urls previously port insert port per default implementation filesystem get default port runtime cause application erroneously try contacting port remote blob store service connection fails ultimately renders was b probably custom filesystem implementations outside had oop source tree completely unusable default filesystem applications using file context fail default filesystem configured was be tc
1281,0,had oop introduced way set java static values posix flags resulted compilation error windows fix native compilation windows had oop
1282,0,linux set netg rent returns linux something wrong happen memory unknown group unavailable service etc error message set exception thrown set netg rent returns linux exception thrown
1283,0,provide better visibility parsing configuration failure logging full error message propagate error message parsing configuration back client throw exception fs permissions umask modem is configured
1284,1,using ldap groups mapping had oop nested groups supported example user jdoe part group member group b group mapping currently returns group currently facility available shell based unix groups mappings ssd similar tools would good feature part ldap groups mapping directly add support nested groups ldap groups mapping
1285,0,test patch sh give detail warning report seen jenkins run hdfs https builds apache org job pre commit hdfs build artifact patch process new patch findbugs warnings had oop hdfs html test patch sh give detail findbugs warning report
1286,1,order enable significantly better unit testing well enhanced functionality large portions config sh pulled functions see first comment pull argument parsing function
1287,0,similar issue had oop had oop found customer h base cluster logs piece storage exception complaining lease id updating folder last modified time was b
1288,0,had oop yet us quote release doc maker py work behind proxy urllib url open care environment varia libes like http proxy https proxy quote release doc maker py work behind proxy
1289,0,jira propose collect disks usages node jira part larger effort monitoring resource usage s nodes collect disks usages node
1290,0,jira propose collect network usage node jira part larger effort monitoring resource usage s nodes collect network usage node
1291,1,proto c maven plugin currently generates new java classes every time means maven always picks changed files build would better proto c plugin generated new java classes source proto c files change support incremental generation proto c plugin
1292,1,network top logy uses nodes list children access children slow linear search network topology efficient adding getting removing nodes
1293,1,monitoring functions could moved yarn common easier sharing move resource calculator plugin yarn common
1294,0,tests eg https builds apache org job pre commit had oop build initial mvn install triggered full testsuite run jenkins switches old test patch new test patch badcopy test patch bindir prior exec
1295,0,using syntax like cygwin may error oun sly evaluated true cygwin unset need fix branch fix un recommended syntax usages had oop hdfs yarn script cygwin branch
1296,1,given test patch tendency get forked variety different projects makes lot sense make apache tlp everyone benefit common codebase umbrella split test patch tlp
1297,0,error message argument different wanted printstream println disk quota rem disk quotas sd quot are mssd quota archive quota rem archive quota pathname org apache had oop fs shell test count process path with quotas by qt vh test count java actual invocation different arguments printstream println ssd quot are mssd quota disk quota rem disk quota archive quota rem archive quota pathname org apache had oop fs shell count process options count java check following report https builds apache org job pre commit had oop build test report org apache had oop fs shell test count fails
1298,0,similar issue had oop had oop happens h base distributed log splitting jira happens h base deleting old wals trying update h base old wals folder fix had oop storage exception complaining lease id updating folder last modified time was b
1299,0,kms client provider call validates content type returned going going work locales uppercase returned kms client provider uses equalsignorecase application json
1300,0,mvn package pdi std skip tests fails jdk illegal tags follows jdk fix javadoc errors caused incorrect illegal tags
1301,1,requirement support ldap based authentication scheme via had oop authentication filter had oop added support plug custom authentication scheme addition kerberos via alt kerberos authentication handler class based selecting authentication mechanism based user agent http header conform http protocol semantics per rfc httpwww w org protocols rfc rfc html http protocol provides simple challenge response authentication mechanism used server challenge client request client provide necessary authentication information mechanism initiated server sending authenticate response www authenticate header includes least one challenge indicates authentication scheme parameters applicable request uri case server supports multiple authentication schemes may return multiple challenges authenticate response challenge may use different auth scheme user agent must choose use strongest auth scheme understands request credentials user based upon challenge existing had oop authentication filter implementation supports kerberos authentication scheme uses negotiate challenge part www authenticate response header per following documentation negotiate challenge scheme applicable kerberos windows ntlm authentication schemes spnego based kerberos ntlm http authentication http tools ietf org html rfc understanding http authentication https msdn microsoft co menus library ms vvs aspx hand ldap authentication typically basic authentication scheme used note tls mandatory basic authentication scheme http httpd apache org docs trunk mod mod auth nz ldap html hence feature idea would provide custom implementation had oop authentication handler authenticator interfaces would support schemes kerberos via negotiate auth challenge ldap via basic auth challenge authentication phase would send challenges let client pick appropriate one client responds authorization header tagged negotiate use kerberos authentication client responds authorization header tagged basic use ldap authentication note httpclient seg curl apache http java client need configured use one scheme eg curl tool supports option use either kerberos via negotiate flag username password based authentication via basic u flags apache httpclient library configured use specific authentication scheme http hc apache org http components client ga tutorial html authentication html typically web browsers automatically choose authentication scheme based notion strength security eg take look design chrome browser http authentication https www chromium org developers design documents http authentication support multiple authentication schemes via authentication filter
1302,0,currently bit check security user group information java uses os arch checks x returned ibm z platform x bit without change try use hdfs spark get fatal error unable login find login class address fixes said issue identifying x bit platform thus allowing spark run z linux simple fix big implications fix user group information java support bitz linux
1303,0,default policy retry utils retry re triable exceptions even default retry policy enabled true discovered via hdfs client failing retry get file block locations check nn startup failed default retry policy handle re triable exception correctly
1304,0,abstract java keystore provider class credential provider api cache member variable interrogation access populate incomplete cache mechanism credential provider api
1305,0,azure filesystem page blob input stream returne of scenarios causes infinite hands reading files eg copy to local hang forever azure filesystem page blob input stream returne of
1306,0,conftest dist ch jni path trace enabled had oop cmd kerb name enabled appear help message bin had oop subcommands available windows
1307,1,guice work lambda statement https git hub com google guice issues upgrade includes fix jdk update guice version
1308,1,right credentials works cleartext passwords configs secret accesskey uri non uri version use credential providers fall back cleartext option credentials support use credential provider
1309,0,disk checker check dirs check null pointer return value file list files based document file list files https docs oracle com java se docs api java io file html list files good check null pointer throw disk error exception null use directory stream disk checker check dirs detect errors listing directory
1310,1,since min version jdk hardlink support via files means deprecate jni implementation discontinue usage deprecate usage native io link
1311,1,http authentication cookie contains authentication token dropped expiry time authentication token configured via had oop http authentication token validity default value hours clusters require enhanced security desirable configurable max inactive interval authentication token activity max inactive interval authentication token invalidated max inactive interval less had oop http authentication token validity default value minutes enable max inactive interval had oop http auth token
1312,0,ru option had oop f sls introduced had oop targeted means feature unimplemented documented http had oop apache org docs r had oop project dist had oop common filesystem shell html ls fix document remove unimplemented option had oop f sls document branch
1313,0,had oop related issues implemented parallel tests maven profile running j unit tests multiple concurrent processes issue proposes activate profile pre commit speed execution enable parallel j unit tests pre commit
1314,1,thread local random used available place thread local jdk difference minimal jdk starts including optimizations thread local random replace uses thread local jdk thread local random
1315,0,right initialization hte thread local factories encoder decoder text marked final means end static initializer guaranteed finished running members visible heavy contention means initialization users get npe thread local initialization several classes thread safe
1316,0,had oop reinstated support running bash scripts cygwin logic involves setting cygwin flag variable indicate script executing cygwin flag set interactive scripts had oop hdfs yarn map red flag set had oop daemons h though cause erroneous overwrite had oop home java library path inside had oop config sh variable cygwin undefined had oop config sh executed had oop daemons h
1317,0,log examples get rid kind log production environment even debug loglevel sasl message md challenge text log even debuglevel
1318,1,azure storage exceptions currently logged part wab code often informative azure storage sdk supports client side logging enabled logs relevant information wr request made storage client jira created enable azure storage client side logging job submission level user able configure client side logging per job bases enable azure storage client side logging
1319,0,investigating yarn frustrating metrics system impl logging concurrent modification exception without backtrace logging backtrace would beneficial tracking cause problem metrics system impl fails show backtrace error occurs
1320,1,issue matching regex configurable via altering default add cli arg update invocation test patch issue matching regex configurable
1321,1,deprecated istcpvlogalyzer longer used deprecated istcpvlogalyzer
1322,0,native components build considered optional either build without passing special flags maven allow build proceed dependencies missing build machine components get built pre commit really providing full coverage build issue proposes update test patch sh full build native components guarantee full build native code pre commit
1323,0,attempting use ldap groups mapping gets needed passwords using credential provider unsurprisingly get get password from credential providers chooses java keystore provider like told java keystore provider constructor fs path get filesystem conf guess back path get filesystem started beginning please let know somehow configured something incorrectly figure use java keystore provider ldap groups mapping causes infinite loop
1324,0,seen situation one rm hangs stopping metrics sink adapter looks like sink thread interrupt metrics sink adapters top really interrupt thread cause hang join appears metrics sink adapter hangs stopped
1325,0,javac result report incremental number warnings test patch javac warning check reporting total number warnings instead incremental
1326,0,compile had oop native support encounter compile error undefined reference dlopen link libcrypto better link libdl explicitly c make list had oop pips fix undefined reference dlopen error compiling lib had oop pipes
1327,0,cleanup exit uses wrong result code check fails mv patch dir mv test patchs hmv wrong math
1328,0,branch switching smart apply patch sh needs updated test patch apply patches backport trunk smart apply patch sh branch
1329,0,might useful test patch sh functionality documented use power user hints etc esp bug bash test patch sh documented
1330,0,bytes writable setsize increases buffer size time unsafe operation since restricts maxsize mb since mb gb write test case case order trigger need allocate around mb pretty expensive unit test note throw exception case integer overflow want change behavior callers might expect java lang negative arraysize exception bytes writable fails support g chunks due integer overflow
1331,0,saw building ec branch pretty sure repro trunk though had oop dist dist layout stitching sh work dash
1332,0,following come job failed deletion service trying delete log fiels info org apache had oop yarn server node manager default container executor deleting absolute path null error org apache had oop yarn server node manager deletion serviceexception execution task deletion service java lang nullpointerexception org apache had oop fs file context fix relative part file context java org apache had oop fs file context delete file context java org apache had oop yarn server node manager default container executor delete as user default container executor java org apache had oop yarn server node manager deletion service file deletion task run deletion service java java util concurrent executors runnable adapter call executors java java util concurrent future task run future task java java util concurrent scheduled threadpool executor scheduled future task access scheduled threadpool executor java java util concurrent scheduled threadpool executor scheduled future task run scheduled threadpool executor java java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java file context java fix relative part check null informative exception
1333,0,jenkins java failing due number javadoc violations considered errors following classes authentication filter java certificate util java rollover signer secret provider java signer secret provider java zk signer secret provider java key authorization key provider java jdk authentication filter certificate util signer secret providers key authorization key provider javadoc issues
1334,0,jenkins java failing jwt redirect authentication handler tags something javadoc java considers illegal jwt redirect authentication handler breaks java javadocs
1335,1,set minimum version trunk jdk jdk set minimum version had oop jdk
1336,0,jenkins failing test certificate util test corrupt pem test certificate util test corrupt pem failing jenkins jdk
1337,1,discussed aw avro docker based solution created setuptools full build enables much easier reproduction issues getting running new developers issue copy port setup had oop project preparation bug squash make setting build environment easier
1338,0,http had oop apache org docs current still shows new features had oop document updated update release note index m dvm
1339,1,large source trees dist cp taking long time buildfile listing client code starting mappers dataset used files k dirs taking minutes fix had oop minutes fix speed dist cp build listing using threadpool
1340,0,per https builds apache org job pre commit map reduce build artifact patch process new patch findbugs warnings had oops ls html warnings fixed fix findbugs warnings had oops ls
1341,0,found two issues reviewing patches had oop test annotation test executed clean test methods test codec java
1342,1,commit had oop changes txt file seo led remove remove changes txt files
1343,0,commit had oop need include new format release information trunk jira including old versions tree update src site markdown releases include old versions had oop
1344,0,noticed pre commit builds could end running wrong set unit tests patches instance yarn changes yarn files test run one mr modules suspect race condition multiple builds executing node remnants previous run getting picked fix pre commit builds execute right set tests
1345,0,seems like touch api files j diff dev support missing j diff api files had oop also missing yarn generating j diff api files j diff broken had oop
1346,0,rm fails start nonsecure mode following exception likely regression introduced had oop rm fails start nonsecure mode due authentication filter failure
1347,1,current way generate build artifacts awful plus ugly case release notes hard pick important rework changelog release notes
1348,0,n attempts read encounters ioexception read current logic reopen connection thus ends op committing wrong truncated output stack trace example quote tez child info org apache pig backend had oop executionengine tez runtime pig processor starting output or gap achetez map reduce output mr output dbd vertex scope tez child debug org jets service impl rest httpclient http method release input stream released http method response data stream threw exception org apache httpconnection closed exception premature end content length delimited message body expected received org apache http impli o content length input stream read content length input stream java org apache http conne of sensor input stream reade of sensor input stream java org jets service io interrupt able input stream read interrupt able input stream java org jets service impl rest httpclient http method release input stream read http method release input stream java org apache had oop fs native natives filesystem native sfs input stream read natives filesystem java java io bufferedinputstream read bufferedinputstream java java io bufferedinputstream read bufferedinputstream java java io datainputstream read datainputstream java org apache had oop util line reader fill buffer line reader java org apache had oop util line reader read default line line reader java org apache had oop util line reader readline line reader java org apache had oop map reduce lib input line record reader next key value line record reader java org apache pig built in pig storage get next pig storage java org apache pig backend had oop executionengine map reduce layer pig record reader next key value pig record reader java or gap achetez map reduce lib mr reader map reduce next mr reader map reduce java org apache pig backend had oop executionengine tez plan operator po simple tez load get next tuple po simple tez load java org apache pig backend had oop executionengine physical layer physical operator process input physical operator java org apache pig backend had oop executionengine physical layer relational operators po for each get next tuple po for each java org apache pig backend had oop executionengine physical layer physical operator process input physical operator java org apache pig backend had oop executionengine physical layer relational operators po filter get next tuple po filter java org apache pig backend had oop executionengine physical layer physical operator process input physical operator java org apache pig backend had oop executionengine tez plan operator po store tez get next tuple po store tez java org apache pig backend had oop executionengine tez runtime pig processor run pipeline pig processor java org apache pig backend had oop executionengine tez runtime pig processor run pig processor java or gap achetez runtime logical io processor runtime task run logical io processor runtime task java or gap achetez runtime task tez task runner task runner callable run tez task runner java or gap achetez runtime task tez task runner task runner callable run tez task runner java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java or gap achetez runtime task tez task runner task runner callable call tez task runner java or gap achetez runtime task tez task runner task runner callable call tez task runner java java util concurrent future task run future task java java util concurrent threadpool executor run worker threadpool executor java java util concurrent threadpool executor worker run threadpool executor java java lang thread run thread java tez child info org apache had oop fs native natives filesystem received ioexception reading user had oo ptsa to read large input cloud ian log attempting reopen tez child debug org jets service impl rest httpclient http method release input stream released http method response data stream fully consumed tez child info or gap achetez dag app task attempt listener impt ez dag commit gogo request attempt tez child info org apache tezdagappdagimpltaskimpl attempt given go committing task output quote seems regression introduced following optimizations https issues apache org jira browse had oop https issues apache org jira browse had oop also test cases reviewed covers scenario regression n read failure recovery broken
1349,0,mvn package pdi std skip tests fails jdk caused incorrect illegal tags doc comments jdk fix javadoc errors caused incorrect illegal tags had oop tools
1350,1,extend alt kerberos authentication handler provide webs so flow uis actual authentication done external service handler redirect had oop auth cookie jwt token found in coming request using jwt provides number benefits tied specific authentication mechanism buys us many sso integrations cryptographically verifiable determining whether trusted checking expiration allows limited lifetime window compromised use introduce use nimbus jose jwt library processing validating parsing jwt tokens add redirecting webs so behavior jwt token had oop auth
1351,1,currently view filesystem dispatch snapshot methods mount table snapshot methods throw unsupported operation exception even though underlying mountpoints could hdfs instances support snapshots need update view filesystem implement snapshot methods view file system support snapshot methods
1352,1,dist cpv pretty much unsupported remove removed istcpvlogalyzer
1353,0,had oop covered core bugs surfacing had oop enhancements improve performance proxy custom endpoints jira covers post issues enhancements ber jira phase ii robustness scale performance
1354,0,lots following messages appeared nn log quote warn security logger org apache had oop ipc server auth failed null digest md ioerror acquiring password info org apache had oop ipc serversocket reader port read and process client threw exception org apache had oop ipc standby exception operation category read supported state standby security logger org apache had oop ipc server auth failed null digest md ioerror acquiring password info org apache had oop ipc serversocket reader port read and process client threw exception org apache had oop ipc standby exception operation category read supported state standby quote real reason failure second message standby exception however first message confusing talks digest md ioerror acquiring password filing jira modify first message comprehensive information obtained get cause for invalid token e improve authentication failure warn message avoid user confusion
1355,0,had oop recently fixed x build yarn compiling x results error x build lib win utils broken
1356,1,emc vip recs object storage platform uses proprietary headers starting x emc like amazon x amz headers starting x emc included signature computation done amazon java sdk done emc sdk copy object copies headers object includes x emc headers generates signature mismatch removing x emc headers copy would allow compatible emc vip recs object storage platform removing xx amz headers copy would allow compatible object storage platform using proprietary headers ignore x response headers copying amazon object
1357,0,similar had oop different place h base distributed log splitting multiple threads access folder called recovered edits however lots places was b storage exception complaining lease idh base distributed log splitting
1358,1,file util copy merge currently unused had oop source tree branch part implementation had oop fs get merge shell command branch code shell command rewritten way longer requires method please check details https issues apache org jira browse had oop focused comment id page com atlassian jira plugin system issue tab panels comment tab panel comment deprecate file util copy merge
1359,1,according discussion had oop remove io native lib available trunk always use native libraries exist remove io native lib available
1360,0,fix issues h traced rest receiver resulting unit test failures two main issues better way launch h traced fixes h traced rest receiver logic fix test h traced rest receiver unit test failures
1361,1,new file added source devnull rather root tree would mean b prefix allow allow smart apply patch sh add new files binary git patches
1362,0,convert rest documentation markdown also shell script rewrite changes a long way convert site documentation apt markdown stragglers
1363,1,had oop committed need remove usages yarn daemon ssh had oop daemon ssh start stop scripts converting use new slaves option additionally documentation updated reflect newcommand options updates bin commands documentation use new slaves option
1364,0,execute command yarn daemon log set level xxx xxx xxx resource manager debug b reflecting process log seven performing client level operations clog level changed daemon log documentation misleading
1365,1,add slaves shell option had oop config sh trigger given command slave nodes required deprecate had oop daemon ssh yarn daemons shadd slaves shell option
1366,1,had oop kerberos name around secret hack quite clean output make official exposing via had oop command expose had oop kerberos name had oop subcommand
1367,0,had oop need formally document functions environment variables rd parties expectable exist use formalize shell api
1368,1,rpc server handler thread tied incoming rpc request ideal since essentially implies rpc operations short lived operations could take time end falling back polling mechanism use cases useful yarn submit application currently submits followed poll check application accepted submit operation written storage collapsed single call yarn allocate requests allocations use protocol new allocations received via polling allocate protocol could split request heartbeat along await response request heartbeat sent request much longer heartbeat interval await response always left active rm returns moment something available map reduce tez task communication another example pattern pattern splitting calls used protocols well serve improve latency well reduce network traffic since keepalive heartbeat sent less frequently believe cases hdfs well dn gets told perform operations heartbeat nn allow handoff server side rpc requests
1369,0,check native display nicer error message openssl support compiled currently displays check native display nicer error message openssl support compiled
1370,0,had oop made change include exception h native ioc header includes use nonstandard gcc attribute declaration thus fails compilation windows had oop common native compilation fails windows due missing support attribute declaration
1371,0,hdfs builds failing jenkins checkstyle failing unable instantiate double checked locking check
1372,1,application sitting top had oop got problems using jsch switched java got exception upgrading jsch jsch fixed issue us got conflict had oop jsch version fixed us jar jaring jsch version think jsch got introduce name node ha hdfs guys check ssh part properly working java preventively upgrade jsch lib jsch references problems reported http sourceforge net pj sch mailman jsch users thread loom post gmane org https issues apache org bugzilla show bug cgi id upgrade jsch lib jsch avoid problems running java
1373,1,hdfs dfs admin yarn rm admin service level auth still references had oo pdfs admin mr admin
1374,1,would useful provide way core noncore had oop components plug shell infrastructure would allow us pull hdfs map reduce yarn shell functions had oop functions sh additionally let rd parties h base influence things like classpaths runtime pluggable shell integration
1375,1,along vein had oop several remaining usages guava ap is incompatible recent version eg jira proposes eliminating usages had oop base compatible guava remove uses obsolete guava apis had oop codebase
1376,1,common shell pattern x effectively replace subproject specific vars generics function replacement provides warning end user old shell var deprecated additionally use shell function deprecate shell vars holdovers already deprecate shell vars
1377,0,tried had oops rc branch branch trunk win gave error error failed execute goal org apache maven plugins maven javadoc plugin jar module javadocs project had oop annotations maven report exception error creating archive error exit code e projects had oop common had oop common project had oop annotations src main java org apache had oop classification interface stability java error unexpected end tag error error error error command line c program files java jdk jre bin javadoc exe options packages error error refer generated javadoc files e projects had oop common had oop common project had oop annotations targetdir error help error error see full stack trace errors run maven e switch error run maven using x switch enable full debug logging error error information errors possible solutions please read following articles error help http c wiki apache org confluence display maven mojo execution exception error error correcting problems resume build command error mvn rf had oop annotations jdk can not build windows error unexpected end tag
1378,0,text remove version author information dist cp readme file
1379,0,building windows section building txt obsolete reference cygwin removed avoid confusion remove obsolete reference cygwin building txt
1380,0,following c besides default had oop always treat default clazz think bug please let know work design thing thanks private static final string default clazz org apache had oo pio compress lzo codec public synchronized boolean is supported checked checked true string ext clazz conf getconf lzo class null system get property conf lzo class null string clazz ext clazz null ext clazz default clazz property io compression codec lzo class work value besides default
1381,0,had oop removes org apache had oop fs permission access control exception causing build break hive compiling had oop hive build failure had oop due had oop
1382,0,had oop documentation source files had oo paws moved src site src main site build longer actually generating html site source files src site expected path had oo paws documentation missing
1383,0,today windows parts had oop source official tm variable still honoring had oop prefix set revert had oop prefix go back had oop home
1384,0,part had oop large extremely useful sections rack awareness documentation added had oop wiped restore separate document restore rack awareness documentation
1385,1,hfs permission access control exception deprecated last major releases removed removed deprecated hfs permission access control exception
1386,1,system able read user defined env varsha doo prc add support had oo prc
1387,0,users spnego token large default header buffer used jetty though issue fixed http connections via had oop https connections needs fixed well size header buffer http server small https enabled
1388,1,rewrites ls rumen use news hell framework
1389,0,running tera sort following options had oop jar had oop map reduce examples jar tera sort dio native lib available false d map reduce map output compress true d map reduce map output compress codec org apache had oo pio compress gzip codec tmp tera tmp tera job failed reducer failed fetch output mappers see following stack trace problem jira map reduce added support handle null compressors default non compressed output case io native lib available set false compressor null however decompressor java implementation reducer tries read mapper output uses decompressor output gzip header warn fetcher org apache had oop map reduce task reduce fetcher failed shuffle output attempt b dvs java io ioexception gzip file org apache had oo pio compress zlib built in gzip decompressor process basic header built in gzip decompressor java org apache had oo pio compress zlib built in gzip decompressor execute header state built in gzip decompressor java org apache had oo pio compress zlib built in gzip decompressor decompress built in gzip decompressor java org apache had oo pio compress decompressor stream decompress decompressor stream java org apache had oo pio compress decompressor stream read decompressor stream java org apache had oop ioi outils read fully i outils java org apache had oop map reduce task reduce in memory map output shuffle in memory map output java org apache had oop map reduce task reduce fetcher copy map output fetcher java org apache had oop map reduce task reduce fetcher copy from host fetcher java org apache had oop map reduce task reduce fetcher run fetcher java map reduce job failed due failure fetching mapper output reduce side
1390,0,found data nodes run exceeds limit concurrent xci ever limit k check stack suspect org apache had oop net unix domain socket write array called domain socket watcher kick stuck quote data x ceiver client unixvarrunhadoophdfsdn waiting operation daemon priotidxfcnidx waiting condition xf java lang thread state waiting parking sun misc unsafe park native method parking wait java util concurrent locks reentrant lock non fair sync java util concurrent locks lock support park lock support java java util concurrent locks abstract queued synchronizer park and check interrupt abstract queued synchronizer java java util concurrent locks abstract queued synchronizer acquire queued abstract queued synchronizer java java util concurrent locks abstract queued synchronizer acquire abstract queued synchronizer java java util concurrent locks reentrant lock non fair sync lock reentrant lock java java util concurrent locks reentrant lock lock reentrant lock java org apache had oop net unix domain socket watcher add domain socket watcher java org apache had oop hdfs server data node short circuit registry create new memory segment short circuit registry java org apache had oop hdfs server data node data x ceiver request short circuits hmda tax ceiver java org apache had oop hdfs protocol data transfer receiver op request short circuit shm receiver java org apache had oop hdfs protocol data transfer receiver process op receiver java org apache had oop hdfs server data node data x ceiver run data x ceiver java data x ceiver client unixvarrunhadoophdfsdn waiting operation daemon prio tidxfdecnidxbrunnablexfd bc java lang thread state runnable org apache had oop net unix domain socket write array native method org apache had oop net unix domain socket access domain socket java org apache had oop net unix domain socket domain output stream write domain socket java org apache had oop net unix domain socket watcher kick domain socket watcher java org apache had oop net unix domain socket watcher add domain socket watcher java org apache had oop hdfs server data node short circuit registry create new memory segment short circuit registry java org apache had oop hdfs server data node data x ceiver request short circuits hmda tax ceiver java org apache had oop hdfs protocol data transfer receiver op request short circuit shm receiver java org apache had oop hdfs protocol data transfer receiver process op receiver java org apache had oop hdfs server data node data x ceiver run data x ceiver java java lang thread run thread java data x ceiver client unixvarrunhadoophdfsdn waiting operation daemon priotidxfcnidx waiting condition xf java lang thread state waiting parking sun misc unsafe park native method parking wait java util concurrent locks abstract queued synchronizer condition object java util concurrent locks lock support park lock support java java util concurrent locks abstract queued synchronizer condition object await abstract queued synchronizer java org apache had oop net unix domain socket watcher add domain socket watcher java org apache had oop hdfs server data node short circuit registry create new memory segment short circuit registry java org apache had oop hdfs server data node data x ceiver request short circuits hmda tax ceiver java org apache had oop hdfs protocol data transfer receiver op request short circuit shm receiver java org apache had oop hdfs protocol data transfer receiver process op receiver java org apache had oop hdfs server data node data x ceiver run data x ceiver java java lang thread run thread javathread daemon p riot idxfccnidxrunnablexfaefe java lang thread state runnable org apache had oop net unix domain socket watcher do poll native method org apache had oop net unix domain socket watcher access domain socket watcher java org apache had oop net unix domain socket watcher run domain socket watcher java java lang thread run thread java quote fix deadlock domain socket watcher notification pipe full
1391,0,zk failover controller java exception caught run method single error log causes latent problems manifested failover h problem encountered exception thrown do run method in it hm caused configuration error want repeat seth a health monitor connect retry interval ms nonsensical value thanks zk failover controller log exception do run raises errors
1392,0,see details infraperabayercnauroth feedback creating jira investigate possible bugdev support test patch sh script thanks andrew chris submitting had oop patch trigger jenkins test run
1393,0,filesystem shell apt vmstat accepts formats b size file blocks g group name owner n filename block size r replication u username owner utc date yyyymmdd hhmmss milliseconds since january utc documented update document had oop fs stat
1394,0,command fails following files include non ascii characters comparable version java common configuration keys public java comparable version java javadoc mnt build had oops rc had oop common project had oop common src main java org apache had oop fs common configuration keys public java error un mappable character encoding ascii javadoc mvn package p dist docs d skip tests dtar fails non ascii characters
1395,0,had oop uppercase key names allowed breaking unit tests let fix fix unit tests use uppercase key names
1396,0,had oop daemon ssh throws command found had oop daemon ssh mainly used start cluster ex start df ssh without cluster able start had oop daemon ssh throws host bash host command found
1397,0,had oop uses older version jetty allows ssl v fix patch jetty disables slv
1398,0,checked several download mirrors seem missing common folder things eedisthadoopchukwa blocker since download had oop had oop common directory missing download mirrors checked
1399,0,jira fix javac warnings overlooked had oop format warning home jenkins jenkins slave workspace pre commit had oop build had oop hdfs project had oophdfsnfssrcmainjavaorg apache hadoophdfsnfsconfnfs configuration java deprecation nfs user group update millis key orgapachehadoopnfsnfsnfs constant deprecated warning home jenkins jenkins slave workspace pre commit had oop build had oop hdfs project had oophdfsnfssrcmainjavaorg apache hadoophdfsnfsconfnfs configuration java deprecation nfs static mapping file key orgapachehadoopnfsnfsnfs constant deprecated format fix couple javac warnings nfs
1400,1,had oop drop support java jenkins slaves compiling code using java move jenkins java
1401,0,https builds apache org job pre commit yarn build console https builds apache org job pre commit yarn build console https builds apache org job pre commit yarn build console https builds apache org job pre commit map reduce build console couple jenkins build failure reason seems broken had oop jenkins build seems broken changes test patch sh
1402,1,heavy shuffle packet loss ipc packets observed machine avoid packet loss speed transfer using x qos bits packets add configuration set ipc client trafficclass ip to slow delay ip tos reliability
1403,0,good make jenkins verify mvn site patch contains apt vm changes avoid obvious build failure yarn first time similar issues raised automative verification inform us alert us encounter actual build failure involves site lifecycle jenkins verify mvn site patch contains apt vm changes
1404,0,per discussion hdfs creating jira thank saw work hdfs replace daemon better name script subcommands
1405,1,dist cp uses throttle input stream provides bandwidth throttling specified stream currently dist cp allows max bandwidth value megabytes accept fractional values would better accepts max band wit dh fractional megabytes due able throttle bandwidth kbs prod setup allow d its cp accept band wit dh fraction megabytes
1406,0,trying tezwindowsteztargz failed localize windows env also reproduced issue similar tarball used distributed cache mr sleep job tarball local resource type archive fails localize windows
1407,0,zk dts m tries renew token created peer throws exception message bar trying renew token wrong password zk delegation token secret manager fails renew token created peer
1408,0,kms delegation token operation keeps throwing unable find valid certification path requested target looks much like trust store picked fix delegation token authenticated url pass connection configurator authenticator
1409,0,test fair call queue test put blocks when all full fails trunk branch test fair call queue fails
1410,1,had oop currently supports one jvm defined java home since multiple jvms java active helpful user configuration choose custom supported jvm job words user able choose expected jvm container execution had oop services may running different jvm allow user choose jvm container execution
1411,0,had oop adds zk implementation delegation token secret manager follow jira address review comments findbugs order updates current key fix findbugs zk delegation token secret managers
1412,0,site build broken
1413,1,java coming quickly various clusters making sure had oop seamlessly works java important apache community jira track issues experiences encountered java migration find potential bug please create separate jira either subtask linked jira find had oop jvm configuration tuning create jira well add comment umbrella support java had oop
1414,0,quite java properties expected set shellcode currently undocumented document had oop properties expected set shellcode env sh
1415,0,using token delegation authentication filter noticed specify hosts usergroups proxy user try authenticate get npe rather authorization exception npe hosts specified proxy users
1416,0,private native method names signatures native crc changed hdfs result had oop common jars get un satisifed link errors try perform checksums essentially stops had oop applications running had oop unless rebuilt repackaged had oop jar sun satisifed link error had oop jars had oop due native crc method changes
1417,0,windows maximum path length characters kms includes several long class file names packaging creation distro paths get even longer prepending standard war directory structure share had oop etc structure end result final paths longer characters making impossible deploy distro windows kms can not deploy windows class names long
1418,0,had oop confdir defined points directory either exist actually viable configuration directory sorts weird things happen especially logging shellcode better job verifying directory valid exit detects broken way missing had oop confdir generates strange results
1419,0,crypto input stream int read bytebuffer buf unread value out buffer current return value incorrect case happens caller uses bytes array read firstly bytebuffer read return value read bytebuffer buf crypto input stream incorrect cases
1420,0,dist cpmr branch fails npe using short relative sourcepath failure dist cp java make relative return null following solution change root full path match child getpath dist cpmr branch fails npe using short relative sourcepath
1421,0,running start df ssh script pick java specified java home defined zshrc bashrc java home expected use jdk instead bug occurs trunk branch shell scripts ignore java home osx
1422,1,little reason call had oop daemons h anymore had oop daemon ssh call hdfs directly
1423,1,part had oop java execution across many different shell bits consolidated effectively two routines prior calling two routines classpath exported export really getting handled exec function individual shell bits additionally would good bash x would show content classpath even debug classpath option would echo classpath screen prior java exec help debugging classpath handling consolidated debug gable
1424,0,list had oop prefix bin hadoopfslsuserericpfoorw eric phd fsuserericpfoocathadoop prefix bin hadoopfscatuserericpfoo text had oop prefix bin had oop fs text user eric p foo text java io eofexception java io datainputstream read short datainputstream java org apache had oop fs shell display text getinputstream display java org apache had oop fs shell display cat process path display java org apache had oop fs shell command process paths command java org apache had oop fs shell command process path argument command java org apache had oop fs shell command process argument command java org apache had oop fs shell command process arguments command java org apache had oop fs shell command process raw arguments command java org apache had oop fs shell command run command java org apache had oop fsfs shell run fs shell java org apache had oop util tool runner run tool runner java org apache had oop util tool runner run tool runner java org apache had oop fsfs shell main fs shell java had oop fs text zero length file causes eofexception
1425,0,want pre commit builds rung it repo transition fix test patchwork git repo
1426,0,ha service protocol monitor health throws healthcheck failed exception actual exception protocol buffer rpc remoteexception wraps real exception thus state incorrectly transitioned service responding ha service protocol health state incorrectly transitioned service responding
1427,0,info info maven ant run plugin run make had oop common info executing tasks main exec c compiler identification gnu exec cxx compiler identification gnu exec check working c compiler usr bin cc exec check working c compiler usr bin cc works exec detecting c compiler abi info exec detecting c compiler abi info done exec check working cxx compiler usr bin c exec check working cxx compiler usr bin c works exec java home java jvm library java jvm library not found exec java include path usr lib jvm java open jdk include java include path usr lib jvm java open jdk include linux exec c make error jni flags c make message exec failed find viable jvm installation java home exec call stack recent call first exec c make lists txt include exec exec exec detecting cxx compiler abi info exec detecting cxx compiler abi info done exec configuring incomplete errors occurred exec see also root big top build had oop rpmbuild had oops rc had oop common project had oop common target native c makefiles c make output loginfo info reactor summary info info apache had oop main success info apache had oop project pom success info apache had oop annotations success info apache had oop assemblies success info apache had oop project dist pom success info apache had oop maven plugins success info apache had oop mini kdc success info apache had oop auth success info apache had oop auth examples success info apache had oop common failure info apache had oop nfs skipped info apache had oop common project skipped info apache had oop hdfs skipped info apache had oop http fs skipped info apache had oop hdfs bookkeeper journal skipped info apache had oop hdfs nfs skipped info apache had oop hdfs project skipped info had oop yarn skipped info had oop yarn api skipped info had oop yarn common skipped info had oop yarn server skipped info had oop yarn server common skipped info had oop yarn server node manager skipped info had oop yarn server web proxy skipped info had oop yarn server resource manager skipped info had oop yarn server tests skipped info had oop yarn client skipped info had oop yarn applications skipped info had oop yarn applications distributed shell skipped info had oop yarn applications unmanaged launcher skipped info had oop yarn site skipped info had oop yarn project skipped info had oop map reduce client skipped info had oop map reduce client core skipped info had oop map reduce client common skipped info had oop map reduce client shuffle skipped info had oop map reduce client app skipped info had oop map reduce client hs skipped info had oop map reduce client job client skipped info had oop map reduce client hs plugins skipped info apache had oop map reduce examples skipped info had oop map reduce skipped info apache had oop map reduce streaming skipped info apache had oop distributed copy skipped info apache had oop archives skipped info apache had oop rumen skipped info apache had oop grid mix skipped info apache had oop data join skipped info apache had oop extras skipped info apache had oop pipes skipped info apache had oop open stack support skipped info apache had oop client skipped info apache had oop mini cluster skipped info apache had oop scheduler load simulator skipped info apache had oop tools dist skipped info apache had oop tools skipped info apache had oop distribution skipped info info build failure info info total time info finished info final memory info error failed execute goal org apache maven plugins maven ant run plugin run make project had oop common ant buildexception occured exec returned error around ant part root big top build had oop rpmbuild had oops rc had oop common project had oop common target ant run build main xml error help error error see full stack trace errors run maven e switch error run maven using x switch enable full debug logging error error information errors possible solutions please read following articles error help http c wiki apache org confluence display maven mojo execution exception error error correcting problems resume build command error mvn rf had oop common error bad exit status vartmprpmtmpsuxmus build had oop native build fails detect javalib arch ppc le
1428,0,had oop had oop common native compilation broken windows had oop common native compilation broken windows
1429,0,org apache had oo pio native io native io java posix f advise flag parameter hardcoded common values fcntl h however architectures use values case system z linux better approach would make assumptions fcntl h values system constants bug results calls posix f advise failing z linux flags posix f advise valid architectures
1430,0,glob ber sometimes erroneously return permission denied exception nonterminal wildcard existing unit tests catch happen superusers glob ber sometimes erroneously return permission denied exception nonterminal wildcard
1431,0,create release script include docs binary tarball fix fix create release script include docs necessary txt files
1432,1,post had oop need rework heap configured small footprint machines deprecate options introduce new ones greater flexibility rework heap management vars
1433,1,write metrics sink plugin had oops end metrics directly apache kafka addition current graphite had oop https issues apache org jira browse had oop ganglia file sinks metrics sink plugin apache kafka
1434,0,providing digit octal number fs permissions leads parse error eg dfs permissions umask mode digit octal umask permissions throws parse error
1435,0,null passed impersonation providers remote address unresolvable default imper sation provider npe ipc close connection immediately correct behavior unexpected exceptions client fails eofexception proxy user verification npes remote host unresolvable
1436,0,touch zing file results nullpointerexception need set version name correctly decrypting eek
1437,0,hdfs introduced new native code function creating hard links windows implementation function compile due incorrect call create hardlink compilation fails native link function windows
1438,0,jmx json servlet do get following check jmx servlet fails work kms jmx json servlet fails used within tomcat
1439,0,release build fails obscure findbugs error testing reveals related findbugs heap size increase findbugs max heap size
1440,0,failure message test df variations test mount fails intermittently
1441,0,bunch test path data tests failed intermittently eg https builds apache org job pre commit hdfs build test report example failure log test path data fails intermittently mkdir s failed create
1442,0,symlink tests failure happened time time https builds apache org job pre commit hdfs build test report https builds apache org job pre commit hdfs build test report raw local file system fails read symlink targets via stat command format stat command uses non curly quotes
1443,0,java keystore provider thread safe user provider thread safe user provider thread safe
1444,1,kms http fs using tomcat move get bug fixes security fixes add property tomcat version had oop project pom use property kms http fs update tomcat version used http fs kms latest x version
1445,0,jenkins failing following message https builds apache org job pre commit had oop build console jenkins failing due upgrade svn client
1446,0,documents filesystem api definition created had oop linked had oop project src site site xml fix dead link site xml
1447,0,although releases page current root index html page http had oop apache org index html stops release http had oop apache org index html lists versions
1448,1,kms rewritten use news hell framework reworked take advantage rewrite kms use news hell framework
1449,1,make effort clean shell env var namespace removing unsafe variables see comments list rename remove non had oop etc shell scripts
1450,0,trying had oop freebsd stable name node starts first data node contacts throws exception limits seem high enough limits resource limits current cputime infinity secs filesize infinity kb data size kb stacksize kb core dump size infinity kb memory use infinity kb memory locked infinity kb max processes open files sb size infinity bytes v memory use infinity kb pseudo terminals infinity swap use infinity kb usr local open jdk bin javad procname node xmxdhadooplogdirvarlogha do opd had oop log file had oop hdfs name nodenezabudkalogdhadoop homedir usr local dhadoopidstrhdfsdhadoop root logger in for fad had oop policy file had oop policy xml djavan et prefer ipvstacktruexmxxmsdjava library path usr local libxmxxmsdjava library path usr local libxmxxmsdjava library path usr local libd had oop security logger in for fas org apache had oop hdfs servername node name node name node log war nipc server handler ipc server server java run ipc server handler call org apache had oop hdfs server protocol data node protocol version request call retry java lang outofmemoryerror org apache had oop security jni based unix groups mapping get groups for user native method org apache had oop security jni based unix groups mapping get groups jni based unix groups mapping java org apache had oop security jni based unix groups mapping with fall back get groups jni based unix groups mapping with fall back java org apache had oop security groups get groups groups java org apache had oop security user group information get group names user group information java org apache had oop hdfs servername no defs permission checker fs permission checker java org apache had oop hdfs servername no defs name system get permission checker fs name system java org apache had oop hdfs servername no defs name system check superuser privilege fs name system java org apache had oop hdfs servername node name node rpc server version request name node rpc server java org apache had oop hdfs protocol pb data node protocol server side translator pb version request data node protocol server side translator pb java org apache had oop hdfs protocol proto data node protocol protos data node protocol service call blocking method data node protocol protos java org apache hadoopipcprotobufrpc engine server proto buf rpc invoker call proto buf rpc engine java org apache had oop ipc rpc server call rpc java org apache had oop ipc server handler run server java org apache had oop ipc server handler run server java java security access controller do privileged native method javax security auth subject do as subject java org apache had oop security user group information do as usergroup information java org apache had oop ipc server handler run server java issue had oop had oop user info alloc fails freebsd due incorrect sysconf use
1451,1,storm would like able fetch delegation tokens forward running topologies access hdfs storm need open access apis notably filesystem add delegation tokens token renew credentials get all tokens user group information may others minimum adding storm list allowed api users ideally making public restricting access important functionality mr really makes secure hdfs inaccessible anything except mr tools reuse mr input formats open already widely used apis delegation token fetching renewal ecosystem projects
1452,0,mvn compile p native ends like error failed execute goal project had oop hdfs could resolve dependencies project org apache hadoophadoophdfsjar failure find org apache had oop had oop common jar tests https repository apache org content repositories snapshots cached local repository resolution re attempted update interval apache snapshots https elapsed updates forced help unable build had oop freebsd
1453,0,lz compression fails identify powerpc little endian architecture recognizes bigendian several test cases test compressor decompressor test codec test lz compressor decompressor fails due running org apache had oo pio compress test compressor decompressor tests run failures errors skipped time elapsed sec org junit internal comparison criteria array equals comparison criteria java org junit assert internal array equals assert java org junit assert assert array equals assert java org apache had oo pio compress compress decompress tester compression test strategy assert compression compress decompress tester java org apache had oo pio compress compress decompress tester test compress decompress tester java org apache had oo pio compress test compressor decompressor test compressor decompressor test compressor decompressor java lz compression fails recognize powerpc little endian architecture
1454,0,renaming file another existing file name says file exists colliding file directory results cryptic input output error renaming file directory containing filename results confusing error
1455,0,user want start name node user would got following exception caused missing or gmortbayjettyjspjettyjar pom xml info httphttp server added global filter safety class org apache had oop httphttp server quoting input filter info httphttp server added filter static user filter class org apache had oop httplib static user web filter static user filter context hdfs info httphttp server added filter static user filter class org apache had oop httplib static user web filter static user filter context static info httphttp server added filter static user filter class org apache had oop httplib static user web filter static user filter context logs info httphttp server added filter org apache had oop hdfs web auth filter class org apache had oop hdfs web auth filter info httphttp server add jersey resource package package name org apache had oop hdfs servername node web resources org apache had oop hdfs web resources path spec web hdfs v info httphttp server jetty bound port info mort bay log jetty info mort bay log jsp support find org apache jasper servlet jsp servlet warn mort bay log exception java netconnect exception connection timed java net plain socket impl socket connect native method java net plain socket impl do connect plain socket impl java java net plain socket impl connect to address plain socket impl java java net plain socket impl connect plain socket impl java java net socks socket impl connect socks socket impl java java net socket connect socket java java net socket connect socket java sunnet network client do connect network client java sunnet www httphttp client openserver httpclient java sunnet www httphttp client openserver httpclient java sunnet www httphttp client httpclient java sunnet www httphttp client new httpclient java sunnet www httphttp client new httpclient java sunnet www protocol http httpurlconnection get new httpclient httpurlconnection java sunnet www protocol http httpurlconnection plain connect httpurlconnection java sunnet www protocol http httpurlconnection connect httpurlconnection java sunnet www protocol http httpurlconnection getinputstream httpurlconnection javacom sun org apache xerces internal impl xml entity manager setup current entity xml entity manager javacom sun org apache xerces internal impl xml entity manager start entity xml entity manager javacom sun org apache xerces internal impl xml entity manager start dtd entity xml entity manager javacom sun org apache xerces internal impl xml dtd scanner impl set inputsource xml dtd scanner impl javacom sun org apache xerces internal impl xmldocument scanner impl dtd driver dispatch xmldocument scanner impl javacom sun org apache xerces internal impl xmldocument scanner impl dtd driver next xmldocument scanner impl javacom sun org apache xerces internal impl xmldocument scanner impl prolog driver next xmldocument scanner impl javacom sun org apache xerces internal impl xmldocument scanner impl next xmldocument scanner impl javacom sun org apache xerces internal impl xmlns document scanner impl next xmlns document scanner impl javacom sun org apache xerces internal impl xml documentfragment scanner impl scan document xml documentfragment scanner impl javacom sun org apache xerces internal parsers xml configuration parse xml configuration javacom sun org apache xerces internal parsers xml configuration parse xml configuration javacom sun org apache xerces internal parsers xmlparser parse xmlparser javacom sun org apache xerces internal parsers abstract sax parser parse abstract sax parser javacom sun org apache xerces internal jaxpsaxparserimpljaxpsax parser parse sax parser impl java javax xmlparser s sax parser parse sax parser java org mort bay xml xmlparser parse xmlparser java org mort bay xml xmlparser parse xmlparser java org mort bay jetty webapp taglib configuration configure webapp taglib configuration java org mort bay jetty webapp webapp context start context webapp context java org mort bay jetty handler context handler do start context handler java org mort bay jetty webapp webapp context do start webapp context java org mort bay component abstract lifecycle start abstract lifecycle java org mort bay jetty handler handler collection do start handler collection java org mort bay jetty handler context handler collection do start context handler collection java org mort bay component abstract lifecycle start abstract lifecycle java org mort bay jetty handler handler wrapper do start handler wrapper java org mort bay jetty server do start server java org mort bay component abstract lifecycle start abstract lifecycle java org apache had oop httphttp server start http server java org apache had oop hdfs servername node name node http server start name node http server java org apache had oop hdfs servername node name node start http servername node java org apache had oop hdfs servername node name node initialize name node java org apache had oop hdfs servername node name node name node java org apache had oop hdfs servername node name node name node java org apache had oop hdfs servername node name node create name node name node java org apache had oop hdfs servername node name node main name node java http server load jsp dtd local jars instead going remote
1456,0,patches fixing build had oop native library osx fix build native library macosx
1457,0,using h base export snapshot kerberized cluster exporting using had oop see following problem caused java lang illegalargumentexception java net unknownhostexception patch two org apache had oop security security util build token service security util java problem seems patch had oop get canonical service name export snapshot fails kerberized cluster using
1458,1,had oop streaming longer requires many classes h record jira removes dead code remove dead classes had oop streaming
1459,1,classes h record deprecated year half removed first step jira moves classes had oop streaming project user classes move h record had oop streaming
1460,0,umbrella jira addresses enhancements proxy user capability via subtasks proxy user improvements
1461,0,think would worthwhile add two lines top welcome website page stable version latest version number linking respective release like httpwww apache org dyn closer cgi lucene had oop had o optarg z promote versions latest stable proven thoughts add stable version line website frontpage
1462,0,adds park related project had oop page
1463,1,jetty longer maintained update dependency jetty update jetty dependency version
1464,1,noted map reduce had oop local dir allocator bottleneck multithreaded setups like shuffle handler consider moving lock less design minimizing critical sections small amount time involve operations local dir allocator avoid holding locks accessing filesystem
1465,1,currently test filter filesystem checks filesystem methods must implemented filter filesystem list methods exception rule jira wants make check stricter adding test ensuring methods exception rule list must implemented filter file system also cleans current class methods exception rule list interface avoid provide dummy implementation methods cleanup test filter filesystem
1466,0,backport had oop open stack module branch require little testing backport open stack support branch
1467,1,current data set register temp table actually materialize data considered creating temp view deprecate create new method called dataset create temp view replace if exists boolean default value replace if exists false register temp table call dataset create temp view replace if exists true deprecate register temp table add dataset create temp view
1468,0,comment favorites menu bar shown menu item long possible use comment favorites menu bar
1469,0,answer questionnaire wiki
1470,0,support ci server build demo demo setup
1471,0,supporting build setup pilot testing
1472,0,self learning training
1473,0,knowledge methods page develop protractor tests execute
1474,0,workbench artifact tab develop protractor tests execute
1475,0,workbench page develop protractor tests execute
1476,0,protractor based ui tests
1477,0,generic support user access
1478,0,front end implementation draw uploaded image editor
1479,0,backend implementation draw uploaded image editor
1480,0,learning evaluating simple deployment amelie learning docker container deployment
1481,0,learn evaluate simple deployment amelie
1482,0,nan
1483,0,workflow adding users project
1484,0,vas advise ip address pointing old instance redeploying amelie point correct vas advise ip address redeploying amelie
1485,0,test ci server vm deployments
1486,0,always backup users projects actually databases files necessary create everything new backup databases files
1487,0,deploying new version vm always backup users projects actually databases files necessary create everything new please let know feasible also o kjv as advise ip address pointing old instance redeploying amelie enhancement deploying new version vm
1488,0,require readme described software necessary executed box short explanation front end backend high level concept vas skeleton guide
1489,0,adduser put domain ad001insteadad001 should case sensitive future adding user domain name case sensitive
1490,0,user registration mail sometimes marked possible phishing message changed 1st identify marked phi sing mail registration mail marked possible phishing message
1491,0,fix bugs linked ticket log hours parent task level bug fixes sprint 16
1492,0,regular testing deployment environment
1493,0,reviewer dont show welcome page front end
1494,0,reviewers chandramouli remember option login page make func tonal
1495,0,support pilot user
1496,0,vmc i healthcare munich maintain information various deployment environment update status
1497,0,add apis rest testsuite
1498,0,add header java files
1499,0,transition smooth flickers cloud circles display welcome page transition smooth
1500,0,vas advise change table title to p5 results
1501,0,hints message inconsistent different facets
1502,0,nan
1503,0,nan
1504,0,daily testing amelie application
1505,0,wiki postman etc update document
1506,0,review rework
1507,0,please extend support ui perspective evaluation poc
1508,0,refactor comment stabilize code an it
1509,0,precondition ci server available connectors required libraries installed dod running system ci server enables user connect amelie source files deploy data connectors also ci server
1510,0,architec uture authoring documentation architectural update text according asr word guidance introduced context uc context aware recommendation mechani ms update use case sea model
1511,0,please contact followings was business units requirements document updated seibert eckhard pgierguidoseegerz0004rzk requirements presented provide us feedback review document interview bu architects
1512,0,user able select view project set projects shown landing page provide project list
1513,1,details see https wiki ct siemens de display t04mm data access layer https wiki ct siemens de display t04mm general create data access layer
1514,0,please upload profile picture jira account profile picture upload
1515,0,hi cheng liang unfortunately able start third sprint project see screenshot could please help give hint find strange happens second time reported failure issue https agile siemens net browse amelie113 mistake occurs regards florian sprint management
1516,0,included define generic model communication eg gap service tell rec service help needs provide place post information one idea could use blackboard pattern e post formulated question blackboard services may respond answer define communication approach solution services
1517,0,handle multiple browser tabs session logout
1518,1,necessary s3 reads writes work correctly had oop versions addjets3t dependency spark build
